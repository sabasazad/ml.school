[
  {
    "objectID": "teardown.html",
    "href": "teardown.html",
    "title": "Tear Down",
    "section": "",
    "text": "import sys\nfrom pathlib import Path\n\nCODE_FOLDER = Path(\"code\")\nsys.path.append(f\"./{CODE_FOLDER}\")\nimport boto3\n\nfrom constants import *\nfrom sagemaker.experiments.experiment import Experiment"
  },
  {
    "objectID": "teardown.html#remove-experiments",
    "href": "teardown.html#remove-experiments",
    "title": "Tear Down",
    "section": "Remove Experiments",
    "text": "Remove Experiments\nTo avoid incurring unnecessary charges, delete the SageMaker Experiment resources you no longer need.\n\nEXPERIMENT_NAME = \"tensorfl-ap2uhikoiith-KnMow1gYgw-aws-tuning-job\"\n\nexperiment = Experiment.load(experiment_name=EXPERIMENT_NAME, sagemaker_session=sagemaker_session)\nexperiment._delete_all(action=\"--force\")"
  },
  {
    "objectID": "teardown.html#clean-up-model-registry",
    "href": "teardown.html#clean-up-model-registry",
    "title": "Tear Down",
    "section": "Clean up Model Registry",
    "text": "Clean up Model Registry\nThis section deletes every model registered under a specific model group and then it removes the model group.\n\nfor mp in sagemaker_client.list_model_packages(ModelPackageGroupName=MODEL_PACKAGE_GROUP)[\"ModelPackageSummaryList\"]:\n    print(f\"Deleting {mp['ModelPackageArn']}\")\n    sagemaker_client.delete_model_package(ModelPackageName=mp[\"ModelPackageArn\"])\n\nsagemaker_client.delete_model_package_group(ModelPackageGroupName=MODEL_PACKAGE_GROUP)\n\nDeleting arn:aws:sagemaker:us-east-1:325223348818:model-package/emocional/1\n\n\n{'ResponseMetadata': {'RequestId': '3ee0e924-ab64-46cb-ada0-80ea8e5764a4',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '3ee0e924-ab64-46cb-ada0-80ea8e5764a4',\n   'content-type': 'application/x-amz-json-1.1',\n   'content-length': '0',\n   'date': 'Mon, 14 Aug 2023 18:18:20 GMT'},\n  'RetryAttempts': 0}}"
  },
  {
    "objectID": "cohort.html",
    "href": "cohort.html",
    "title": "Building Production Machine Learning Systems",
    "section": "",
    "text": "This notebook creates a SageMaker Pipeline to build an end-to-end Machine Learning system to solve the problem of classifying penguin species. With a SageMaker Pipeline, you can create, automate, and manage end-to-end Machine Learning workflows at scale.\nYou can find more information about Amazon SageMaker in the Amazon SageMaker Developer Guide. The AWS Machine Learning Blog is an excellent source to stay up-to-date with SageMaker.\nThis example uses the Penguins dataset, the boto3 library, and the SageMaker Python SDK.\nThis notebook is part of the Machine Learning School program."
  },
  {
    "objectID": "cohort.html#initial-setup",
    "href": "cohort.html#initial-setup",
    "title": "Building Production Machine Learning Systems",
    "section": "Initial setup",
    "text": "Initial setup\n\n\n\n\n\n\nNote\n\n\n\nBefore running this notebook, follow the setup instructions for the program.\n\n\nLet’s start by setting up the environment and preparing to run the notebook.\nWe can run this notebook is Local Mode to test the pipeline in your local environment before using SageMaker. You can run the code in Local Mode by setting the LOCAL_MODE constant to True.\n\nLOCAL_MODE = False\n\nLet’s load the S3 bucket name and the AWS Role from the environment variables:\n\nimport os\n\nbucket = os.environ[\"BUCKET\"]\nrole = os.environ[\"ROLE\"]\n\nS3_LOCATION = f\"s3://{bucket}/penguins\"\n\nIf you are running the pipeline in Local Mode on an ARM64 machine, you will need to use a custom Docker image to train and evaluate the model. This is because SageMaker doesn’t provide a TensorFlow image that supports Apple’s M chips.\n\narchitecture = !(uname -m)\nIS_APPLE_M_CHIP = architecture[0] == \"arm64\"\n\nLet’s create a configuration dictionary with different settings depending on whether we are running the pipeline in Local Mode or not:\n\nimport sagemaker\nfrom sagemaker.workflow.pipeline_context import PipelineSession, LocalPipelineSession\n\npipeline_session = PipelineSession(default_bucket=bucket) if not LOCAL_MODE else None\n\nif LOCAL_MODE:\n    config = {\n        \"session\": LocalPipelineSession(default_bucket=bucket),\n        \"instance_type\": \"local\",\n        # We need to use a custom Docker image when we run the pipeline\n        # in Local Model on an ARM64 machine.\n        \"image\": \"sagemaker-tensorflow-toolkit-local\" if IS_APPLE_M_CHIP else None,\n        \"framework_version\": None if IS_APPLE_M_CHIP else \"2.11\",\n        \"py_version\": None if IS_APPLE_M_CHIP else \"py39\",\n    }\nelse:\n    config = {\n        \"session\": pipeline_session,\n        \"instance_type\": \"ml.m5.xlarge\",\n        \"image\": None,\n        \"framework_version\": \"2.11\",\n        \"py_version\": \"py39\",\n    }\n\nLet’s now initialize a few variables that we’ll need throughout the notebook:\n\nimport boto3\n\nsagemaker_session = sagemaker.session.Session()\nsagemaker_client = boto3.client(\"sagemaker\")\niam_client = boto3.client(\"iam\")\nregion = boto3.Session().region_name"
  },
  {
    "objectID": "cohort.html#session-1---production-machine-learning-is-different",
    "href": "cohort.html#session-1---production-machine-learning-is-different",
    "title": "Building Production Machine Learning Systems",
    "section": "Session 1 - Production Machine Learning is Different",
    "text": "Session 1 - Production Machine Learning is Different\nIn this session we’ll run Exploratory Data Analysis on the Penguins dataset and we’ll build a simple SageMaker Pipeline with one step to split and transform the data.\n \nWe’ll use a Scikit-Learn Pipeline for the transformations, and a Processing Step with a SKLearnProcessor to execute a preprocessing script. Check the SageMaker Pipelines Overview for an introduction to the fundamental components of a SageMaker Pipeline.\n\nStep 1 - Exploratory Data Analysis\nLet’s run Exploratory Data Analysis on the dataset. The goal of this section is to understand the data and the problem we are trying to solve.\nLet’s load the Penguins dataset:\n\nimport pandas as pd\nimport numpy as np\n\npenguins = pd.read_csv(DATA_FILEPATH)\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMALE\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n\n\n\n\n\n\n\nWe can see the dataset contains the following columns:\n\nspecies: The species of a penguin. This is the column we want to predict.\nisland: The island where the penguin was found\nculmen_length_mm: The length of the penguin’s culmen (bill) in millimeters\nculmen_depth_mm: The depth of the penguin’s culmen in millimeters\nflipper_length_mm: The length of the penguin’s flipper in millimeters\nbody_mass_g: The body mass of the penguin in grams\nsex: The sex of the penguin\n\nIf you are curious, here is the description of a penguin’s culmen:\n\nNow, let’s get the summary statistics for the features in our dataset.\n\npenguins.describe(include=\"all\")\n\n\n\n\n\n\n\n\nspecies\nisland\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\ncount\n344\n344\n342.000000\n342.000000\n342.000000\n342.000000\n334\n\n\nunique\n3\n3\nNaN\nNaN\nNaN\nNaN\n3\n\n\ntop\nAdelie\nBiscoe\nNaN\nNaN\nNaN\nNaN\nMALE\n\n\nfreq\n152\n168\nNaN\nNaN\nNaN\nNaN\n168\n\n\nmean\nNaN\nNaN\n43.921930\n17.151170\n200.915205\n4201.754386\nNaN\n\n\nstd\nNaN\nNaN\n5.459584\n1.974793\n14.061714\n801.954536\nNaN\n\n\nmin\nNaN\nNaN\n32.100000\n13.100000\n172.000000\n2700.000000\nNaN\n\n\n25%\nNaN\nNaN\n39.225000\n15.600000\n190.000000\n3550.000000\nNaN\n\n\n50%\nNaN\nNaN\n44.450000\n17.300000\n197.000000\n4050.000000\nNaN\n\n\n75%\nNaN\nNaN\n48.500000\n18.700000\n213.000000\n4750.000000\nNaN\n\n\nmax\nNaN\nNaN\n59.600000\n21.500000\n231.000000\n6300.000000\nNaN\n\n\n\n\n\n\n\nLet’s now display the distribution of values for the three categorical columns in our data:\n\nspecies_distribution = penguins[\"species\"].value_counts()\nisland_distribution = penguins[\"island\"].value_counts()\nsex_distribution = penguins[\"sex\"].value_counts()\n\nprint(species_distribution)\nprint()\nprint(island_distribution)\nprint()\nprint(sex_distribution)\n\nspecies\nAdelie       152\nGentoo       124\nChinstrap     68\nName: count, dtype: int64\n\nisland\nBiscoe       168\nDream        124\nTorgersen     52\nName: count, dtype: int64\n\nsex\nMALE      168\nFEMALE    165\n.           1\nName: count, dtype: int64\n\n\nThe distribution of the categories in our data are:\n\nspecies: There are 3 species of penguins in the dataset: Adelie (152), Gentoo (124), and Chinstrap (68).\nisland: Penguins are from 3 islands: Biscoe (168), Dream (124), and Torgersen (52).\nsex: We have 168 male penguins, 165 female penguins, and 1 penguin with an ambiguous gender (‘.’).\n\nLet’s replace the ambiguous value in the sex column with a null value:\n\npenguins[\"sex\"] = penguins[\"sex\"].replace(\".\", np.nan)\npenguins[\"sex\"].value_counts()\n\nsex\nMALE      168\nFEMALE    165\nName: count, dtype: int64\n\n\nNext, let’s check for any missing values in the dataset.\n\npenguins.isnull().sum()\n\nspecies               0\nisland                0\nculmen_length_mm      2\nculmen_depth_mm       2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\nLet’s get rid of the missing values. For now, we are going to replace the missing values with the most frequent value in the column. Later, we’ll use a different strategy to replace missing numeric values.\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy=\"most_frequent\")\npenguins.iloc[:, :] = imputer.fit_transform(penguins)\npenguins.isnull().sum()\n\nspecies              0\nisland               0\nculmen_length_mm     0\nculmen_depth_mm      0\nflipper_length_mm    0\nbody_mass_g          0\nsex                  0\ndtype: int64\n\n\nLet’s visualize the distribution of categorical features.\n\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(3, 1, figsize=(6, 10))\n\naxs[0].bar(species_distribution.index, species_distribution.values)\naxs[0].set_ylabel(\"Count\")\naxs[0].set_title(\"Distribution of Species\")\n\naxs[1].bar(island_distribution.index, island_distribution.values)\naxs[1].set_ylabel(\"Count\")\naxs[1].set_title(\"Distribution of Island\")\n\naxs[2].bar(sex_distribution.index, sex_distribution.values)\naxs[2].set_ylabel(\"Count\")\naxs[2].set_title(\"Distribution of Sex\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nLet’s visualize the distribution of numerical columns.\n\nfig, axs = plt.subplots(2, 2, figsize=(8, 6))\n\naxs[0, 0].hist(penguins[\"culmen_length_mm\"], bins=20)\naxs[0, 0].set_ylabel(\"Count\")\naxs[0, 0].set_title(\"Distribution of culmen_length_mm\")\n\naxs[0, 1].hist(penguins[\"culmen_depth_mm\"], bins=20)\naxs[0, 1].set_ylabel(\"Count\")\naxs[0, 1].set_title(\"Distribution of culmen_depth_mm\")\n\naxs[1, 0].hist(penguins[\"flipper_length_mm\"], bins=20)\naxs[1, 0].set_ylabel(\"Count\")\naxs[1, 0].set_title(\"Distribution of flipper_length_mm\")\n\naxs[1, 1].hist(penguins[\"body_mass_g\"], bins=20)\naxs[1, 1].set_ylabel(\"Count\")\naxs[1, 1].set_title(\"Distribution of body_mass_g\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nLet’s display the covariance matrix of the dataset. The “covariance” measures how changes in one variable are associated with changes in a second variable. In other words, the covariance measures the degree to which two variables are linearly associated.\n\npenguins.cov(numeric_only=True)\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nculmen_length_mm\n29.679415\n-2.516984\n50.260588\n2596.971151\n\n\nculmen_depth_mm\n-2.516984\n3.877201\n-16.108849\n-742.660180\n\n\nflipper_length_mm\n50.260588\n-16.108849\n197.269501\n9792.552037\n\n\nbody_mass_g\n2596.971151\n-742.660180\n9792.552037\n640316.716388\n\n\n\n\n\n\n\nHere are three examples of what we get from interpreting the covariance matrix below:\n\nPenguins that weight more tend to have a larger culmen.\nThe more a penguin weights, the shallower its culmen tends to be.\nThere’s a small variance between the culmen depth of penguins.\n\nLet’s now display the correlation matrix. “Correlation” measures both the strength and direction of the linear relationship between two variables.\n\npenguins.corr(numeric_only=True)\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nculmen_length_mm\n1.000000\n-0.234635\n0.656856\n0.595720\n\n\nculmen_depth_mm\n-0.234635\n1.000000\n-0.582472\n-0.471339\n\n\nflipper_length_mm\n0.656856\n-0.582472\n1.000000\n0.871302\n\n\nbody_mass_g\n0.595720\n-0.471339\n0.871302\n1.000000\n\n\n\n\n\n\n\nHere are three examples of what we get from interpreting the correlation matrix below:\n\nPenguins that weight more tend to have larger flippers.\nPenguins with a shallower culmen tend to have larger flippers.\nThe length and depth of the culmen have a slight negative correlation.\n\nLet’s display the distribution of species by island.\n\nunique_species = penguins[\"species\"].unique()\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfor species in unique_species:\n    data = penguins[penguins[\"species\"] == species]\n    ax.hist(data[\"island\"], bins=5, alpha=0.5, label=species)\n\nax.set_xlabel(\"Island\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Distribution of Species by Island\")\nax.legend()\nplt.show()\n\n\n\n\nLet’s display the distribution of species by sex.\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nfor species in unique_species:\n    data = penguins[penguins[\"species\"] == species]\n    ax.hist(data[\"sex\"], bins=3, alpha=0.5, label=species)\n\nax.set_xlabel(\"Sex\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Distribution of Species by Sex\")\n\nax.legend()\nplt.show()\n\n\n\n\n\n\nStep 2 - Creating the Preprocessing Script\nThe first step we need in the pipeline is a Processing Step to run a script that will split and transform the data. This Processing Step will create a SageMaker Processing Job in the background, run the script, and upload the output to S3. You can use Processing Jobs to perform data preprocessing, post-processing, feature engineering, data validation, and model evaluation. Check the ProcessingStep SageMaker’s SDK documentation for more information.\nThe first step is to create the script that will split and transform the input data.\n\n\n\npreprocessor.py\n\nimport os\nimport sys\nimport argparse\nimport json\nimport tarfile\nimport tempfile\nimport time\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nfrom io import StringIO\nfrom pathlib import Path\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n\n\ndef preprocess(base_directory):\n    \"\"\"\n    This function loads the supplied data, splits it and transforms it.\n    \"\"\"\n\n    df = _read_data_from_input_csv_files(base_directory)\n    \n    target_transformer = ColumnTransformer(\n        transformers=[(\"species\", OrdinalEncoder(), [0])]\n    )\n    \n    numeric_transformer = make_pipeline(\n        SimpleImputer(strategy=\"mean\"),\n        StandardScaler()\n    )\n\n    categorical_transformer = make_pipeline(\n        SimpleImputer(strategy=\"most_frequent\"),\n        OneHotEncoder()\n    )\n    \n    features_transformer = ColumnTransformer(\n        transformers=[\n            (\"numeric\", numeric_transformer, make_column_selector(dtype_exclude=\"object\")),\n            (\"categorical\", categorical_transformer, [\"island\"]),\n        ]\n    )\n\n    df_train, df_validation, df_test = _split_data(df)\n\n    _save_baselines(base_directory, df_train, df_test)\n\n    y_train = target_transformer.fit_transform(np.array(df_train.species.values).reshape(-1, 1))\n    y_validation = target_transformer.transform(np.array(df_validation.species.values).reshape(-1, 1))\n    y_test = target_transformer.transform(np.array(df_test.species.values).reshape(-1, 1))\n    \n    df_train = df_train.drop(\"species\", axis=1)\n    df_validation = df_validation.drop(\"species\", axis=1)\n    df_test = df_test.drop(\"species\", axis=1)\n\n    X_train = features_transformer.fit_transform(df_train)\n    X_validation = features_transformer.transform(df_validation)\n    X_test = features_transformer.transform(df_test)\n\n    _save_splits(base_directory, X_train, y_train, X_validation, y_validation, X_test, y_test)\n    _save_model(base_directory, target_transformer, features_transformer)\n    \n\ndef _read_data_from_input_csv_files(base_directory):\n    \"\"\"\n    This function reads every CSV file available and concatenates\n    them into a single dataframe.\n    \"\"\"\n\n    input_directory = Path(base_directory) / \"input\"\n    files = [file for file in input_directory.glob(\"*.csv\")]\n    \n    if len(files) == 0:\n        raise ValueError(f\"The are no CSV files in {str(input_directory)}/\")\n        \n    raw_data = [pd.read_csv(file) for file in files]\n    df = pd.concat(raw_data)\n    \n    # Shuffle the data\n    return df.sample(frac=1, random_state=42)\n\n\ndef _split_data(df):\n    \"\"\"\n    Splits the data into three sets: train, validation and test.\n    \"\"\"\n\n    df_train, temp = train_test_split(df, test_size=0.3)\n    df_validation, df_test = train_test_split(temp, test_size=0.5)\n\n    return df_train, df_validation, df_test\n\n\ndef _save_baselines(base_directory, df_train, df_test):\n    \"\"\"\n    During the data and quality monitoring steps, we will need baselines\n    to compute constraints and statistics. This function saves the \n    untransformed data to disk so we can use them as baselines later.\n    \"\"\"\n\n    for split, data in [(\"train\", df_train), (\"test\", df_test)]:\n        baseline_path = Path(base_directory) / f\"{split}-baseline\"\n        baseline_path.mkdir(parents=True, exist_ok=True)\n\n        df = data.copy().dropna()\n\n        # We want to save the header only for the train baseline\n        # but not for the test baseline. We'll use the test baseline\n        # to generate predictions later, and we can't have a header line\n        # because the model won't be able to make a prediction for it.\n        header = split == \"train\"\n        df.to_csv(baseline_path / f\"{split}-baseline.csv\", header=header, index=False)\n\n\ndef _save_splits(base_directory, X_train, y_train, X_validation, y_validation, X_test, y_test):\n    \"\"\"\n    This function concatenates the transformed features and the target variable, and\n    saves each one of the split sets to disk.\n    \"\"\"\n\n    train = np.concatenate((X_train, y_train), axis=1)\n    validation = np.concatenate((X_validation, y_validation), axis=1)\n    test = np.concatenate((X_test, y_test), axis=1)\n\n    train_path = Path(base_directory) / \"train\"\n    validation_path = Path(base_directory) / \"validation\"\n    test_path = Path(base_directory) / \"test\"\n\n    train_path.mkdir(parents=True, exist_ok=True)\n    validation_path.mkdir(parents=True, exist_ok=True)\n    test_path.mkdir(parents=True, exist_ok=True)\n\n    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n    pd.DataFrame(validation).to_csv(validation_path / \"validation.csv\", header=False, index=False)\n    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n\n\ndef _save_model(base_directory, target_transformer, features_transformer):\n    \"\"\"\n    This function creates a model.tar.gz file that contains the two transformation\n    pipelines we built to transform the data.\n    \"\"\"\n\n    with tempfile.TemporaryDirectory() as directory:\n        joblib.dump(target_transformer, os.path.join(directory, \"target.joblib\"))\n        joblib.dump(features_transformer, os.path.join(directory, \"features.joblib\"))\n    \n        model_path = Path(base_directory) / \"model\"\n        model_path.mkdir(parents=True, exist_ok=True)\n    \n        with tarfile.open(f\"{str(model_path / 'model.tar.gz')}\", \"w:gz\") as tar:\n            tar.add(os.path.join(directory, \"target.joblib\"), arcname=\"target.joblib\")\n            tar.add(os.path.join(directory, \"features.joblib\"), arcname=\"features.joblib\")\n\n    \nif __name__ == \"__main__\":\n    preprocess(base_directory=\"/opt/ml/processing\")\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nimport os\nimport shutil\nimport tarfile\nimport pytest\nimport tempfile\nimport joblib\nfrom preprocessor import preprocess\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    preprocess(base_directory=directory)\n    \n    yield directory\n    \n    shutil.rmtree(directory)\n\n\ndef test_preprocess_generates_data_splits(directory):\n    output_directories = os.listdir(directory)\n    \n    assert \"train\" in output_directories\n    assert \"validation\" in output_directories\n    assert \"test\" in output_directories\n\n\ndef test_preprocess_generates_baselines(directory):\n    output_directories = os.listdir(directory)\n\n    assert \"train-baseline\" in output_directories\n    assert \"test-baseline\" in output_directories\n\n\ndef test_preprocess_creates_two_models(directory):\n    model_path = directory / \"model\"\n    tar = tarfile.open(model_path / \"model.tar.gz\", \"r:gz\")\n\n    assert \"features.joblib\" in tar.getnames()\n    assert \"target.joblib\" in tar.getnames()\n\n\ndef test_splits_are_transformed(directory):\n    train = pd.read_csv(directory / \"train\" / \"train.csv\", header=None)\n    validation = pd.read_csv(directory / \"validation\" / \"validation.csv\", header=None)\n    test = pd.read_csv(directory / \"test\" / \"test.csv\", header=None)\n\n    # After transforming the data, the number of features should be 7:\n    # * 3 - island (one-hot encoded)\n    # * 1 - culmen_length_mm = 1\n    # * 1 - culmen_depth_mm\n    # * 1 - flipper_length_mm\n    # * 1 - body_mass_g\n    number_of_features = 7\n\n    # The transformed splits should have an additional column for the target\n    # variable.\n    assert train.shape[1] == number_of_features + 1\n    assert validation.shape[1] == number_of_features + 1\n    assert test.shape[1] == number_of_features + 1\n\n\ndef test_train_baseline_is_not_transformed(directory):\n    baseline = pd.read_csv(directory / \"train-baseline\" / \"train-baseline.csv\", header=None)\n\n    island = baseline.iloc[:, 1].unique()\n\n    assert \"Biscoe\" in island\n    assert \"Torgersen\" in island\n    assert \"Dream\" in island\n\n\ndef test_test_baseline_is_not_transformed(directory):\n    baseline = pd.read_csv(directory / \"test-baseline\" / \"test-baseline.csv\", header=None)\n\n    island = baseline.iloc[:, 1].unique()\n\n    assert \"Biscoe\" in island\n    assert \"Torgersen\" in island\n    assert \"Dream\" in island\n\n\ndef test_train_baseline_includes_header(directory):\n    baseline = pd.read_csv(directory / \"train-baseline\" / \"train-baseline.csv\")\n    assert baseline.columns[0] == \"species\"\n\n\ndef test_test_baseline_does_not_include_header(directory):\n    baseline = pd.read_csv(directory / \"test-baseline\" / \"test-baseline.csv\")\n    assert baseline.columns[0] != \"species\"\n\n\n\n\nStep 3 - Setting up the Processing Step\nLet’s now define the ProcessingStep that we’ll use in the pipeline to run the script that will split and transform the data.\nSeveral SageMaker Pipeline steps support caching. When a step runs, and dependending on the configured caching policy, SageMaker will try to reuse the result of a previous successful run of the same step. You can find more information about this topic in Caching Pipeline Steps. Let’s define a caching policy that we’ll reuse on every step:\n\nfrom sagemaker.workflow.steps import CacheConfig\n\ncache_config = CacheConfig(enable_caching=True, expire_after=\"15d\")\n\nWe can parameterize a SageMaker Pipeline to make it more flexible. In this case, we’ll use a paramater to pass the location of the dataset we want to process. We can execute the pipeline with different datasets by changing the value of this parameter. To read more about these parameters, check Pipeline Parameters.\n\nfrom sagemaker.workflow.parameters import ParameterString\n\ndataset_location = ParameterString(\n    name=\"dataset_location\",\n    default_value=f\"{S3_LOCATION}/data\",\n)\n\nA processor gives the Processing Step information about the hardware and software that SageMaker should use to launch the Processing Job. To run the script we created, we need access to Scikit-Learn, so we can use the SKLearnProcessor processor that comes out-of-the-box with the SageMaker’s Python SDK. The Data Processing with Framework Processors page discusses other built-in processors you can use. The Docker Registry Paths and Example Code page contains information about the available framework versions for each region.\n\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nprocessor = SKLearnProcessor(\n    base_job_name=\"split-and-transform-data\",\n    framework_version=\"1.2-1\",\n    # By default, a new account doesn't have access to `ml.m5.xlarge` instances.\n    # If you haven't requested a quota increase yet, you can use an\n    # `ml.t3.medium` instance type instead. This will work out of the box, but\n    # the Processing Job will take significantly longer than it should have.\n    # To get access to `ml.m5.xlarge` instances, you can request a quota\n    # increase under the Service Quotas section in your AWS account.\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    role=role,\n    sagemaker_session=config[\"session\"],\n)\n\nINFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n\n\nLet’s now define the Processing Step that we’ll use in the pipeline. This step requires a list of inputs that we need on the preprocessing script. In this case, the input is the dataset we stored in S3. We also have a few outputs that we want SageMaker to capture when the Processing Job finishes.\n\nfrom sagemaker.workflow.steps import ProcessingStep\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\n\nsplit_and_transform_data_step = ProcessingStep(\n    name=\"split-and-transform-data\",\n    step_args=processor.run(\n        code=f\"{CODE_FOLDER}/preprocessor.py\",\n        inputs=[\n            ProcessingInput(\n                source=dataset_location, destination=\"/opt/ml/processing/input\"\n            ),\n        ],\n        outputs=[\n            ProcessingOutput(\n                output_name=\"train\",\n                source=\"/opt/ml/processing/train\",\n                destination=f\"{S3_LOCATION}/preprocessing/train\",\n            ),\n            ProcessingOutput(\n                output_name=\"validation\",\n                source=\"/opt/ml/processing/validation\",\n                destination=f\"{S3_LOCATION}/preprocessing/validation\",\n            ),\n            ProcessingOutput(\n                output_name=\"test\",\n                source=\"/opt/ml/processing/test\",\n                destination=f\"{S3_LOCATION}/preprocessing/test\",\n            ),\n            ProcessingOutput(\n                output_name=\"model\",\n                source=\"/opt/ml/processing/model\",\n                destination=f\"{S3_LOCATION}/preprocessing/model\",\n            ),\n            ProcessingOutput(\n                output_name=\"train-baseline\",\n                source=\"/opt/ml/processing/train-baseline\",\n                destination=f\"{S3_LOCATION}/preprocessing/train-baseline\",\n            ),\n            ProcessingOutput(\n                output_name=\"test-baseline\",\n                source=\"/opt/ml/processing/test-baseline\",\n                destination=f\"{S3_LOCATION}/preprocessing/test-baseline\",\n            ),\n        ],\n    ),\n    cache_config=cache_config,\n)\n\n\n\nStep 4 - Creating the Pipeline\nWe can now create the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nfrom sagemaker.workflow.pipeline import Pipeline\nfrom sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n\npipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n\nsession1_pipeline = Pipeline(\n    name=\"session1-pipeline\",\n    parameters=[dataset_location],\n    steps=[\n        split_and_transform_data_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession1_pipeline.upsert(role_arn=role)\n\nWe can now start the pipeline:\n\nsession1_pipeline.start()\n\n\n\nAssignments\n\nAssignment 1.1 The SageMaker Pipeline we built supports running a few steps in Local Mode. The goal of this assignment is to run the pipeline on your local environment using Local Mode.\nAssignment 1.2 For this assignment, we want to run the end-to-end pipeline in SageMaker Studio. Ensure you turn off Local Mode before doing so.\nAssignment 1.3 The pipeline uses Random Sampling to split the dataset. Modify the code to use Stratified Sampling instead.\nAssignment 1.4 For this assignment, we want to run a distributed Processing Job across multiple instances to capitalize the island column of the dataset. Your dataset will consist of 10 different files stored in S3. Set up a Processing Job using two instances. When specifying the input to the Processing Job, you must set the ProcessingInput.s3_data_distribution_type attribute to ShardedByS3Key. By doing this, SageMaker will run a cluster with two instances simultaneously, each with access to half the files.\nAssignment 1.5 You can use Amazon SageMaker Data Wrangler to complete each step of the data preparation workflow (including data selection, cleansing, exploration, visualization, and processing at scale) from a single visual interface. For this assignment, load the Data Wrangler interface and use it to build the same transformations we implemented using the Scikit-Learn Pipeline. If you have questions, open the Penguins Data Flow included in this repository."
  },
  {
    "objectID": "cohort.html#session-2---building-models-and-the-training-pipeline",
    "href": "cohort.html#session-2---building-models-and-the-training-pipeline",
    "title": "Building Production Machine Learning Systems",
    "section": "Session 2 - Building Models And The Training Pipeline",
    "text": "Session 2 - Building Models And The Training Pipeline\nThis session extends the SageMaker Pipeline we built in the previous session with a step to train a model. We’ll explore the Training Step and the Tuning Step.\n \nWe’ll introduce Amazon SageMaker Experiments and use them during training. For more information about this topic, check the SageMaker Experiments’ SDK documentation.\n\nStep 1 - Creating the Training Script\nThis following script is responsible for training a neural network using the train data, validating the model, and saving it so we can later use it:\n\n\n\ntrain.py\n\nimport os\nimport argparse\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom pathlib import Path\nfrom sklearn.metrics import accuracy_score\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\n\n\ndef train(model_directory, train_path, validation_path, epochs=50, batch_size=32):\n    X_train = pd.read_csv(Path(train_path) / \"train.csv\")\n    y_train = X_train[X_train.columns[-1]]\n    X_train.drop(X_train.columns[-1], axis=1, inplace=True)\n    \n    X_validation = pd.read_csv(Path(validation_path) / \"validation.csv\")\n    y_validation = X_validation[X_validation.columns[-1]]\n    X_validation.drop(X_validation.columns[-1], axis=1, inplace=True)\n        \n    model = Sequential([\n        Dense(10, input_shape=(X_train.shape[1],), activation=\"relu\"),\n        Dense(8, activation=\"relu\"),\n        Dense(3, activation=\"softmax\"),\n    ])\n    \n    model.compile(\n        optimizer=SGD(learning_rate=0.01),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n\n    model.fit(\n        X_train, \n        y_train, \n        validation_data=(X_validation, y_validation),\n        epochs=epochs, \n        batch_size=batch_size,\n        verbose=2,\n    )\n\n    predictions = np.argmax(model.predict(X_validation), axis=-1)\n    print(f\"Validation accuracy: {accuracy_score(y_validation, predictions)}\")\n    \n    model_filepath = Path(model_directory) / \"001\"\n    model.save(model_filepath)    \n    \n\nif __name__ == \"__main__\":\n    # Any hyperparameters provided by the training job are passed to \n    # the entry point as script arguments. \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--epochs\", type=int, default=50)\n    parser.add_argument(\"--batch_size\", type=int, default=32)\n    args, _ = parser.parse_known_args()\n    \n\n    train(\n        # This is the location where we need to save our model. SageMaker will\n        # create a model.tar.gz file with anything inside this directory when\n        # the training script finishes.\n        model_directory=os.environ[\"SM_MODEL_DIR\"],\n\n        # SageMaker creates one channel for each one of the inputs to the\n        # Training Step.\n        train_path=os.environ[\"SM_CHANNEL_TRAIN\"],\n        validation_path=os.environ[\"SM_CHANNEL_VALIDATION\"],\n\n        epochs=args.epochs,\n        batch_size=args.batch_size,\n    )\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nimport os\nimport shutil\nimport tarfile\nimport pytest\nimport tempfile\nimport joblib\n\nfrom preprocessor import preprocess\nfrom train import train\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    \n    preprocess(base_directory=directory)\n    train(\n        model_directory=directory / \"model\",\n        train_path=directory / \"train\", \n        validation_path=directory / \"validation\",\n        epochs=1\n    )\n    \n    yield directory\n    \n    shutil.rmtree(directory)\n\n\ndef test_train_saves_a_folder_with_model_assets(directory):\n    output = os.listdir(directory / \"model\")\n    assert \"001\" in output\n    \n    assets = os.listdir(directory / \"model\" / \"001\")\n    assert \"saved_model.pb\" in assets\n\n\n\n\nStep 2 - Setting up the Training Step\nWe can now create a Training Step that we can add to the pipeline. This Training Step will create a SageMaker Training Job in the background, run the training script, and upload the output to S3. Check the TrainingStep SageMaker’s SDK documentation for more information.\nSageMaker uses the concept of an Estimator to handle end-to-end training and deployment tasks. For this example, we will use the built-in TensorFlow Estimator to run the training script we wrote before. The Docker Registry Paths and Example Code page contains information about the available framework versions for each region. Here, you can also check the available SageMaker Deep Learning Container images.\nNotice the list of hyperparameters defined below. SageMaker will pass these hyperparameters as arguments to the entry point of the training script.\nWe are going to use SageMaker Experiments to log information from the Training Job. For more information, check Manage Machine Learning with Amazon SageMaker Experiments. The list of metric definitions will tell SageMaker which metrics to track and how to parse them from the Training Job logs.\n\nfrom sagemaker.tensorflow import TensorFlow\n\nestimator = TensorFlow(\n    base_job_name=\"training\",\n    entry_point=f\"{CODE_FOLDER}/train.py\",\n    # SageMaker will pass these hyperparameters as arguments\n    # to the entry point of the training script.\n    hyperparameters={\n        \"epochs\": 50,\n        \"batch_size\": 32,\n    },\n    # SageMaker will track these metrics as part of the experiment\n    # associated to this pipeline. The metric definitions tells\n    # SageMaker how to parse the values from the Training Job logs.\n    metric_definitions=[\n        {\"Name\": \"loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\"},\n        {\"Name\": \"accuracy\", \"Regex\": \"accuracy: ([0-9\\\\.]+)\"},\n        {\"Name\": \"val_loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n        {\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"},\n    ],\n    image_uri=config[\"image\"],\n    framework_version=config[\"framework_version\"],\n    py_version=config[\"py_version\"],\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    disable_profiler=True,\n    output_path=f\"{S3_LOCATION}/model\",\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\nWe can now create a Training Step. This Training Step will create a SageMaker Training Job in the background, run the training script, and upload the output to S3. Check the TrainingStep SageMaker’s SDK documentation for more information.\nThis step will receive the train and validation split from the previous step as inputs.\nHere, we are using two input channels, train and validation. SageMaker will automatically create an environment variable corresponding to each of these channels following the format SM_CHANNEL_[channel_name]:\n\nSM_CHANNEL_TRAIN: This environment variable will contain the path to the data in the train channel\nSM_CHANNEL_VALIDATION: This environment variable will contain the path to the data in the validation channel\n\n\nfrom sagemaker.workflow.steps import TrainingStep\nfrom sagemaker.inputs import TrainingInput\n\n\ntrain_model_step = TrainingStep(\n    name=\"train-model\",\n    step_args=estimator.fit(\n        inputs={\n            \"train\": TrainingInput(\n                s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n                    \"train\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\",\n            ),\n            \"validation\": TrainingInput(\n                s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n                    \"validation\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\",\n            ),\n        }\n    ),\n    cache_config=cache_config,\n)\n\n\n\nStep 3 - Setting up a Tuning Step\nLet’s now create a Tuning Step. This Tuning Step will create a SageMaker Hyperparameter Tuning Job in the background and use the training script to train different model variants and choose the best one. Check the TuningStep SageMaker’s SDK documentation for more information.\nSince we could use the Training of the Tuning Step to create the model, we’ll define this constant to indicate which approach we want to run.\n\nUSE_TUNING_STEP = False\n\nThe Tuning Step requires a HyperparameterTuner reference to configure the Hyperparameter Tuning Job.\nHere is the configuration that we’ll use to find the best model:\n\nobjective_metric_name: This is the name of the metric the tuner will use to determine the best model.\nobjective_type: This is the objective of the tuner. Should it “Minimize” the metric or “Maximize” it? In this example, since we are using the validation accuracy of the model, we want the objective to be “Maximize.” If we were using the loss of the model, we would set the objective to “Minimize.”\nmetric_definitions: Defines how the tuner will determine the metric’s value by looking at the output logs of the training process.\n\nThe tuner expects the list of the hyperparameters you want to explore. You can use subclasses of the Parameter class to specify different types of hyperparameters. This example explores different values for the epochs hyperparameter.\nFinally, you can control the number of jobs and how many of them will run in parallel using the following two arguments:\n\nmax_jobs: Defines the maximum total number of training jobs to start for the hyperparameter tuning job.\nmax_parallel_jobs: Defines the maximum number of parallel training jobs to start.\n\n\nfrom sagemaker.tuner import HyperparameterTuner\nfrom sagemaker.parameter import IntegerParameter\n\ntuner = HyperparameterTuner(\n    estimator,\n    objective_metric_name=\"val_accuracy\",\n    objective_type=\"Maximize\",\n    hyperparameter_ranges={\n        \"epochs\": IntegerParameter(10, 50),\n    },\n    metric_definitions=[{\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}],\n    max_jobs=3,\n    max_parallel_jobs=3,\n)\n\nWe can now create the Tuning Step using the tuner we configured before:\n\nfrom sagemaker.workflow.steps import TuningStep\n\ntune_model_step = TuningStep(\n    name=\"tune-model\",\n    step_args=tuner.fit(\n        inputs={\n            \"train\": TrainingInput(\n                s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n                    \"train\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\",\n            ),\n            \"validation\": TrainingInput(\n                s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n                    \"validation\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\",\n            ),\n        },\n    ),\n    cache_config=cache_config,\n)\n\n\n\nStep 4 - Creating the Pipeline\nLet’s define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession2_pipeline = Pipeline(\n    name=\"session2-pipeline\",\n    parameters=[dataset_location],\n    steps=[\n        split_and_transform_data_step,\n        tune_model_step if USE_TUNING_STEP else train_model_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession2_pipeline.upsert(role_arn=role)\n\nWe can now start the pipeline:\n\nsession2_pipeline.start()\n\n\n\nAssignments\n\nAssignment 2.1 The training script trains the model using a hard-coded learning rate value. Modify the code to accept the learning rate as a parameter we can control from outside the script.\nAssignment 2.2 We currently define the number of epochs to train the model as a constant that we pass to the Estimator using the list of hyperparameters. Replace this constant with a new Pipeline Parameter named training_epochs. You’ll need to specify this new parameter when creating the Pipeline.\nAssignment 2.3 The current tuning process aims to find the model with the highest validation accuracy. Modify the code to focus on the model with the lowest training loss.\nAssignment 2.4 We used an instance of SKLearnProcessor to run the script that transforms and splits the data, but there’s no way to add additional dependencies to the processing container. Modify the code to use an instance of FrameworkProcessor instead. This class will allow you to specify a directory containing a requirements.txt file containing a list of dependencies. SageMaker will install these libraries in the processing container before triggering the processing job.\nAssignment 2.5 We want to execute the pipeline whenever the dataset changes. We can accomplish this by using Amazon EventBridge. Configure an event to automatically start the pipeline when a new file is added to the S3 bucket where we store our dataset. Check Amazon EventBridge Integration for an implementation tutorial."
  },
  {
    "objectID": "cohort.html#session-3---evaluating-and-versioning-models",
    "href": "cohort.html#session-3---evaluating-and-versioning-models",
    "title": "Building Production Machine Learning Systems",
    "section": "Session 3 - Evaluating and Versioning Models",
    "text": "Session 3 - Evaluating and Versioning Models\nThis session extends the SageMaker Pipeline with a step to evaluate the model and register it if it reaches a predefined accuracy threshold.\n \nWe’ll use a Processing Step to execute an evaluation script. We’ll use a Condition Step to determine whether the model’s accuracy is above a threshold, and a Model Step to register the model in the SageMaker Model Registry.\n\nStep 1 - Creating the Evaluation Script\nLet’s create the evaluation script. The Processing Step will spin up a Processing Job and run this script inside a container. This script is responsible for loading the model we created and evaluating it on the test set. Before finishing, this script will generate an evaluation report of the model.\n\n\n\nevaluation.py\n\nimport os\nimport json\nimport tarfile\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom tensorflow import keras\nfrom sklearn.metrics import accuracy_score\n\n\nMODEL_PATH = \"/opt/ml/processing/model/\"\nTEST_PATH = \"/opt/ml/processing/test/\"\nOUTPUT_PATH = \"/opt/ml/processing/evaluation/\"\n\n\ndef evaluate(model_path, test_path, output_path):\n    # The first step is to extract the model package so we can load \n    # it in memory.\n    with tarfile.open(Path(model_path) / \"model.tar.gz\") as tar:\n        tar.extractall(path=Path(model_path))\n        \n    model = keras.models.load_model(Path(model_path) / \"001\")\n    \n    X_test = pd.read_csv(Path(test_path) / \"test.csv\")\n    y_test = X_test[X_test.columns[-1]]\n    X_test.drop(X_test.columns[-1], axis=1, inplace=True)\n    \n    predictions = np.argmax(model.predict(X_test), axis=-1)\n    accuracy = accuracy_score(y_test, predictions)\n    print(f\"Test accuracy: {accuracy}\")\n\n    # Let's create an evaluation report using the model accuracy.\n    evaluation_report = {\n        \"metrics\": {\n            \"accuracy\": {\n                \"value\": accuracy\n            },\n        },\n    }\n    \n    Path(output_path).mkdir(parents=True, exist_ok=True)\n    with open(Path(output_path) / \"evaluation.json\", \"w\") as f:\n        f.write(json.dumps(evaluation_report))\n        \n        \nif __name__ == \"__main__\":\n    evaluate(\n        model_path=MODEL_PATH, \n        test_path=TEST_PATH,\n        output_path=OUTPUT_PATH\n    )\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nimport os\nimport shutil\nimport tarfile\nimport pytest\nimport tempfile\nimport joblib\n\nfrom preprocessor import preprocess\nfrom train import train\nfrom evaluation import evaluate\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    \n    preprocess(base_directory=directory)\n    \n    train(\n        model_directory=directory / \"model\",\n        train_path=directory / \"train\", \n        validation_path=directory / \"validation\",\n        epochs=1\n    )\n    \n    # After training a model, we need to prepare a package just like\n    # SageMaker would. This package is what the evaluation script is\n    # expecting as an input.\n    with tarfile.open(directory / \"model.tar.gz\", \"w:gz\") as tar:\n        tar.add(directory / \"model\" / \"001\", arcname=\"001\")\n        \n    evaluate(\n        model_path=directory, \n        test_path=directory / \"test\",\n        output_path=directory / \"evaluation\",\n    )\n\n    yield directory / \"evaluation\"\n    \n    shutil.rmtree(directory)\n\n\ndef test_evaluate_generates_evaluation_report(directory):\n    output = os.listdir(directory)\n    assert \"evaluation.json\" in output\n\n\ndef test_evaluation_report_contains_accuracy(directory):\n    with open(directory / \"evaluation.json\", 'r') as file:\n        report = json.load(file)\n        \n    assert \"metrics\" in report\n    assert \"accuracy\" in report[\"metrics\"]\n\n\n\n\nStep 2 - Setting up the Evaluation Step\nTo run the evaluation script, we will use a Processing Step configured with TensorFlowProcessor because the script needs access to TensorFlow.\n\nfrom sagemaker.tensorflow import TensorFlowProcessor\n\ntensorflow_processor = TensorFlowProcessor(\n    base_job_name=\"evaluation-processor\",\n    image_uri=config[\"image\"],\n    framework_version=config[\"framework_version\"],\n    py_version=config[\"py_version\"],\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    role=role,\n    sagemaker_session=config[\"session\"],\n)\n\nOne of the inputs to the Evaluation Step will be the model assets. We can use the USE_TUNING_STEP flag to determine whether we created the model using a Training Step or a Tuning Step. In case we are using the Tuning Step, we can use the TuningStep.get_top_model_s3_uri() function to get the model assets from the top performing training job of the Hyperparameter Tuning Job.\n\nmodel_assets = train_model_step.properties.ModelArtifacts.S3ModelArtifacts\n\nif USE_TUNING_STEP:\n    model_assets = tune_model_step.get_top_model_s3_uri(top_k=0, s3_bucket=bucket)\n\nSageMaker supports mapping outputs to property files. This is useful when accessing a specific property from the pipeline. In our case, we want to access the accuracy of the model in the Condition Step, so we’ll map the evaluation report to a property file. Check How to Build and Manage Property Files for more information.\n\nfrom sagemaker.workflow.properties import PropertyFile\n\nevaluation_report = PropertyFile(\n    name=\"evaluation-report\", output_name=\"evaluation\", path=\"evaluation.json\"\n)\n\nWe are now ready to define the ProcessingStep that will run the evaluation script:\n\nevaluate_model_step = ProcessingStep(\n    name=\"evaluate-model\",\n    step_args=tensorflow_processor.run(\n        inputs=[\n            # The first input is the test split that we generated on\n            # the first step of the pipeline when we split and\n            # transformed the data.\n            ProcessingInput(\n                source=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n                    \"test\"\n                ].S3Output.S3Uri,\n                destination=\"/opt/ml/processing/test\",\n            ),\n            # The second input is the model that we generated on\n            # the Training or Tunning Step.\n            ProcessingInput(\n                source=model_assets,\n                destination=\"/opt/ml/processing/model\",\n            ),\n        ],\n        outputs=[\n            # The output is the evaluation report that we generated\n            # in the evaluation script.\n            ProcessingOutput(\n                output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"\n            ),\n        ],\n        code=f\"{CODE_FOLDER}/evaluation.py\",\n    ),\n    property_files=[evaluation_report],\n    cache_config=cache_config,\n)\n\n\n\nStep 3 - Registering the Model\nLet’s now create a new version of the model and register it in the Model Registry. Check Register a Model Version for more information about model registration.\nFirst, let’s define the name of the group where we’ll register the model:\n\nMODEL_PACKAGE_GROUP = \"penguins\"\n\nLet’s now create the model that we’ll register in the Model Registry. The model we trained uses TensorFlow, so we can use the built-in TensorFlowModel class to create an instance of the model:\n\nfrom sagemaker.tensorflow.model import TensorFlowModel\n\ntensorflow_model = TensorFlowModel(\n    model_data=model_assets,\n    image_uri=config[\"image\"],\n    framework_version=config[\"framework_version\"],\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\nWhen we register a model in the Model Registry, we can attach relevant metadata to it. We’ll use the evaluation report we generated during the Evaluation Step to populate the metrics of this model:\n\nfrom sagemaker.model_metrics import ModelMetrics, MetricsSource\nfrom sagemaker.workflow.functions import Join\n\nmodel_metrics = ModelMetrics(\n    model_statistics=MetricsSource(\n        s3_uri=Join(\n            on=\"/\",\n            values=[\n                evaluate_model_step.properties.ProcessingOutputConfig.Outputs[\n                    \"evaluation\"\n                ].S3Output.S3Uri,\n                \"evaluation.json\",\n            ],\n        ),\n        content_type=\"application/json\",\n    )\n)\n\nWe can use a Model Step to register the model. Check the ModelStep SageMaker’s SDK documentation for more information.\n\nfrom sagemaker.workflow.model_step import ModelStep\n\nregister_model_step = ModelStep(\n    name=\"register-model\",\n    step_args=tensorflow_model.register(\n        model_package_group_name=MODEL_PACKAGE_GROUP,\n        approval_status=\"Approved\",\n        model_metrics=model_metrics,\n        content_types=[\"text/csv\"],\n        response_types=[\"text/csv\"],\n        # This is the suggested inference instance types when\n        # deploying the model or using it as part of a batch\n        # transform job.\n        inference_instances=[\"ml.m5.xlarge\"],\n        transform_instances=[\"ml.m5.xlarge\"],\n        domain=\"MACHINE_LEARNING\",\n        task=\"CLASSIFICATION\",\n        framework=\"TENSORFLOW\",\n        framework_version=config[\"framework_version\"],\n    ),\n)\n\n\n\nStep 4 - Setting up a Condition Step\nWe only want to register a new model if its accuracy exceeds a predefined threshold. We can use a Condition Step together with the evaluation report we generated to accomplish this.\nLet’s define a new Pipeline Parameter to specify the minimum accuracy that the model should reach for it to be registered.\n\nfrom sagemaker.workflow.parameters import ParameterFloat\n\naccuracy_threshold = ParameterFloat(name=\"accuracy_threshold\", default_value=0.70)\n\nIf the model’s accuracy is not greater than or equal our threshold, we will send the pipeline to a Fail Step with the appropriate error message. Check the FailStep SageMaker’s SDK documentation for more information.\n\nfrom sagemaker.workflow.fail_step import FailStep\n\nfail_step = FailStep(\n    name=\"fail\",\n    error_message=Join(\n        on=\" \",\n        values=[\n            \"Execution failed because the model's accuracy was lower than\",\n            accuracy_threshold,\n        ],\n    ),\n)\n\nWe can use a ConditionGreaterThanOrEqualTo condition to compare the model’s accuracy with the threshold. Look at the Conditions section in the documentation for more information about the types of supported conditions.\n\nfrom sagemaker.workflow.functions import JsonGet\nfrom sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n\ncondition = ConditionGreaterThanOrEqualTo(\n    left=JsonGet(\n        step_name=evaluate_model_step.name,\n        property_file=evaluation_report,\n        json_path=\"metrics.accuracy.value\",\n    ),\n    right=accuracy_threshold,\n)\n\nLet’s now define the Condition Step:\n\nfrom sagemaker.workflow.condition_step import ConditionStep\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[condition],\n    if_steps=[register_model_step] if not LOCAL_MODE else [],\n    else_steps=[fail_step],\n)\n\n\n\nStep 5 - Creating the Pipeline\nWe can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession3_pipeline = Pipeline(\n    name=\"session3-pipeline\",\n    parameters=[dataset_location, accuracy_threshold],\n    steps=[\n        split_and_transform_data_step,\n        tune_model_step if USE_TUNING_STEP else train_model_step,\n        evaluate_model_step,\n        condition_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession3_pipeline.upsert(role_arn=role)\n\nWe can now start the pipeline:\n\nsession3_pipeline.start()\n\n\n\nAssignments\n\nAssignment 3.1 The evaluation script computes the accuracy of the model and exports it as part of the evaluation report. Extend the evaluation report by adding the precision and the recall of the model on each one of the classes.\nAssignment 3.2 Extend the evaluation script to test the model on each island separately. The evaluation report should contain the accuracy of the model on each island and the overall accuracy.\nAssignment 3.3 The Condition Step uses a hard-coded threshold value to determine if the model’s accuracy is good enough to proceed. Modify the code so the pipeline uses the accuracy of the latest registered model version as the threshold. We want to register a new model version only if its performance is better than the previous version we registered.\nAssignment 3.4 The current pipeline uses either a Training Step or a Tuning Step to build a model. Modify the pipeline to use both steps at the same time. The evaluation script should evaluate the model coming from the Training Step and the best model coming from the Tuning Step and output the accuracy and location in S3 of the best model. You should modify the code to register the model assets specified in the evaluation report.\nAssignment 3.5 Pipeline steps can encounter exceptions. In some cases, retrying can resolve these issues. For this assignment, configure the Processing Step so it automatically retries the step a maximum of 5 times if it encounters an InternalServerError. Check the Retry Policy for Pipeline Steps documentation for more information."
  },
  {
    "objectID": "cohort.html#session-4---deploying-models-and-serving-predictions",
    "href": "cohort.html#session-4---deploying-models-and-serving-predictions",
    "title": "Building Production Machine Learning Systems",
    "section": "Session 4 - Deploying Models and Serving Predictions",
    "text": "Session 4 - Deploying Models and Serving Predictions\nIn this session we’ll explore how to deploy a model to a SageMaker Endpoint and how to use a SageMaker Inference Pipeline to control the data that goes in and comes out of the endpoint.\n \nLet’s start by defining the name of the endpoint where we’ll deploy the model and creating a constant pointing to the location where we’ll store the data that the endpoint will capture:\n\nfrom sagemaker.predictor import Predictor\n\nENDPOINT = \"penguins-endpoint\"\nDATA_CAPTURE_DESTINATION = f\"{S3_LOCATION}/monitoring/data-capture\"\n\n\nStep 1 - Deploying Model From Registry\nLet’s manually deploy the latest model from the Model Registry to an endpoint.\nWe want to query the list of approved models from the Model Registry and get the last one:\n\nresponse = sagemaker_client.list_model_packages(\n    ModelPackageGroupName=MODEL_PACKAGE_GROUP,\n    ModelApprovalStatus=\"Approved\",\n    SortBy=\"CreationTime\",\n    MaxResults=1,\n)\n\npackage = (\n    response[\"ModelPackageSummaryList\"][0]\n    if response[\"ModelPackageSummaryList\"]\n    else None\n)\npackage\n\n{'ModelPackageGroupName': 'penguins',\n 'ModelPackageVersion': 67,\n 'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:325223348818:model-package/penguins/67',\n 'CreationTime': datetime.datetime(2023, 10, 17, 17, 7, 1, 325000, tzinfo=tzlocal()),\n 'ModelPackageStatus': 'Completed',\n 'ModelApprovalStatus': 'Approved'}\n\n\nWe can now create a Model Package using the ARN of the model from the Model Registry:\n\nfrom sagemaker import ModelPackage\n\nmodel_package = ModelPackage(\n    model_package_arn=package[\"ModelPackageArn\"],\n    sagemaker_session=sagemaker_session,\n    role=role,\n)\n\nLet’s now deploy the model to an endpoint:\n\nmodel_package.deploy(\n    endpoint_name=ENDPOINT, \n    initial_instance_count=1, \n    instance_type=config[\"instance_type\"]\n)\n\nAfter deploying the model, we can test the endpoint to make sure it works.\nEach line of the payload we’ll send to the endpoint contains the information of a penguin. Notice the model expects data that’s already transformed. We can’t provide the original data from our dataset because the model we registered will not work with it.\nThe endpoint will return the predictions for each of these lines.\n\npayload = \"\"\"\n0.6569590202313976,-1.0813829646495108,1.2097102831892812,0.9226343641317372,1.0,0.0,0.0\n-0.7751048801481084,0.8822689351285553,-1.2168066120762704,0.9226343641317372,0.0,1.0,0.0\n-0.837387834894918,0.3386660813829646,-0.26237731892812,-1.92351941317372,0.0,0.0,1.0\n\"\"\"\n\nLet’s send the payload to the endpoint and print its response:\n\npredictor = Predictor(endpoint_name=ENDPOINT)\n\ntry:\n    response = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n    response = json.loads(response.decode(\"utf-8\"))\n\n    print(json.dumps(response, indent=2))\n    print(f\"\\nSpecies: {np.argmax(response['predictions'], axis=1)}\")\nexcept Exception as e:\n    print(e)\n\nAn error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint penguins-endpoint of account 325223348818 not found.\n\n\nAfter testing the endpoint, we need to ensure we delete it:\n\npredictor.delete_endpoint()\n\nDeploying the model we trained directly to an endpoint doesn’t lets us control the data that goes in and comes out of the endpoint. The TensorFlow model we trained requires transformed data, which makes it useless to other applications. Fortunately, we can create an Inference Pipeline using SageMaker to control the data that goes in and comes out of the endpoint.\nOur inference pipeline will have three components:\n\nA preprocessing transformer that will transform the input data into the format the model expects.\nThe TensorFlow model we trained.\nA postprocessing transformer that will transform the output of the model into a human-readable format.\n\nWe want our endpoint to handle unprocessed data in CSV and JSON format and return the penguin’s species. Here is an example of the payload input we want the endpoint to support:\n{\n    \"island\": \"Biscoe\",\n    \"culmen_length_mm\": 48.6,\n    \"culmen_depth_mm\": 16.0,\n    \"flipper_length_mm\": 230.0,\n    \"body_mass_g\": 5800.0,\n}\nAnd here is an example of the output we’d like to get from the endpoint:\n{\n    \"prediction\": \"Adelie\",\n    \"confidence\": 0.802672\n}\n\n\nStep 2 - Creating the Preprocessing Script\nThe first component of our inference pipeline will transform the input data into the format the model expects. We’ll use the Scikit-Learn transformer we saved when we split and transformed the data. To deploy this component as part of an inference pipeline, we need to write a script that loads the transformer, uses it to modify the input data, and returns the output in the format the TensorFlow model expects.\n\n\n\npreprocessing_component.py\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport json\nimport joblib\n\nfrom io import StringIO\n\ntry:\n    from sagemaker_containers.beta.framework import encoders, worker\nexcept ImportError:\n    # We don't have access to the `worker` instance when testing locally. \n    # We'll set it to None so we can change the way functions create a response.\n    worker = None\n\n\nTARGET_COLUMN = \"species\"\nFEATURE_COLUMNS = [\n    \"island\",\n    \"culmen_length_mm\",\n    \"culmen_depth_mm\", \n    \"flipper_length_mm\",\n    \"body_mass_g\",\n    \"sex\"\n]\n\n\ndef input_fn(input_data, content_type):\n    \"\"\"\n    Parses the input payload and creates a Pandas DataFrame.\n    \n    This function will check whether the target column is present in the\n    input data, and will remove it.\n    \"\"\"\n    \n    if content_type == \"text/csv\":\n        df = pd.read_csv(StringIO(input_data), header=None, skipinitialspace=True)\n\n        if len(df.columns) == len(FEATURE_COLUMNS) + 1:\n            df = df.drop(df.columns[0], axis=1)\n        \n        df.columns = FEATURE_COLUMNS\n        return df\n    \n    if content_type == \"application/json\":\n        df = pd.DataFrame([json.loads(input_data)])\n        \n        if \"species\" in df.columns:\n            df = df.drop(\"species\", axis=1)\n        \n        return df\n    \n    else:\n        raise ValueError(f\"{content_type} is not supported.!\")\n\n\ndef output_fn(prediction, accept):\n    \"\"\"\n    Formats the prediction output to generate a response.\n    \n    The default accept/content-type between containers for serial inference is JSON. \n    Since this model will preceed a TensorFlow model, we want to return a JSON object\n    following TensorFlow's input requirements.\n    \"\"\"\n    \n    if prediction is None:\n        raise Exception(f\"There was an error transforming the input data\")\n\n    if accept == \"text/csv\":\n        return worker.Response(encoders.encode(prediction, accept), mimetype=accept) if worker else prediction, accept \n    \n    if accept == \"application/json\":\n        instances = [p for p in prediction.tolist()]\n        response = {\"instances\": instances}\n        return worker.Response(json.dumps(response), mimetype=accept) if worker else (response, accept)\n\n    raise Exception(f\"{accept} accept type is not supported.\")\n\n\ndef predict_fn(input_data, model):\n    \"\"\"\n    Preprocess the input using the transformer.\n    \"\"\"\n    \n    try:\n        response = model.transform(input_data)\n        return response\n    except ValueError as e:\n        print(\"Error transforming the input data\", e)\n        return None\n\n\ndef model_fn(model_dir):\n    \"\"\"\n    Deserializes the model that will be used in this container.\n    \"\"\"\n    \n    return joblib.load(os.path.join(model_dir, \"features.joblib\"))\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nfrom preprocessing_component import input_fn, predict_fn, output_fn, model_fn\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    \n    preprocess(base_directory=directory)\n    \n    with tarfile.open(directory / \"model\" / \"model.tar.gz\") as tar:\n        tar.extractall(path=directory / \"model\")\n    \n    yield directory / \"model\"\n    \n    shutil.rmtree(directory)\n\n\n\ndef test_input_csv_drops_target_column_if_present():\n    input_data = \"\"\"\n    Adelie, Torgersen, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    df = input_fn(input_data, \"text/csv\")\n    assert len(df.columns) == 6 and \"species\" not in df.columns\n\n\ndef test_input_json_drops_target_column_if_present():\n    input_data = json.dumps({\n        \"species\": \"Adelie\", \n        \"island\": \"Torgersen\",\n        \"culmen_length_mm\": 44.1,\n        \"culmen_depth_mm\": 18.0,\n        \"flipper_length_mm\": 210.0,\n        \"body_mass_g\": 4000.0,\n        \"sex\": \"MALE\"\n    })\n    \n    df = input_fn(input_data, \"application/json\")\n    assert len(df.columns) == 6 and \"species\" not in df.columns\n\n\ndef test_input_csv_works_without_target_column():\n    input_data = \"\"\"\n    Torgersen, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    df = input_fn(input_data, \"text/csv\")\n    assert len(df.columns) == 6\n\n\ndef test_input_json_works_without_target_column():\n    input_data = json.dumps({\n        \"island\": \"Torgersen\",\n        \"culmen_length_mm\": 44.1,\n        \"culmen_depth_mm\": 18.0,\n        \"flipper_length_mm\": 210.0,\n        \"body_mass_g\": 4000.0,\n        \"sex\": \"MALE\"\n    })\n    \n    df = input_fn(input_data, \"application/json\")\n    assert len(df.columns) == 6\n\n\ndef test_output_csv_raises_exception_if_prediction_is_none():\n    with pytest.raises(Exception):\n        output_fn(None, \"text/csv\")\n    \n    \ndef test_output_json_raises_exception_if_prediction_is_none():\n    with pytest.raises(Exception):\n        output_fn(None, \"application/json\")\n    \n    \ndef test_output_csv_returns_prediction():\n    prediction = np.array([\n        [-1.3944109908736013,1.15488062669371,-0.7954340636549508,-0.5536447804097907,0.0,1.0,0.0],\n        [1.0557485835338234,0.5040085971987002,-0.5824506029515057,-0.5851840035995248,0.0,1.0,0.0]\n    ])\n    \n    response = output_fn(prediction, \"text/csv\")\n    \n    assert response == (prediction, \"text/csv\")\n    \n    \ndef test_output_json_returns_tensorflow_ready_input():\n    prediction = np.array([\n        [-1.3944109908736013,1.15488062669371,-0.7954340636549508,-0.5536447804097907,0.0,1.0,0.0],\n        [1.0557485835338234,0.5040085971987002,-0.5824506029515057,-0.5851840035995248,0.0,1.0,0.0]\n    ])\n    \n    response = output_fn(prediction, \"application/json\")\n    \n    assert response[0] == {\n        \"instances\": [\n            [-1.3944109908736013,1.15488062669371,-0.7954340636549508,-0.5536447804097907,0.0,1.0,0.0],\n            [1.0557485835338234,0.5040085971987002,-0.5824506029515057,-0.5851840035995248,0.0,1.0,0.0]\n        ]\n    }\n    \n    assert response[1] == \"application/json\"\n\n    \ndef test_predict_transforms_data(directory):\n    input_data = \"\"\"\n    Torgersen, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    model = model_fn(str(directory))\n    df = input_fn(input_data, \"text/csv\")\n    response = predict_fn(df, model)\n    assert type(response) is np.ndarray\n    \n\ndef test_predict_returns_none_if_invalid_input(directory):\n    input_data = \"\"\"\n    Invalid, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    model = model_fn(str(directory))\n    df = input_fn(input_data, \"text/csv\")\n    assert predict_fn(df, model) is None\n\n\n\n\nStep 3 - Creating the Postprocessing Script\nThe final component of our inference pipeline will transform the output from the model into a human-readable format. We’ll use the Scikit-Learn target transformer we saved when we split and transformed the data. To deploy this component as part of an inference pipeline, we need to write a script that loads the transformer, uses it to modify the output from the model, and returns a human-readable format.\n\n\n\npostprocessing_component.py\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport json\nimport tarfile\nimport joblib\n\nfrom pathlib import Path\nfrom io import StringIO\n\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, OrdinalEncoder\nfrom pickle import dump, load\n\n\ntry:\n    from sagemaker_containers.beta.framework import encoders, worker\nexcept ImportError:\n    # We don't have access to the `worker` instance when testing locally. \n    # We'll set it to None so we can change the way functions create a response.\n    worker = None\n\n\ndef input_fn(input_data, content_type):\n    if content_type == \"application/json\":\n        predictions = json.loads(input_data)[\"predictions\"]\n        return predictions\n    \n    else:\n        raise ValueError(f\"{content_type} is not supported.!\")\n\n\ndef output_fn(prediction, accept):\n    if accept == \"text/csv\":\n        return worker.Response(encoders.encode(prediction, accept), mimetype=accept) if worker else (prediction, accept)\n    \n    if accept == \"application/json\":\n        response = []\n        for p, c in prediction:\n            response.append({\n                \"prediction\": p,\n                \"confidence\": c\n            })\n\n        # If there's only one prediction, we'll return it\n        # as a single object.\n        if len(response) == 1:\n            response = response[0]\n            \n        return worker.Response(json.dumps(response), mimetype=accept) if worker else (response, accept)\n    \n    raise RuntimeException(f\"{accept} accept type is not supported.\")\n\n\ndef predict_fn(input_data, model):\n    \"\"\"\n    Transforms the prediction into its corresponding category.\n    \"\"\"\n\n    predictions = np.argmax(input_data, axis=-1)\n    confidence = np.max(input_data, axis=-1)\n    return [(model[prediction], confidence) for confidence, prediction in zip(confidence, predictions)]\n\n\ndef model_fn(model_dir):\n    \"\"\"\n    Deserializes the target model and returns the list of fitted categories.\n    \"\"\"\n    \n    model = joblib.load(os.path.join(model_dir, \"target.joblib\"))\n    return model.named_transformers_[\"species\"].categories_[0]\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nimport numpy as np\n\nfrom postprocessing_component import predict_fn, output_fn\n\n\ndef test_predict_returns_prediction_as_first_column():\n    input_data = [\n        [0.6, 0.2, 0.2], \n        [0.1, 0.8, 0.1],\n        [0.2, 0.1, 0.7]\n    ]\n    \n    categories = [\"Adelie\", \"Gentoo\", \"Chinstrap\"]\n    \n    response = predict_fn(input_data, categories)\n    \n    assert response == [\n        (\"Adelie\", 0.6),\n        (\"Gentoo\", 0.8),\n        (\"Chinstrap\", 0.7)\n    ]\n\n\ndef test_output_does_not_return_array_if_single_prediction():\n    prediction = [(\"Adelie\", 0.6)]\n    response, _ = output_fn(prediction, \"application/json\")\n\n    assert response[\"prediction\"] == \"Adelie\"\n\n\ndef test_output_returns_array_if_multiple_predictions():\n    prediction = [(\"Adelie\", 0.6), (\"Gentoo\", 0.8)]\n    response, _ = output_fn(prediction, \"application/json\")\n\n    assert len(response) == 2\n    assert response[0][\"prediction\"] == \"Adelie\"\n    assert response[1][\"prediction\"] == \"Gentoo\"\n\n\n\n\nStep 4 - Setting up the Inference Pipeline\nWe can now create a PipelineModel to define our inference pipeline.\nWe’ll use the model we generated from the first step of the pipeline as the input to the first and last components of the inference pipeline. This model.tar.gz file contains the two transformers we need to preprocess and postprocess the data. Let’s create a variable with the URI to this file:\n\ntransformation_pipeline_model = Join(\n    on=\"/\",\n    values=[\n        split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n            \"model\"\n        ].S3Output.S3Uri,\n        \"model.tar.gz\",\n    ],\n)\n\nHere is the first component of the inference pipeline. It will preprocess the data before sending it to the TensorFlow model:\n\nfrom sagemaker.sklearn.model import SKLearnModel\n\npreprocessing_model = SKLearnModel(\n    model_data=transformation_pipeline_model,\n    entry_point=\"preprocessing_component.py\",\n    source_dir=str(INFERENCE_CODE_FOLDER),\n    framework_version=\"1.2-1\",\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\nHere is the last component of the inference pipeline. It will postprocess the output from the TensorFlow model before sending it back to the user:\n\npost_processing_model = SKLearnModel(\n    model_data=transformation_pipeline_model,\n    entry_point=\"postprocessing_component.py\",\n    source_dir=str(INFERENCE_CODE_FOLDER),\n    framework_version=\"1.2-1\",\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\nWe can now create the inference pipeline using the three models:\n\nfrom sagemaker.pipeline import PipelineModel\n\npipeline_model = PipelineModel(\n    name=\"inference-model\",\n    models=[preprocessing_model, tensorflow_model, post_processing_model],\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\n\n\nStep 5 - Registering the Model\nWe’ll modify the pipeline to register the Pipeline Model in the Model Registry. We’ll use a different group name to keep Pipeline Models separate.\n\nPIPELINE_MODEL_PACKAGE_GROUP = \"pipeline\"\n\nLet’s now register the model. Notice that we will register the model with “PendingManualApproval” status. This means that we’ll need to manually approve the model before it can be deployed to an endpoint. Check Register a Model Version for more information about model registration.\n\nregister_model_step = ModelStep(\n    name=\"register\",\n    display_name=\"register-model\",\n    step_args=pipeline_model.register(\n        model_package_group_name=PIPELINE_MODEL_PACKAGE_GROUP,\n        model_metrics=model_metrics,\n        approval_status=\"PendingManualApproval\",\n        # Our inference pipeline model supports two content\n        # types: text/csv and application/json.\n        content_types=[\"text/csv\", \"application/json\"],\n        response_types=[\"text/csv\", \"application/json\"],\n        # This is the suggested inference instance types when\n        # deploying the model or using it as part of a batch\n        # transform job.\n        inference_instances=[\"ml.m5.xlarge\"],\n        transform_instances=[\"ml.m5.xlarge\"],\n        domain=\"MACHINE_LEARNING\",\n        task=\"CLASSIFICATION\",\n        framework=\"TENSORFLOW\",\n        framework_version=config[\"framework_version\"],\n    ),\n)\n\n\n\nStep 6 - Modifying the Condition Step\nSince we modified the registration step, we also need to modify the Condition Step to use the new registration:\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[condition],\n    if_steps=[register_model_step] if not LOCAL_MODE else [],\n    else_steps=[fail_step],\n)\n\n\n\nStep 7 - Creating the Pipeline\nWe can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession4_pipeline = Pipeline(\n    name=\"session4-pipeline\",\n    parameters=[dataset_location, accuracy_threshold],\n    steps=[\n        split_and_transform_data_step,\n        tune_model_step if USE_TUNING_STEP else train_model_step,\n        evaluate_model_step,\n        condition_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession4_pipeline.upsert(role_arn=role)\n\nWe can now start the pipeline:\n\nsession4_pipeline.start()\n\n\n\nStep 8 - Creating the Lambda Function\nWe will use Amazon EventBridge to trigger a Lambda function that will deploy the model whenever its status changes from “PendingManualApproval” to “Approved.” Let’s start by writing the Lambda function to take the model information and create a new endpoint.\nWe’ll enable Data Capture as part of the endpoint configuration. With Data Capture we can record the inputs and outputs of the endpoint to use them later for monitoring the model:\n\nInitialSamplingPercentage represents the percentage of traffic that we want to capture.\nDestinationS3Uri specifies the S3 location where we want to store the captured data.\n\n\nimport os\nimport json\nimport boto3\nimport time\n\nsagemaker = boto3.client(\"sagemaker\")\n\ndef lambda_handler(event, context):\n    model_package_arn = event[\"detail\"][\"ModelPackageArn\"]\n    approval_status = event[\"detail\"][\"ModelApprovalStatus\"]\n\n    print(f\"Model: {model_package_arn}\")\n    print(f\"Approval status: {approval_status}\")\n    \n    # We only want to deploy the approved models\n    if approval_status != \"Approved\":\n        response = {\n            \"message\": \"Skipping deployment.\",\n            \"approval_status\": approval_status,\n        }\n\n        print(response)\n        return {\n            \"statusCode\": 200,\n            \"body\": json.dumps(response)\n        }    \n    \n    endpoint_name = os.environ[\"ENDPOINT\"]\n    data_capture_destination = os.environ[\"DATA_CAPTURE_DESTINATION\"]\n    role = os.environ[\"ROLE\"]\n    \n    timestamp = time.strftime(\"%m%d%H%M%S\", time.localtime())\n    model_name = f\"{endpoint_name}-model-{timestamp}\"\n    endpoint_config_name = f\"{endpoint_name}-config-{timestamp}\"\n\n    sagemaker.create_model(\n        ModelName=model_name, \n        ExecutionRoleArn=role, \n        Containers=[{\n            \"ModelPackageName\": model_package_arn\n        }] \n    )\n\n    sagemaker.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[{\n            \"ModelName\": model_name,\n            \"InstanceType\": \"ml.m5.xlarge\",\n            \"InitialVariantWeight\": 1,\n            \"InitialInstanceCount\": 1,\n            \"VariantName\": \"AllTraffic\",\n        }],\n        \n        # We can enable Data Capture to record the inputs and outputs\n        # of the endpoint to use them later for monitoring the model. \n        DataCaptureConfig={\n            \"EnableCapture\": True,\n            \"InitialSamplingPercentage\": 100,\n            \"DestinationS3Uri\": data_capture_destination,\n            \"CaptureOptions\": [\n                {\n                    \"CaptureMode\": \"Input\"\n                },\n                {\n                    \"CaptureMode\": \"Output\"\n                },\n            ],\n            \"CaptureContentTypeHeader\": {\n                \"CsvContentTypes\": [\n                    \"text/csv\",\n                    \"application/octect-stream\"\n                ],\n                \"JsonContentTypes\": [\n                    \"application/json\",\n                    \"application/octect-stream\"\n                ]\n            }\n        },\n    )\n    \n    response = sagemaker.list_endpoints(NameContains=endpoint_name, MaxResults=1)\n\n    if len(response[\"Endpoints\"]) == 0:\n        # If the endpoint doesn't exist, let's create it.\n        sagemaker.create_endpoint(\n            EndpointName=endpoint_name, \n            EndpointConfigName=endpoint_config_name,\n        )\n    else:\n        # If the endpoint already exist, let's update it with the\n        # new configuration.\n        sagemaker.update_endpoint(\n            EndpointName=endpoint_name, \n            EndpointConfigName=endpoint_config_name,\n        )\n    \n    return {\n        \"statusCode\": 200,\n        \"body\": json.dumps(\"Endpoint deployed successfully\")\n    }\n\nOverwriting code/lambda.py\n\n\nWe need to ensure our Lambda function has permission to interact with SageMaker, so let’s create a new role and then create the lambda function.\n\nlambda_role_name = \"lambda-deployment-role\"\nlambda_role_arn = None\n\ntry:\n    response = iam_client.create_role(\n        RoleName=lambda_role_name,\n        AssumeRolePolicyDocument=json.dumps(\n            {\n                \"Version\": \"2012-10-17\",\n                \"Statement\": [\n                    {\n                        \"Effect\": \"Allow\",\n                        \"Principal\": {\n                            \"Service\": [\"lambda.amazonaws.com\", \"events.amazonaws.com\"]\n                        },\n                        \"Action\": \"sts:AssumeRole\",\n                    }\n                ],\n            }\n        ),\n        Description=\"Lambda Endpoint Deployment\",\n    )\n\n    lambda_role_arn = response[\"Role\"][\"Arn\"]\n\n    iam_client.attach_role_policy(\n        RoleName=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\",\n        PolicyArn=lambda_role_arn,\n    )\n\n    iam_client.attach_role_policy(\n        RoleName=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\",\n        PolicyArn=lambda_role_arn,\n    )\n\n    print(f'Role \"{lambda_role_name}\" created with ARN \"{lambda_role_arn}\".')\nexcept iam_client.exceptions.EntityAlreadyExistsException:\n    print(f\"Role {lambda_role_name} already exists.\")\n    response = iam_client.get_role(RoleName=lambda_role_name)\n    lambda_role_arn = response[\"Role\"][\"Arn\"]\n\nRole lambda-deployment-role already exists.\n\n\nWe can now create the Lambda function:\n\nfrom sagemaker.lambda_helper import Lambda\n\n\ndeploy_lambda_fn = Lambda(\n    function_name=\"deploy_fn\",\n    execution_role_arn=lambda_role_arn,\n    script=str(CODE_FOLDER / \"lambda.py\"),\n    handler=\"lambda.lambda_handler\",\n    timeout=600,\n    session=sagemaker_session,\n    runtime=\"python3.11\",\n    environment={\n        \"Variables\": {\n            \"ENDPOINT\": ENDPOINT,\n            \"DATA_CAPTURE_DESTINATION\": DATA_CAPTURE_DESTINATION,\n            \"ROLE\": role,\n        }\n    },\n)\n\nlambda_response = None\nif not LOCAL_MODE:\n    lambda_response = deploy_lambda_fn.upsert()\n\nlambda_response\n\n\n\nStep 9 - Setting Up EventBridge\nWe can now create an EventBridge rule that triggers the deployment process whenever a model approval status becomes “Approved”. To do this, let’s define the event pattern that will trigger the deployment process:\n\nevent_pattern = f\"\"\"\n{{\n  \"source\": [\"aws.sagemaker\"],\n  \"detail-type\": [\"SageMaker Model Package State Change\"],\n  \"detail\": {{\n    \"ModelPackageGroupName\": [\"{PIPELINE_MODEL_PACKAGE_GROUP}\"],\n    \"ModelApprovalStatus\": [\"Approved\"]\n  }}\n}}\n\"\"\"\n\nLet’s now create the EventBridge rule:\n\nevents_client = boto3.client(\"events\")\nrule_response = events_client.put_rule(\n    Name=\"PipelineModelApprovedRule\",\n    EventPattern=event_pattern,\n    State=\"ENABLED\",\n    RoleArn=role,\n)\n\nNow, we need to define the target of the rule. The target will trigger whenever the rule matches an event. In this case, we want to trigger the Lambda function we created before:\n\nresponse = events_client.put_targets(\n    Rule=\"PipelineModelApprovedRule\",\n    Targets=[\n        {\n            \"Id\": \"1\",\n            \"Arn\": lambda_response[\"FunctionArn\"],\n        }\n    ],\n)\n\nFinally, we need to give the Lambda function permission to be triggered by the EventBridge rule:\n\nlambda_client = boto3.client(\"lambda\")\ntry:\n    response = lambda_client.add_permission(\n        Action=\"lambda:InvokeFunction\",\n        FunctionName=lambda_response[\"FunctionName\"],\n        Principal=\"events.amazonaws.com\",\n        SourceArn=rule_response[\"RuleArn\"],\n        StatementId=\"EventBridge\",\n    )\nexcept lambda_client.exceptions.ResourceConflictException as e:\n    print(f'Function \"{lambda_response[\"FunctionName\"]}\" already has permissions.')\n\nFunction \"deploy_fn\" already has permissions.\n\n\n\n\nStep 10 - Testing the Endpoint\nLet’s now test the endpoint we deployed automatically with the pipeline. We will use the function to create a predictor with a JSON encoder and decoder.\n\nfrom sagemaker.serializers import CSVSerializer\n\npredictor = Predictor(\n    endpoint_name=ENDPOINT, \n    serializer=CSVSerializer(),\n    sagemaker_session=sagemaker_session\n)\n\ndata = pd.read_csv(DATA_FILEPATH)\ndata = data.drop(\"species\", axis=1)\n\npayload = data.iloc[:3].to_csv(header=False, index=False)\nprint(f\"Payload:\\n{payload}\")\n\ntry:\n    response = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n    print(response.decode(\"utf-8\"))\nexcept Exception as e:\n    print(e)\n\nPayload:\nTorgersen,39.1,18.7,181.0,3750.0,MALE\nTorgersen,39.5,17.4,186.0,3800.0,FEMALE\nTorgersen,40.3,18.0,195.0,3250.0,FEMALE\n\nAn error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint penguins-endpoint of account 325223348818 not found.\n\n\nLet’s delete the endpoint:\n\npredictor.delete_endpoint()\n\n\n\nAssignments\n\nAssignment 4.1 Every Endpoint has an invocation URL you can use to generate predictions with the model from outside AWS. As part of this assignment, write a simple Python script that will run on your local computer and run a few samples through the Endpoint. You will need your AWS access key and secret to connect to the Endpoint.\nAssignment 4.2 We can use model variants to perform A/B testing between a new model and an old model. Create a function that given the ARN of two models in the Model Registry deploys them to an Endpoint as separate variants. Each variant should receive 50% of the traffic. Write another function that invokes the endpoint by default, but allows the caller to invoke a specific variant if they want to.\nAssignment 4.3 We can use SageMaker Model Shadow Deployments to create shadow variants to validate a new model version before promoting it to production. Write a function that given the ARN of a model in the Model Registry, updates an Endpoint and deploys the model as a shadow variant. Check Shadow variants for more information about this topic. Send some traffic to the Endpoint and compare the results from the main model with its shadow variant.\nAssignment 4.4 SageMaker supports auto scaling your models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in the workload. For this assignment, define a target-tracking scaling policy for a variant of your Endpoint and use the SageMakerVariantInvocationsPerInstance metric. SageMakerVariantInvocationsPerInstance is the average number of times per minute that the variant is invoked. Check Automatically Scale Amazon SageMaker Models for more information about auto scaling models.\nAssignment 4.5 Modify the SageMaker Pipeline by adding a Lambda Step that will deploy the model directly as part of the pipeline. You won’t need to set up Event Bridge anymore because your pipeline will automatically deploy the model."
  },
  {
    "objectID": "cohort.html#session-5---data-distribution-shifts-and-model-monitoring",
    "href": "cohort.html#session-5---data-distribution-shifts-and-model-monitoring",
    "title": "Building Production Machine Learning Systems",
    "section": "Session 5 - Data Distribution Shifts And Model Monitoring",
    "text": "Session 5 - Data Distribution Shifts And Model Monitoring\nIn this session we’ll set up a monitoring process to analyze the quality of the data our endpoint receives and the endpoint predictions. For this, we need to check the data received by the endpoint, generate ground truth labels, and compare them with a baseline performance.\n \nTo enable this functionality, we need a couple of steps:\n\nCreate baselines we can use to compare against real-time traffic.\nSet up a schedule to continuously evaluate and compare against the baselines.\n\nCheck Amazon SageMaker Model Monitor for a brief explanation of how to use SageMaker’s Model Monitoring functionality. Monitor models for data and model quality, bias, and explainability is a much more extensive guide to monitoring in Amazon SageMaker.\nLet’s start by defining three variables we’ll use throughout the session:\n\nGROUND_TRUTH_LOCATION = f\"{S3_LOCATION}/monitoring/groundtruth\"\nDATA_QUALITY_LOCATION = f\"{S3_LOCATION}/monitoring/data-quality\"\nMODEL_QUALITY_LOCATION = f\"{S3_LOCATION}/monitoring/model-quality\"\n\n\nStep 1 - Generating Data Quality Baseline\nLet’s start by configuring a Quality Check Step to compute the general statistics of the data we used to build our model.\nWe can configure the instance that will run the quality check using the CheckJobConfig class, and we can use the DataQualityCheckConfig class to configure the job.\n\nfrom sagemaker.workflow.quality_check_step import (\n    QualityCheckStep,\n    DataQualityCheckConfig,\n)\nfrom sagemaker.workflow.check_job_config import CheckJobConfig\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\n\ndata_quality_baseline_step = QualityCheckStep(\n    name=\"generate-data-quality-baseline\",\n    check_job_config=CheckJobConfig(\n        instance_type=\"ml.c5.xlarge\",\n        instance_count=1,\n        volume_size_in_gb=20,\n        sagemaker_session=pipeline_session,\n        role=role,\n    ),\n    quality_check_config=DataQualityCheckConfig(\n        baseline_dataset=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n            \"train-baseline\"\n        ].S3Output.S3Uri,\n        dataset_format=DatasetFormat.csv(header=True, output_columns_position=\"START\"),\n        output_s3_uri=DATA_QUALITY_LOCATION,\n    ),\n    skip_check=True,\n    register_new_baseline=True,\n    cache_config=cache_config,\n)\n\n\n\nStep 2 - Generating Test Predictions\nTo create a baseline to compare the model performance, we must create predictions for the test set and compare the model’s metrics with the model performance on production data. We can do this by running a Batch Transform Job to predict every sample from the test set. We can use a Transform Step as part of the pipeline to run this job. This Batch Transform Job will run every sample from the training dataset through the model so we can compute the baseline metrics.\nThe Transform Step requires a model to generate predictions, so we need a Model Step that creates a model:\n\nfrom sagemaker.workflow.model_step import ModelStep\n\ncreate_model_step = ModelStep(\n    name=\"create\",\n    display_name=\"create-model\",\n    step_args=pipeline_model.create(instance_type=config[\"instance_type\"]),\n)\n\nLet’s configure the Batch Transform Job using an instance of the Transformer class:\n\nfrom sagemaker.transformer import Transformer\n\ntransformer = Transformer(\n    model_name=create_model_step.properties.ModelName,\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    strategy=\"MultiRecord\",\n    accept=\"text/csv\",\n    assemble_with=\"Line\",\n    output_path=f\"{S3_LOCATION}/transform\",\n    sagemaker_session=pipeline_session,\n)\n\nWe can now set up the Transform Step using the transformer we configured before.\nNotice the following:\n\nWe’ll generate predictions for the baseline output that we generated when we split and transformed the data. This baseline is the same data we used to test the model, but we saved it in its original format before transforming it.\nThe output of this Batch Transform Job will have two fields. The first one will be the ground truth label, and the second one will be the prediction of the model.\n\n\nfrom sagemaker.workflow.steps import TransformStep\n\ngenerate_test_predictions_step = TransformStep(\n    name=\"generate-test-predictions\",\n    step_args=transformer.transform(\n        # We will use the baseline set we generated when we split the data.\n        # This set corresponds to the test split before the transformation step.\n        data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n            \"test-baseline\"\n        ].S3Output.S3Uri,\n\n        join_source=\"Input\",\n        split_type=\"Line\",\n        content_type=\"text/csv\",\n        \n        # We want to output the first and the last field from the joint set.\n        # The first field corresponds to the groundtruth, and the last field\n        # corresponds to the prediction.\n        output_filter=\"$[0,-1]\",\n    ),\n    cache_config=cache_config,\n)\n\n\n\nStep 3 - Generating Model Quality Baseline\nLet’s now configure the Quality Check Step and feed it the data we generated in the Transform Step. This step will automatically compute the performance metrics of the model on the test set:\n\nfrom sagemaker.workflow.quality_check_step import ModelQualityCheckConfig\n\nmodel_quality_baseline_step = QualityCheckStep(\n    name=\"generate-model-quality-baseline\",\n    check_job_config=CheckJobConfig(\n        instance_type=\"ml.c5.xlarge\",\n        instance_count=1,\n        volume_size_in_gb=20,\n        sagemaker_session=pipeline_session,\n        role=role,\n    ),\n    quality_check_config=ModelQualityCheckConfig(\n        # We are going to use the output of the Transform Step to generate\n        # the model quality baseline.\n        baseline_dataset=generate_test_predictions_step.properties.TransformOutput.S3OutputPath,\n        dataset_format=DatasetFormat.csv(header=False),\n\n        # We need to specify the problem type and the fields where the prediction\n        # and groundtruth are so the process knows how to interpret the results.\n        problem_type=\"MulticlassClassification\",\n        \n        # Since the data doesn't have headers, SageMaker will autocreate headers for it.\n        # _c0 corresponds to the first column, and _c1 corresponds to the second column.\n        ground_truth_attribute=\"_c0\",\n        inference_attribute=\"_c1\",\n        output_s3_uri=MODEL_QUALITY_LOCATION,\n    ),\n    skip_check=True,\n    register_new_baseline=True,\n    cache_config=cache_config,\n)\n\n\n\nStep 4 - Setting up Model Metrics\nWe can configure a new set of ModelMetrics using the results of the Data and Model Quality Steps. Check Baseline and model version lifecycle and evolution with SageMaker Pipelines for an explanation of how SageMaker uses the DriftCheckBaselines.\n\nfrom sagemaker.drift_check_baselines import DriftCheckBaselines\n\nmodel_metrics = ModelMetrics(\n    model_data_statistics=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineStatistics,\n        content_type=\"application/json\",\n    ),\n    model_data_constraints=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineConstraints,\n        content_type=\"application/json\",\n    ),\n    model_statistics=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.CalculatedBaselineStatistics,\n        content_type=\"application/json\",\n    ),\n    model_constraints=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.CalculatedBaselineConstraints,\n        content_type=\"application/json\",\n    ),\n)\n\ndrift_check_baselines = DriftCheckBaselines(\n    model_data_statistics=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckStatistics,\n        content_type=\"application/json\",\n    ),\n    model_data_constraints=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckConstraints,\n        content_type=\"application/json\",\n    ),\n    model_statistics=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.BaselineUsedForDriftCheckStatistics,\n        content_type=\"application/json\",\n    ),\n    model_constraints=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.BaselineUsedForDriftCheckConstraints,\n        content_type=\"application/json\",\n    ),\n)\n\n\n\nStep 5 - Modifying the Registration Step\nSince we want to register the model using the new metrics, we need to modify the Registration Step to use the new metrics:\n\nregister_model_step = ModelStep(\n    name=\"register\",\n    display_name=\"register-model\",\n    step_args=pipeline_model.register(\n        model_package_group_name=PIPELINE_MODEL_PACKAGE_GROUP,\n        model_metrics=model_metrics,\n        drift_check_baselines=drift_check_baselines,\n        approval_status=\"PendingManualApproval\",\n        content_types=[\"text/csv\", \"application/json\"],\n        response_types=[\"text/csv\", \"application/json\"],\n        inference_instances=[\"ml.m5.xlarge\"],\n        transform_instances=[\"ml.m5.xlarge\"],\n        domain=\"MACHINE_LEARNING\",\n        task=\"CLASSIFICATION\",\n        framework=\"TENSORFLOW\",\n        framework_version=config[\"framework_version\"],\n    ),\n)\n\n\n\nStep 6 - Modifying the Condition Step\nSince we modified the registration step and added a few more steps, we need to modify the Condition Step. Now, we want to generate the test predictions and compute the model quality baseline if the condition is successful:\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[condition],\n    if_steps=[\n        create_model_step,\n        generate_test_predictions_step,\n        model_quality_baseline_step,\n        register_model_step,\n    ]\n    if not LOCAL_MODE\n    else [],\n    else_steps=[fail_step],\n)\n\n\n\nStep 7 - Creating the Pipeline\nWe can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession5_pipeline = Pipeline(\n    name=\"session5-pipeline\",\n    parameters=[dataset_location, accuracy_threshold],\n    steps=[\n        split_and_transform_data_step,\n        tune_model_step if USE_TUNING_STEP else train_model_step,\n        evaluate_model_step,\n        data_quality_baseline_step,\n        condition_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession5_pipeline.upsert(role_arn=role)\n\nWe can now start the pipeline:\n\nsession5_pipeline.start()\n\n\n\nStep 8 - Checking Constraints and Statistics\nOur pipeline generated data baseline statistics and constraints. We can take a look at what these values look like by downloading them from S3. You need to wait for the pipeline to finish running before these files are available.\nHere are the data quality statistics:\n\nfrom sagemaker.s3 import S3Downloader\n\ntry:\n    response = json.loads(\n        S3Downloader.read_file(f\"{DATA_QUALITY_LOCATION}/statistics.json\")\n    )\n    print(json.dumps(response[\"features\"][0], indent=2))\nexcept Exception as e:\n    pass\n\n{\n  \"name\": \"species\",\n  \"inferred_type\": \"String\",\n  \"string_statistics\": {\n    \"common\": {\n      \"num_present\": 232,\n      \"num_missing\": 0\n    },\n    \"distinct_count\": 3.0,\n    \"distribution\": {\n      \"categorical\": {\n        \"buckets\": [\n          {\n            \"value\": \"Adelie\",\n            \"count\": 94\n          },\n          {\n            \"value\": \"Chinstrap\",\n            \"count\": 48\n          },\n          {\n            \"value\": \"Gentoo\",\n            \"count\": 90\n          }\n        ]\n      }\n    }\n  }\n}\n\n\nHere are the data quality constraints:\n\ntry:\n    response = json.loads(S3Downloader.read_file(f\"{DATA_QUALITY_LOCATION}/constraints.json\"))\n    print(json.dumps(response[\"features\"][0], indent=2))\nexcept Exception as e:\n    pass\n\n{\n  \"name\": \"species\",\n  \"inferred_type\": \"String\",\n  \"completeness\": 1.0,\n  \"string_constraints\": {\n    \"domains\": [\n      \"Adelie\",\n      \"Chinstrap\",\n      \"Gentoo\"\n    ]\n  }\n}\n\n\nAnd here are the model quality constraints:\n\ntry:\n    response = json.loads(S3Downloader.read_file(f\"{MODEL_QUALITY_LOCATION}/constraints.json\"))\n    print(json.dumps(response, indent=2))\nexcept Exception as e:\n    pass\n\n{\n  \"version\": 0.0,\n  \"multiclass_classification_constraints\": {\n    \"accuracy\": {\n      \"threshold\": 0.9259259259259259,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_recall\": {\n      \"threshold\": 0.9259259259259259,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_precision\": {\n      \"threshold\": 0.933862433862434,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_f0_5\": {\n      \"threshold\": 0.928855833521148,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_f1\": {\n      \"threshold\": 0.9247293447293448,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_f2\": {\n      \"threshold\": 0.9242942991137502,\n      \"comparison_operator\": \"LessThanThreshold\"\n    }\n  }\n}\n\n\n\n\nStep 9 - Generating Fake Traffic\nTo test the monitoring functionality, we need to generate traffic to the endpoint. To generate traffic, we will send every sample from the dataset to the endpoint to simulate real prediction requests:\n\nfrom sagemaker.serializers import JSONSerializer\n\ndata = penguins.drop([\"species\"], axis=1)\ndata = data.dropna()\n\npredictor = Predictor(\n    endpoint_name=ENDPOINT,\n    serializer=JSONSerializer(),\n    sagemaker_session=sagemaker_session,\n)\n\nfor index, row in data.iterrows():\n    try:\n        predictor.predict(row.to_dict(), inference_id=str(index))\n    except Exception as e:\n        print(e)\n        break\n\nWe can check the location where the endpoint stores the captured data, download a file, and display its content. It may take a few minutes for the first few files to show up in S3.\nThese files contain the data captured by the endpoint in a SageMaker-specific JSON-line format. Each inference request is captured in a single line in the jsonl file. The line contains both the input and output merged together:\n\nfiles = S3Downloader.list(DATA_CAPTURE_DESTINATION)[:3]\nif len(files):\n    lines = S3Downloader.read_file(files[0])\n    print(json.dumps(json.loads(lines.split(\"\\n\")[0]), indent=2))\n\n{\n  \"captureData\": {\n    \"endpointInput\": {\n      \"observedContentType\": \"application/json\",\n      \"mode\": \"INPUT\",\n      \"data\": \"{\\\"island\\\": \\\"Torgersen\\\", \\\"culmen_length_mm\\\": 39.1, \\\"culmen_depth_mm\\\": 18.7, \\\"flipper_length_mm\\\": 181.0, \\\"body_mass_g\\\": 3750.0, \\\"sex\\\": \\\"MALE\\\"}\",\n      \"encoding\": \"JSON\"\n    },\n    \"endpointOutput\": {\n      \"observedContentType\": \"application/json\",\n      \"mode\": \"OUTPUT\",\n      \"data\": \"{\\\"prediction\\\": \\\"Adelie\\\", \\\"confidence\\\": 0.953110516}\",\n      \"encoding\": \"JSON\"\n    }\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"ddf80c99-e582-4243-9309-4bc9085c01ec\",\n    \"inferenceId\": \"0\",\n    \"inferenceTime\": \"2023-10-24T19:10:30Z\"\n  },\n  \"eventVersion\": \"0\"\n}\n\n\nThese files contain the data captured by the endpoint in a SageMaker-specific JSON-line format. Each inference request is captured in a single line in the jsonl file. The line contains both the input and output merged together:\n\n\nStep 10 - Generating Fake Labels\nTo test the performance of the model, we need to label the samples captured by the endpoint. We can simulate the labeling process by generating a random label for every sample. Check Ingest Ground Truth Labels and Merge Them With Predictions for more information about this.\n\nimport random\nfrom datetime import datetime\nfrom sagemaker.s3 import S3Uploader\n\nrecords = []\nfor inference_id in range(len(data)):\n    random.seed(inference_id)\n\n    records.append(json.dumps({\n        \"groundTruthData\": {\n            \"data\": random.choice([\"Adelie\", \"Chinstrap\", \"Gentoo\"]),\n            \"encoding\": \"CSV\",\n        },\n        \"eventMetadata\": {\n            \"eventId\": str(inference_id),\n        },\n        \"eventVersion\": \"0\",\n    }))\n\ngroundtruth_payload = \"\\n\".join(records)\nupload_time = datetime.utcnow()\nuri = f\"{GROUND_TRUTH_LOCATION}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\nS3Uploader.upload_string_as_file_body(groundtruth_payload, uri)\n\n\n\nStep 11 - Preparing Monitoring Functions\nLet’s create a few functions that will help us work with monitoring schedules later on:\n\nfrom sagemaker.model_monitor import MonitoringExecution\n\n\ndef describe_monitoring_schedules(endpoint_name):\n    schedules = []\n    response = sagemaker_client.list_monitoring_schedules(EndpointName=endpoint_name)[\n        \"MonitoringScheduleSummaries\"\n    ]\n    for item in response:\n        name = item[\"MonitoringScheduleName\"]\n        schedule = {\n            \"MonitoringScheduleName\": name,\n            \"MonitoringType\": item[\"MonitoringType\"],\n        }\n\n        description = sagemaker_client.describe_monitoring_schedule(\n            MonitoringScheduleName=name\n        )\n\n        schedule[\"Status\"] = description[\"LastMonitoringExecutionSummary\"][\n            \"MonitoringExecutionStatus\"\n        ]\n\n        if schedule[\"Status\"] == \"Failed\":\n            schedule[\"FailureReason\"] = description[\"LastMonitoringExecutionSummary\"][\n                \"FailureReason\"\n            ]\n        elif schedule[\"Status\"] == \"CompletedWithViolations\":\n            processing_job_arn = description[\"LastMonitoringExecutionSummary\"][\n                \"ProcessingJobArn\"\n            ]\n            execution = MonitoringExecution.from_processing_arn(\n                sagemaker_session=sagemaker_session,\n                processing_job_arn=processing_job_arn,\n            )\n            execution_destination = execution.output.destination\n\n            violations_filepath = os.path.join(\n                execution_destination, \"constraint_violations.json\"\n            )\n            violations = json.loads(S3Downloader.read_file(violations_filepath))[\n                \"violations\"\n            ]\n\n            schedule[\"Violations\"] = violations\n\n        schedules.append(schedule)\n\n    return schedules\n\n\ndef describe_monitoring_schedule(endpoint_name, monitoring_type):\n    found = False\n\n    schedules = describe_monitoring_schedules(endpoint_name)\n    for schedule in schedules:\n        if schedule[\"MonitoringType\"] == monitoring_type:\n            found = True\n            print(json.dumps(schedule, indent=2))\n\n    if not found:\n        print(f\"There's no {monitoring_type} Monitoring Schedule.\")\n\n\ndef describe_data_monitoring_schedule(endpoint_name):\n    describe_monitoring_schedule(endpoint_name, \"DataQuality\")\n\n\ndef describe_model_monitoring_schedule(endpoint_name):\n    describe_monitoring_schedule(endpoint_name, \"ModelQuality\")\n\n\ndef delete_monitoring_schedule(endpoint_name, monitoring_type):\n    attempts = 30\n    found = False\n\n    response = sagemaker_client.list_monitoring_schedules(EndpointName=endpoint_name)[\n        \"MonitoringScheduleSummaries\"\n    ]\n    for item in response:\n        if item[\"MonitoringType\"] == monitoring_type:\n            found = True\n            status = sagemaker_client.describe_monitoring_schedule(\n                MonitoringScheduleName=item[\"MonitoringScheduleName\"]\n            )[\"MonitoringScheduleStatus\"]\n            while status in (\"Pending\", \"InProgress\") and attempts &gt; 0:\n                attempts -= 1\n                print(\n                    f\"Monitoring schedule status: {status}. Waiting for it to finish.\"\n                )\n                sleep(30)\n\n                status = sagemaker_client.describe_monitoring_schedule(\n                    MonitoringScheduleName=item[\"MonitoringScheduleName\"]\n                )[\"MonitoringScheduleStatus\"]\n\n            if status not in (\"Pending\", \"InProgress\"):\n                sagemaker_client.delete_monitoring_schedule(\n                    MonitoringScheduleName=item[\"MonitoringScheduleName\"]\n                )\n                print(\"Monitoring schedule deleted.\")\n            else:\n                print(\"Waiting for monitoring schedule timed out\")\n\n    if not found:\n        print(f\"There's no {monitoring_type} Monitoring Schedule.\")\n\n\ndef delete_data_monitoring_schedule(endpoint_name):\n    delete_monitoring_schedule(endpoint_name, \"DataQuality\")\n\n\ndef delete_model_monitoring_schedule(endpoint_name):\n    delete_monitoring_schedule(endpoint_name, \"ModelQuality\")\n\n\n\nStep 12 - Setting Up Data Monitoring Job\nSageMaker looks for violations in the data captured by the endpoint. By default, it combines the input data with the endpoint output and compares the result with the baseline we generated. If we let SageMaker do this, we will get a few violations, for example an “extra column check” violation because the field confidence doesn’t exist in the baseline data.\nWe can fix these violations by creating a preprocessing script configuring the data we want the monitoring job to use. Check Preprocessing and Postprocessing for more information about how to configure these scripts.\nLet’s define the name of the preprocessing script:\n\nDATA_QUALITY_PREPROCESSOR = \"data_quality_preprocessor.py\"\n\nWe can now define the preprocessing script. Notice that this script will return the input data the endpoint receives with a new species column containing the prediction of the model:\n\nimport json\n\ndef preprocess_handler(inference_record):\n    input_data = inference_record.endpoint_input.data\n    output_data = json.loads(inference_record.endpoint_output.data)\n\n    response = json.loads(input_data)\n    response[\"species\"] = output_data[\"prediction\"]\n\n    # The `response` variable contains the data that we want the\n    # monitoring job to use to compare with the baseline.\n    return response\n\nThe monitoring schedule expects an S3 location pointing to the preprocessing script. Let’s upload the script to the default bucket.\n\nbucket = boto3.Session().resource(\"s3\").Bucket(pipeline_session.default_bucket())\nprefix = \"penguins-monitoring\"\nbucket.Object(os.path.join(prefix, DATA_QUALITY_PREPROCESSOR)).upload_file(\n    str(CODE_FOLDER / DATA_QUALITY_PREPROCESSOR)\n)\ndata_quality_preprocessor = (\n    f\"s3://{os.path.join(bucket.name, prefix, DATA_QUALITY_PREPROCESSOR)}\"\n)\n\nWe can now set up the Data Quality Monitoring Job using the DefaultModelMonitor class. Notice how we specify the record_preprocessor_script using the S3 location where we uploaded our script.\n\nfrom sagemaker.model_monitor import CronExpressionGenerator, DefaultModelMonitor\n\ndata_monitor = DefaultModelMonitor(\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=1,\n    max_runtime_in_seconds=3600,\n    role=role,\n)\n\ndata_monitor.create_monitoring_schedule(\n    monitor_schedule_name=\"penguins-data-monitoring-schedule\",\n    endpoint_input=ENDPOINT,\n    record_preprocessor_script=data_quality_preprocessor,\n    statistics=f\"{DATA_QUALITY_LOCATION}/statistics.json\",\n    constraints=f\"{DATA_QUALITY_LOCATION}/constraints.json\",\n    schedule_cron_expression=CronExpressionGenerator.hourly(),\n    output_s3_uri=DATA_QUALITY_LOCATION,\n    enable_cloudwatch_metrics=True,\n)\n\nWe can check the results of the monitoring job by looking at whether it generated any violations:\n\ndescribe_data_monitoring_schedule(ENDPOINT)\n\n{\n  \"MonitoringScheduleName\": \"penguins-data-monitoring-schedule\",\n  \"MonitoringType\": \"DataQuality\",\n  \"Status\": \"Failed\",\n  \"FailureReason\": \"Job inputs had no data\"\n}\n\n\n\n\nStep 13 - Setting up Model Monitoring Job\nTo set up a Model Quality Monitoring Job, we can use the ModelQualityMonitor class. The EndpointInput instance configures the attribute the monitoring job should use to determine the prediction from the model.\nCheck Amazon SageMaker Model Quality Monitor for a complete tutorial on how to run a Model Monitoring Job in SageMaker.\nWe can now start the Model Quality Monitoring Job:\n\nfrom sagemaker.model_monitor import ModelQualityMonitor, EndpointInput\n\nmodel_monitor = ModelQualityMonitor(\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=1,\n    max_runtime_in_seconds=1800,\n    role=role\n)\n\nmodel_monitor.create_monitoring_schedule(\n    monitor_schedule_name=\"penguins-model-monitoring-schedule\",\n    \n    endpoint_input = EndpointInput(\n        endpoint_name=ENDPOINT,\n        inference_attribute=\"prediction\",\n        destination=\"/opt/ml/processing/input_data\",\n    ),\n    \n    problem_type=\"MulticlassClassification\",\n    ground_truth_input=GROUND_TRUTH_LOCATION,\n    constraints=f\"{MODEL_QUALITY_LOCATION}/constraints.json\",\n    schedule_cron_expression=CronExpressionGenerator.hourly(),\n    output_s3_uri=MODEL_QUALITY_LOCATION,\n    enable_cloudwatch_metrics=True,\n)\n\nWe can check the results of the monitoring job by looking at whether it generated any violations.\n\ndescribe_model_monitoring_schedule(ENDPOINT)\n\n{\n  \"MonitoringScheduleName\": \"penguins-model-monitoring-schedule\",\n  \"MonitoringType\": \"ModelQuality\",\n  \"Status\": \"CompletedWithViolations\",\n  \"Violations\": [\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedF2 with 0.3505018546481581 +/- 0.004778110439777429 was LessThanThreshold '0.9242942991137502'\",\n      \"metric_name\": \"weightedF2\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric accuracy with 0.35755813953488375 +/- 0.004625699974871179 was LessThanThreshold '0.9259259259259259'\",\n      \"metric_name\": \"accuracy\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedRecall with 0.3575581395348837 +/- 0.004625699974871179 was LessThanThreshold '0.9259259259259259'\",\n      \"metric_name\": \"weightedRecall\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedPrecision with 0.35662633279042494 +/- 0.005592963346101618 was LessThanThreshold '0.933862433862434'\",\n      \"metric_name\": \"weightedPrecision\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedF1 with 0.34519661584972283 +/- 0.004997774377359799 was LessThanThreshold '0.9247293447293448'\",\n      \"metric_name\": \"weightedF1\"\n    }\n  ]\n}\n\n\n\n\nStep 14 - Tearing Down Resources\nThe following code will stop the monitoring jobs and delete the endpoint.\n\ndelete_data_monitoring_schedule(ENDPOINT)\ndelete_model_monitoring_schedule(ENDPOINT)\n\nLet’s delete the endpoint:\n\npredictor.delete_endpoint()\n\n\n\nAssignments\n\nAssignment 5.1 You can visualize the results of your monitoring jobs in Amazon SageMaker Studio. Go to your endpoint, and visit the Data quality and Model quality tabs. View the details of your monitoring jobs, and create a few charts to explore the baseline and the captured values for any metric that the monitoring job calculates.\nAssignment 5.2 The QualityCheck Step runs a processing job to compute baseline statistics and constraints from the input dataset. We configured the pipeline to generate the initial baselines every time it runs. Modify the code to prevent the pipeline from registering a new version of the model if the dataset violates the baseline of the previous model version. You can configure the QualityCheck Step to accomplish this.\nAssignment 5.3 We are generating predictions for the test set twice during the execution of our pipeline. First, during the Evaluation Step, and then using a Transform Step in anticipation of generating the baseline to monitor the model. Modify the Evaluation Step so it reuses the model performance computed by the QualityCheck Step instead of generating predictions again.\nAssignment 5.4 Evidently AI is an open-source Machine Learning observability platform that you can use to evaluate, test, and monitor models. For this assignment, integrate the endpoint we built with Evidently AI to use its capabilities to monitor the model.\nAssignment 5.5 Instead of running the entire pipeline from start to finish, sometimes you may only need to iterate over particular steps. SageMaker Pipelines supports Selective Execution for Pipeline Steps. In this assignment you will use Selective Execution to only run one specific step of the pipeline. Unlocking efficiency: Harnessing the power of Selective Execution in Amazon SageMaker Pipelines is a great article that explains this feature."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup Instructions",
    "section": "",
    "text": "Here are the steps you need to follow to set up the project:\nStart by forking the program’s GitHub Repository and clone it on your local computer.\nCreate and activate a virtual environment:\nOnce the virtual environment is active, you can update pip and install the libraries in the requirements.txt file:\nWe’ll use Jupyter Notebooks during the program. Using the following command, you can install a Jupyter kernel in the virtual environment. If you use Visual Studio Code, you can point your kernel to the virtual environment, and it will install it automatically:\nInstall Docker. You’ll find installation instructions on their site for your particular environment. After you install it, you can verify Docker is running using the following command:"
  },
  {
    "objectID": "setup.html#configuring-aws",
    "href": "setup.html#configuring-aws",
    "title": "Setup Instructions",
    "section": "Configuring AWS",
    "text": "Configuring AWS\nIf you don’t have one yet, create a new AWS account. You want to use a user with administrative privileges.\nWe’ll need access to ml.m5.xlarge instances during the program. By default, the quota for a new account is zero, so you need to request a quota increase. You can do this in your AWS account, under Service Quotas &gt; AWS Services &gt; Amazon SageMaker. Find ml.m5.xlarge and request a quota increase for processing jobs, training jobs, transform jobs, and endpoint usage. Ask for a minimum of 3 instances.\nYou’ll need access to AWS from your local environment. Install the AWS CLI and configure it with your aws_access_key_id and aws_secret_access_key.\nTo get an access key, you first need to open the IAM service, find your user, select Security Credentials, then assign a Multi-Factor Authentication (MFA) device and follow the prompts. After setup and verified, you can click to create an access key.\nAfter you finish configuring the CLI, create a new S3 bucket where we will store the data and every resource we are going to create during the program. The name of the bucket must be unique:\n$ aws s3api create-bucket --bucket [YOUR-BUCKET-NAME]\n\n\n\n\n\n\nNote\n\n\n\nIf you want to create a bucket in a region other than us-east-1, you need to use the --create-bucket-configuration argument to specify your LocationConstraint. See the example below:\n\n\n$ aws s3api create-bucket --bucket [YOUR-BUCKET-NAME] \\\n    --create-bucket-configuration LocationConstraint=\"eu-west-1\"\nUpload the dataset to the S3 bucket you just created:\n$ aws s3 cp program/penguins.csv s3://[YOUR-BUCKET-NAME]/penguins/data/data.csv"
  },
  {
    "objectID": "setup.html#configuring-sagemaker",
    "href": "setup.html#configuring-sagemaker",
    "title": "Setup Instructions",
    "section": "Configuring SageMaker",
    "text": "Configuring SageMaker\nIf you don’t have one yet, create a SageMaker domain. The Getting Started on Amazon SageMaker Studio video will walk you through the process.\nAfter you are done, run the following command to return the Domain Id and the User Profile Name of your SageMaker domain:\n$ aws sagemaker list-user-profiles | grep -E '\"DomainId\"|\"UserProfileName\"' \\\n    | awk -F'[:,\"]+' '{print $2\":\"$3 $4 $5}'\nUse the DomainId and the UserProfileName from the response and replace them in the following command that we’ll return the execution role attached to the user:\n$ aws sagemaker describe-user-profile \\\n    --domain-id [YOUR-DOMAIN-ID] \\\n    --user-profile-name [YOUR-USER-PROFILE-NAME] \\\n    | grep -E \"ExecutionRole\" | awk -F'[\"]' '{print $2\": \"$4}'\nCreate an .env file in the root directory of your repository with the following content. Make sure you replace the value of each variable with the correct value:\nBUCKET=[YOUR-BUCKET-NAME]\nDOMAIN_ID=[YOUR-DOMAIN-ID]\nUSER_PROFILE=[YOUR-USER-PROFILE]\nROLE=[YOUR-EXECUTION-ROLE]\nOpen the Amazon IAM service, find the Execution Role from before and edit the custom Execution Policy assigned to it. Edit the permissions of the Execution Policy and replace them with the JSON below. These permissions will give the Execution Role access to the resources we’ll use during the program:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"IAM0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:CreateServiceLinkedRole\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:AWSServiceName\": [\n                        \"autoscaling.amazonaws.com\",\n                        \"ec2scheduled.amazonaws.com\",\n                        \"elasticloadbalancing.amazonaws.com\",\n                        \"spot.amazonaws.com\",\n                        \"spotfleet.amazonaws.com\",\n                        \"transitgateway.amazonaws.com\"\n                    ]\n                }\n            }\n        },\n        {\n            \"Sid\": \"IAM1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:CreateRole\",\n                \"iam:DeleteRole\",\n                \"iam:PassRole\",\n                \"iam:AttachRolePolicy\",\n                \"iam:DetachRolePolicy\",\n                \"iam:CreatePolicy\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"Lambda\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:CreateFunction\",\n                \"lambda:DeleteFunction\",\n                \"lambda:InvokeFunctionUrl\",\n                \"lambda:InvokeFunction\",\n                \"lambda:UpdateFunctionCode\",\n                \"lambda:InvokeAsync\",\n                \"lambda:AddPermission\",\n                \"lambda:RemovePermission\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"SageMaker\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:UpdateDomain\",\n                \"sagemaker:UpdateUserProfile\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"CloudWatch\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"cloudwatch:PutMetricData\",\n                \"cloudwatch:GetMetricData\",\n                \"cloudwatch:DescribeAlarmsForMetric\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:CreateLogGroup\",\n                \"logs:DescribeLogStreams\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"ECR\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:BatchGetImage\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"S3\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:CreateBucket\",\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": \"arn:aws:s3:::*\"\n        },\n        {\n            \"Sid\": \"EventBridge\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"events:PutRule\",\n                \"events:PutTargets\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\nFinally, find the Trust relationships section under the same Execution Role, edit the configuration, and replace it with the JSON below:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": [\n                    \"sagemaker.amazonaws.com\", \n                    \"events.amazonaws.com\"\n                ]\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}"
  },
  {
    "objectID": "setup.html#apple-silicon",
    "href": "setup.html#apple-silicon",
    "title": "Setup Instructions",
    "section": "Apple silicon",
    "text": "Apple silicon\nIf your local environment is running on Apple silicon, you need to build a TensorFlow docker image to run some of the Pipeline Steps on your local computer. This is because SageMaker doesn’t provide out-of-the-box TensorFlow images compatible with Apple silicon.\nYou can build the image running the following command:\n$ docker build -t sagemaker-tensorflow-toolkit-local container/."
  },
  {
    "objectID": "setup.html#running-the-code-in-sagemaker-studio",
    "href": "setup.html#running-the-code-in-sagemaker-studio",
    "title": "Setup Instructions",
    "section": "Running the code in SageMaker Studio",
    "text": "Running the code in SageMaker Studio\nYou can run the code of the program from your local environment. If you are planning to run it from inside SageMaker Studio, you will need to create a Lifecycle Configuration to update the kernel. To do this, you need to run the studio-setup.ipynb notebook once inside SageMaker Studio. After doing this, you can use the TensorFlow 2.11 Python 3.9 CPU Optimized kernel with the start-up script named ml-school."
  },
  {
    "objectID": "studio-setup.html",
    "href": "studio-setup.html",
    "title": "Setting up SageMaker Studio",
    "section": "",
    "text": "import os\nimport sys\nfrom pathlib import Path\n\nCODE_FOLDER = Path(\"code\")\nCODE_FOLDER.mkdir(parents=True, exist_ok=True)\n\nsys.path.append(f\"./{CODE_FOLDER}\")\n\nDOMAIN_ID=os.environ[\"DOMAIN_ID\"]\nUSER_PROFILE=os.environ[\"USER_PROFILE\"]\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nThe dotenv extension is already loaded. To reload it, use:\n  %reload_ext dotenv"
  },
  {
    "objectID": "studio-setup.html#step-1---customize-kernel-libraries",
    "href": "studio-setup.html#step-1---customize-kernel-libraries",
    "title": "Setting up SageMaker Studio",
    "section": "Step 1 - Customize Kernel Libraries",
    "text": "Step 1 - Customize Kernel Libraries\nYou can customize SageMaker Studio using Lifecycle configurations. These are shell scripts that will be triggered by lifecycle events, such as starting a new Studio notebook.\nThe following script upgrades the packages on a SageMaker Studio Kernel Application.\n\n#!/bin/bash\n# This script upgrades the packages on a SageMaker \n# Studio Kernel Application.\n\nset -eux\n\npip install -q --upgrade pip\npip install -q --upgrade awscli boto3\npip install -q --upgrade scikit-learn==1.3.1\npip install -q --upgrade PyYAML==6.0\npip install -q --upgrade sagemaker\npip install -q --upgrade ipytest\n\nOverwriting code/packages.sh\n\n\nWe can now create a new lifecycle configuration.\n\nDOMAIN_ID=$(echo \"$1\")\nUSER_PROFILE=$(echo \"$2\")\n\nLCC_CONTENT=`openssl base64 -A -in $3/packages.sh`\n\naws sagemaker delete-studio-lifecycle-config \\\n    --studio-lifecycle-config-name ml-school\n\nresponse=$(aws sagemaker create-studio-lifecycle-config \\\n    --studio-lifecycle-config-name ml-school \\\n    --studio-lifecycle-config-content $LCC_CONTENT \\\n    --studio-lifecycle-config-app-type KernelGateway) \n\narn=$(echo \"${response}\" | python3 -c \"import sys, json; print(json.load(sys.stdin)['StudioLifecycleConfigArn'])\")\necho \"${arn}\"\n\naws sagemaker update-user-profile --domain-id $DOMAIN_ID \\\n    --user-profile-name $USER_PROFILE \\\n    --user-settings '{\n        \"KernelGatewayAppSettings\": {\n            \"LifecycleConfigArns\": [\"'$arn'\"]\n        }\n    }'\n\narn:aws:sagemaker:us-east-1:325223348818:studio-lifecycle-config/ml-school\n{\n    \"UserProfileArn\": \"arn:aws:sagemaker:us-east-1:325223348818:user-profile/d-givocgtibv1g/default-1682182522641\"\n}"
  },
  {
    "objectID": "studio-setup.html#step-2---set-up-auto-shutdown",
    "href": "studio-setup.html#step-2---set-up-auto-shutdown",
    "title": "Setting up SageMaker Studio",
    "section": "Step 2 - Set up Auto-Shutdown",
    "text": "Step 2 - Set up Auto-Shutdown\nThe following script configures auto-shutdown of inactive kernels.\n\n#!/bin/bash\n# This script installs the idle notebook auto-checker server extension to SageMaker Studio\n# The original extension has a lab extension part where users can set the idle timeout via a Jupyter Lab widget.\n# In this version the script installs the server side of the extension only. The idle timeout\n# can be set via a command-line script which will be also created by this create and places into the\n# user's home folder\n#\n# Installing the server side extension does not require Internet connection (as all the dependencies are stored in the\n# install tarball) and can be done via VPCOnly mode.\n\nset -eux\n\n# timeout in minutes\nexport TIMEOUT_IN_MINS=60\n\n# Should already be running in user home directory, but just to check:\ncd /home/sagemaker-user\n\n# By working in a directory starting with \".\", we won't clutter up users' Jupyter file tree views\nmkdir -p .auto-shutdown\n\n# Create the command-line script for setting the idle timeout\ncat &gt; .auto-shutdown/set-time-interval.sh &lt;&lt; EOF\n#!/opt/conda/bin/python\nimport json\nimport requests\nTIMEOUT=${TIMEOUT_IN_MINS}\nsession = requests.Session()\n# Getting the xsrf token first from Jupyter Server\nresponse = session.get(\"http://localhost:8888/jupyter/default/tree\")\n# calls the idle_checker extension's interface to set the timeout value\nresponse = session.post(\"http://localhost:8888/jupyter/default/sagemaker-studio-autoshutdown/idle_checker\",\n            json={\"idle_time\": TIMEOUT, \"keep_terminals\": False},\n            params={\"_xsrf\": response.headers['Set-Cookie'].split(\";\")[0].split(\"=\")[1]})\nif response.status_code == 200:\n    print(\"Succeeded, idle timeout set to {} minutes\".format(TIMEOUT))\nelse:\n    print(\"Error!\")\n    print(response.status_code)\nEOF\nchmod +x .auto-shutdown/set-time-interval.sh\n\n# \"wget\" is not part of the base Jupyter Server image, you need to install it first if needed to download the tarball\nsudo yum install -y wget\n# You can download the tarball from GitHub or alternatively, if you're using VPCOnly mode, you can host on S3\nwget -O .auto-shutdown/extension.tar.gz https://github.com/aws-samples/sagemaker-studio-auto-shutdown-extension/raw/main/sagemaker_studio_autoshutdown-0.1.5.tar.gz\n\n# Or instead, could serve the tarball from an S3 bucket in which case \"wget\" would not be needed:\n# aws s3 --endpoint-url [S3 Interface Endpoint] cp s3://[tarball location] .auto-shutdown/extension.tar.gz\n\n# Installs the extension\ncd .auto-shutdown\ntar xzf extension.tar.gz\ncd sagemaker_studio_autoshutdown-0.1.5\n\n# Activate studio environment just for installing extension\nexport AWS_SAGEMAKER_JUPYTERSERVER_IMAGE=\"${AWS_SAGEMAKER_JUPYTERSERVER_IMAGE:-'jupyter-server'}\"\nif [ \"$AWS_SAGEMAKER_JUPYTERSERVER_IMAGE\" = \"jupyter-server-3\" ] ; then\n    eval \"$(conda shell.bash hook)\"\n    conda activate studio\nfi;\npip install --no-dependencies --no-build-isolation -e .\njupyter serverextension enable --py sagemaker_studio_autoshutdown\nif [ \"$AWS_SAGEMAKER_JUPYTERSERVER_IMAGE\" = \"jupyter-server-3\" ] ; then\n    conda deactivate\nfi;\n\n# Restarts the jupyter server\nnohup supervisorctl -c /etc/supervisor/conf.d/supervisord.conf restart jupyterlabserver\n\n# Waiting for 30 seconds to make sure the Jupyter Server is up and running\nsleep 30\n\n# Calling the script to set the idle-timeout and active the extension\n/home/sagemaker-user/.auto-shutdown/set-time-interval.sh\n\nOverwriting autoshutdown.sh\n\n\nWe can now create a new lifecycle configuration.\n\nDOMAIN_ID=$(echo \"$1\")\nUSER_PROFILE=$(echo \"$2\")\n\nLCC_CONTENT=`openssl base64 -A -in $3/autoshutdown.sh`\n\naws sagemaker delete-studio-lifecycle-config \\\n    --studio-lifecycle-config-name autoshutdown 2&gt; /dev/null\n\nresponse=$(aws sagemaker create-studio-lifecycle-config \\\n    --studio-lifecycle-config-name autoshutdown \\\n    --studio-lifecycle-config-content $LCC_CONTENT \\\n    --studio-lifecycle-config-app-type JupyterServer) \n\narn=$(echo \"${response}\" | python3 -c \"import sys, json; print(json.load(sys.stdin)['StudioLifecycleConfigArn'])\")\necho \"${arn}\"\n\naws sagemaker update-user-profile --domain-id $DOMAIN_ID \\\n    --user-profile-name $USER_PROFILE \\\n    --user-settings '{\n        \"JupyterServerAppSettings\": {\n            \"DefaultResourceSpec\": {\n                \"LifecycleConfigArn\": \"'$arn'\",\n                \"InstanceType\": \"system\"\n            },\n            \"LifecycleConfigArns\": [\"'$arn'\"]\n        }\n    }'"
  },
  {
    "objectID": "cohort-old.html",
    "href": "cohort-old.html",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "",
    "text": "import sys\nfrom pathlib import Path\n\nCODE_FOLDER = Path(\"code\")\nCODE_FOLDER.mkdir(parents=True, exist_ok=True)\n\nsys.path.append(f\"./{CODE_FOLDER}\")\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nThe dotenv extension is already loaded. To reload it, use:\n  %reload_ext dotenv\nTo run this notebook in Local Mode set the LOCAL_MODE constant to True.\nTo build our system using SageMaker, we need access to ml.m5.xlarge instances. By default, the quota on a new AWS account is zero, so you need to request a quota increase. You can do that under Service Quotas &gt; AWS Services &gt; Amazon SageMaker. Find ml.m5.xlarge and request a quota increase for processing jobs, training jobs, transform jobs, and endpoint usage. In the meantime, you can use ml.t3.large as a substitute.\nimport os\nimport logging\n\n# By default, The SageMaker SDK logs events related to the default\n# configuration using the INFO level. To prevent these from spoiling\n# the output of this notebook cells, we can change the logging\n# level to ERROR instead.\nlogging.getLogger(\"sagemaker.config\").setLevel(logging.ERROR)\n\nimport sagemaker\nfrom sagemaker.workflow.pipeline_context import PipelineSession, LocalPipelineSession\n\nLOCAL_MODE = True\nBUCKET = os.environ[\"BUCKET\"]\n\nrole = os.environ[\"ROLE\"]\n\n# If you are running this notebook on an ARM64 machine, you will need\n# to build a custom Docker image using the setup notebook. This is\n# because SageMaker doesn't provide a TensorFlow image that supports \n# The Apple M chips.\narchitecture = !(uname -m)\nIS_APPLE_M_CHIP = architecture[0] == \"arm64\"\n\n# We'll use these two variables to configure the steps that do not support\n# Local Mode.\npipeline_session = PipelineSession(default_bucket=BUCKET) if not LOCAL_MODE else None\n\nif LOCAL_MODE:\n    config = {\n        \"session\": LocalPipelineSession(default_bucket=BUCKET),\n        \"instance_type\": \"local\",\n\n        # We need to use a custom Docker image when we run the pipeline\n        # in Local Model on an ARM64 machine.\n        \"image\": \"sagemaker-tensorflow-training-toolkit-local\" if IS_APPLE_M_CHIP else None,\n        \"framework_version\": None if IS_APPLE_M_CHIP else \"2.11\",\n        \"py_version\": None if IS_APPLE_M_CHIP else \"py39\",\n    }\nelse:\n    config = {\n        \"session\": pipeline_session,\n        \"instance_type\": \"ml.m5.xlarge\",\n        \"image\": None,        \n        \"framework_version\": \"2.11\",\n        \"py_version\": \"py39\",\n    }"
  },
  {
    "objectID": "cohort-old.html#preprocessing",
    "href": "cohort-old.html#preprocessing",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Preprocessing",
    "text": "Preprocessing\nWe’ll use a Processing Step to split and transform the data.\nLet’s create the preprocessing script. The Processing Step will spin up a Processing Job and run this script inside a container.\n\nimport os\nimport sys\nimport argparse\nimport json\nimport tarfile\nimport tempfile\nimport time\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nfrom io import StringIO\nfrom pathlib import Path\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n\n\ndef preprocess(base_directory):\n    \"\"\"\n    This function loads the supplied data, splits it and transforms it.\n    \"\"\"\n\n    df = _read_data_from_input_csv_files(base_directory)\n    \n    target_transformer = ColumnTransformer(\n        transformers=[(\"species\", OrdinalEncoder(), [0])]\n    )\n    \n    numeric_transformer = make_pipeline(\n        SimpleImputer(strategy=\"mean\"),\n        StandardScaler()\n    )\n\n    categorical_transformer = make_pipeline(\n        SimpleImputer(strategy=\"most_frequent\"),\n        OneHotEncoder()\n    )\n    \n    features_transformer = ColumnTransformer(\n        transformers=[\n            (\"numeric\", numeric_transformer, make_column_selector(dtype_exclude=\"object\")),\n            (\"categorical\", categorical_transformer, [\"island\"]),\n        ]\n    )\n\n    df_train, df_validation, df_test = _split_data(df)\n\n    _save_baseline(base_directory, df_test)\n\n    y_train = target_transformer.fit_transform(np.array(df_train.species.values).reshape(-1, 1))\n    y_validation = target_transformer.transform(np.array(df_validation.species.values).reshape(-1, 1))\n    y_test = target_transformer.transform(np.array(df_test.species.values).reshape(-1, 1))\n    \n    df_train = df_train.drop(\"species\", axis=1)\n    df_validation = df_validation.drop(\"species\", axis=1)\n    df_test = df_test.drop(\"species\", axis=1)\n\n    X_train = features_transformer.fit_transform(df_train)\n    X_validation = features_transformer.transform(df_validation)\n    X_test = features_transformer.transform(df_test)\n\n    _save_splits(base_directory, X_train, y_train, X_validation, y_validation, X_test, y_test)\n    _save_model(base_directory, target_transformer, features_transformer)\n    \n\ndef _read_data_from_input_csv_files(base_directory):\n    \"\"\"\n    This function reads every CSV file available and concatenates\n    them into a single dataframe.\n    \"\"\"\n\n    input_directory = Path(base_directory) / \"input\"\n    files = [file for file in input_directory.glob(\"*.csv\")]\n    \n    if len(files) == 0:\n        raise ValueError(f\"The are no CSV files in {str(input_directory)}/\")\n        \n    raw_data = [pd.read_csv(file) for file in files]\n    df = pd.concat(raw_data)\n    \n    # Shuffle the data\n    return df.sample(frac=1, random_state=42)\n\n\ndef _split_data(df):\n    \"\"\"\n    Splits the data into three sets: train, validation and test.\n    \"\"\"\n\n    df_train, temp = train_test_split(df, test_size=0.3)\n    df_validation, df_test = train_test_split(temp, test_size=0.5)\n\n    return df_train, df_validation, df_test\n\n\ndef _save_baseline(base_directory, df_test):\n    \"\"\"\n    This function saves the untransformed test split to disk. This file will\n    be used later as a baseline to monitor the performance of the model.\n    \"\"\"\n\n    baseline_path = Path(base_directory) / f\"baseline\"\n    baseline_path.mkdir(parents=True, exist_ok=True)\n    df_test.to_csv(baseline_path / \"baseline.csv\", header=False, index=False)\n\n\ndef _save_splits(base_directory, X_train, y_train, X_validation, y_validation, X_test, y_test):\n    \"\"\"\n    This function concatenates the transformed features and the target variable, and\n    saves each one of the split sets to disk.\n    \"\"\"\n\n    train = np.concatenate((X_train, y_train), axis=1)\n    validation = np.concatenate((X_validation, y_validation), axis=1)\n    test = np.concatenate((X_test, y_test), axis=1)\n\n    train_path = Path(base_directory) / \"train\"\n    validation_path = Path(base_directory) / \"validation\"\n    test_path = Path(base_directory) / \"test\"\n\n    train_path.mkdir(parents=True, exist_ok=True)\n    validation_path.mkdir(parents=True, exist_ok=True)\n    test_path.mkdir(parents=True, exist_ok=True)\n\n    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n    pd.DataFrame(validation).to_csv(validation_path / \"validation.csv\", header=False, index=False)\n    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n\n\ndef _save_model(base_directory, target_transformer, features_transformer):\n    \"\"\"\n    This function creates a model.tar.gz file that contains the two transformation\n    pipelines we built to transform the data.\n    \"\"\"\n\n    with tempfile.TemporaryDirectory() as directory:\n        joblib.dump(target_transformer, os.path.join(directory, \"target.joblib\"))\n        joblib.dump(features_transformer, os.path.join(directory, \"features.joblib\"))\n    \n        model_path = Path(base_directory) / \"model\"\n        model_path.mkdir(parents=True, exist_ok=True)\n    \n        with tarfile.open(f\"{str(model_path / 'model.tar.gz')}\", \"w:gz\") as tar:\n            tar.add(os.path.join(directory, \"target.joblib\"), arcname=\"target.joblib\")\n            tar.add(os.path.join(directory, \"features.joblib\"), arcname=\"features.joblib\")\n\n    \nif __name__ == \"__main__\":\n    preprocess(base_directory=\"/opt/ml/processing\")\n\nOverwriting code/preprocessor.py\n\n\nLet’s test the script to ensure everything is working as expected.\n\nimport os\nimport shutil\nimport tarfile\nimport pytest\nimport tempfile\nimport joblib\nfrom preprocessor import preprocess\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    preprocess(base_directory=directory)\n    \n    yield directory\n    \n    shutil.rmtree(directory)\n\n\ndef test_preprocess_generates_data_splits(directory):\n    output_directories = os.listdir(directory)\n    \n    assert \"train\" in output_directories\n    assert \"validation\" in output_directories\n    assert \"test\" in output_directories\n\n\ndef test_preprocess_generates_baseline(directory):\n    output_directories = os.listdir(directory)\n\n    assert \"baseline\" in output_directories\n\n\ndef test_preprocess_creates_two_models(directory):\n    model_path = directory / \"model\"\n    tar = tarfile.open(model_path / \"model.tar.gz\", \"r:gz\")\n\n    assert \"features.joblib\" in tar.getnames()\n    assert \"target.joblib\" in tar.getnames()\n\n\ndef test_splits_are_transformed(directory):\n    train = pd.read_csv(directory / \"train\" / \"train.csv\", header=None)\n    validation = pd.read_csv(directory / \"validation\" / \"validation.csv\", header=None)\n    test = pd.read_csv(directory / \"test\" / \"test.csv\", header=None)\n\n    # After transforming the data, the number of features should be 7:\n    # * 3 - island (one-hot encoded)\n    # * 1 - culmen_length_mm = 1\n    # * 1 - culmen_depth_mm\n    # * 1 - flipper_length_mm\n    # * 1 - body_mass_g\n    number_of_features = 7\n\n    # The transformed splits should have an additional column for the target\n    # variable.\n    assert train.shape[1] == number_of_features + 1\n    assert validation.shape[1] == number_of_features + 1\n    assert test.shape[1] == number_of_features + 1\n\n\ndef test_baseline_is_not_transformed(directory):\n    baseline = pd.read_csv(directory / \"baseline\" / \"baseline.csv\", header=None)\n\n    island = baseline.iloc[:, 1].unique()\n\n    assert \"Biscoe\" in island\n    assert \"Torgersen\" in island\n    assert \"Dream\" in island\n\n.....\n5 passed in 0.30s\n\n\nThe first step we need in the pipeline is a Processing Step to run the preprocessing script. This Processing Step will create a SageMaker Processing Job in the background, run the script, and upload the output to S3. You can use Processing Jobs to perform data preprocessing, post-processing, feature engineering, data validation, and model evaluation. Check the ProcessingStep SageMaker’s SDK documentation for more information.\nWe can parameterize a SageMaker Pipeline to make it more flexible. To read more about these parameters, check Pipeline Parameters. The dataset_location represents the location of the data we can to process in our pipeline. We can execute the pipeline with different datasets by changing the value of this parameter.\nA processor gives the Processing Step information about the hardware and software that SageMaker should use to launch the Processing Job. To run the script, we need access to Scikit-Learn, so we can use the SKLearnProcessor processor that comes out-of-the-box with the SageMaker’s Python SDK. The Data Processing with Framework Processors page discusses other built-in processors you can use. The Docker Registry Paths and Example Code page contains information about the available framework versions for each region.\nThe ProcessingStep requires a list of inputs that we need on the preprocessing script. In this case, the input is the dataset we stored in S3. We also have a few outputs that we want SageMaker to capture when the Processing Job finishes.\n\nfrom sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.workflow.steps import ProcessingStep\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\nfrom sagemaker.workflow.parameters import ParameterString\n\n\ndataset_location = ParameterString(\n    name=\"dataset_location\",\n    default_value=f\"{S3_LOCATION}/data\",\n)\n\nprocessor = SKLearnProcessor(\n    base_job_name=\"split-and-transform-data\",\n    framework_version=\"1.2-1\",\n\n    # By default, a new account doesn't have access to `ml.m5.xlarge` instances.\n    # If you haven't requested a quota increase yet, you can use an\n    # `ml.t3.medium` instance type instead. This will work out of the box, but\n    # the Processing Job will take significantly longer than it should have.\n    # To get access to `ml.m5.xlarge` instances, you can request a quota \n    # increase under the Service Quotas section in your AWS account.\n    instance_type=config[\"instance_type\"],\n    \n    instance_count=1,\n    role=role,\n    sagemaker_session=config[\"session\"],\n)\n\nsplit_and_transform_data_step = ProcessingStep(\n    name=\"split-and-transform-data\",\n    step_args=processor.run(\n        code=f\"{CODE_FOLDER}/preprocessor.py\",\n        inputs=[\n            ProcessingInput(source=dataset_location, destination=\"/opt/ml/processing/input\"),  \n        ],\n        outputs=[\n            ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n            ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n            ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n            ProcessingOutput(output_name=\"model\", source=\"/opt/ml/processing/model\"),\n            \n            # The baseline output points to the test set before transforming the data. This set\n            # will be helpful to generate a quality baseline for the model performance.\n            ProcessingOutput(output_name=\"baseline\", source=\"/opt/ml/processing/baseline\"),\n        ]\n    ),\n    cache_config=cache_config\n)\n\nINFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n/Users/svpino/dev/ml.school/.venv/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning:\n\nRunning within a PipelineSession, there will be No Wait, No Logs, and No Job being started."
  },
  {
    "objectID": "cohort-old.html#training",
    "href": "cohort-old.html#training",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Training",
    "text": "Training\nWe’ll use a Training Step to build a model.\nThis following script is responsible for training a neural network using the train data, validating the model, and saving it so we can later use it.\n\nimport os\nimport argparse\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom pathlib import Path\nfrom sklearn.metrics import accuracy_score\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\n\n\ndef train(model_directory, train_path, validation_path, epochs=50, batch_size=32):\n    train_files = [file for file in Path(train_path).glob(\"*.csv\")]\n    validation_files = [file for file in Path(validation_path).glob(\"*.csv\")]\n    \n    if len(train_files) == 0 or len(validation_files) == 0:\n        raise ValueError(\"The are no train or validation files\")\n        \n    train_data = [pd.read_csv(file, header=None) for file in train_files]\n    X_train = pd.concat(train_data)\n    y_train = X_train[X_train.columns[-1]]\n    X_train.drop(X_train.columns[-1], axis=1, inplace=True)\n    \n    \n    validation_data = [pd.read_csv(file, header=None) for file in validation_files]\n    X_validation = pd.concat(validation_data)\n    y_validation = X_validation[X_validation.columns[-1]]\n    X_validation.drop(X_validation.columns[-1], axis=1, inplace=True)\n    \n    model = Sequential([\n        Dense(10, input_shape=(X_train.shape[1],), activation=\"relu\"),\n        Dense(8, activation=\"relu\"),\n        Dense(3, activation=\"softmax\"),\n    ])\n    \n    model.compile(\n        optimizer=SGD(learning_rate=0.01),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n\n    model.fit(\n        X_train, \n        y_train, \n        validation_data=(X_validation, y_validation),\n        epochs=epochs, \n        batch_size=batch_size,\n        verbose=2,\n    )\n\n    predictions = np.argmax(model.predict(X_validation), axis=-1)\n    print(f\"Validation accuracy: {accuracy_score(y_validation, predictions)}\")\n    \n    model_filepath = Path(model_directory) / \"001\"\n    model.save(model_filepath)    \n    \n\nif __name__ == \"__main__\":\n    # Any hyperparameters provided by the training job are passed to \n    # the entry point as script arguments. \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--epochs\", type=int, default=50)\n    parser.add_argument(\"--batch_size\", type=int, default=32)\n    args, _ = parser.parse_known_args()\n    \n\n    train(\n        # This is the location where we need to save our model. SageMaker will\n        # create a model.tar.gz file with anything inside this directory when\n        # the training script finishes.\n        model_directory=os.environ[\"SM_MODEL_DIR\"],\n\n        # SageMaker creates one channel for each one of the inputs to the\n        # Training Step.\n        train_path=os.environ[\"SM_CHANNEL_TRAIN\"],\n        validation_path=os.environ[\"SM_CHANNEL_VALIDATION\"],\n\n        epochs=args.epochs,\n        batch_size=args.batch_size,\n    )\n\nOverwriting code/train.py\n\n\nLet’s test the script to ensure everything is working as expected.\n\nimport os\nimport shutil\nimport tarfile\nimport pytest\nimport tempfile\nimport joblib\n\nfrom preprocessor import preprocess\nfrom train import train\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    \n    preprocess(base_directory=directory)\n    train(\n        model_directory=directory / \"model\",\n        train_path=directory / \"train\", \n        validation_path=directory / \"validation\",\n        epochs=1\n    )\n    \n    yield directory\n    \n    shutil.rmtree(directory)\n\n\ndef test_train_saves_a_folder_with_model_assets(directory):\n    output = os.listdir(directory / \"model\")\n    assert \"001\" in output\n    \n    assets = os.listdir(directory / \"model\" / \"001\")\n    assert \"saved_model.pb\" in assets\n\nWARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\nINFO:tensorflow:Assets written to: /var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp04ai5la6/model/001/assets\n\n\n8/8 - 0s - loss: 1.0332 - accuracy: 0.4417 - val_loss: 1.0067 - val_accuracy: 0.4808 - 202ms/epoch - 25ms/step\n2/2 [==============================] - 0s 1ms/step\nValidation accuracy: 0.4807692307692308\nINFO:tensorflow:Assets written to: /var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp04ai5la6/model/001/assets\n.\n1 passed in 0.45s\n\n\nSageMaker uses the concept of an Estimator to handle end-to-end training tasks. For this example, we will use the built-in TensorFlow Estimator to run the training script we wrote before. The Docker Registry Paths and Example Code page contains information about the available framework versions for each region. Here, you can also check the available SageMaker Deep Learning Container images.\n\nfrom sagemaker.tensorflow import TensorFlow\n\n\nestimator = TensorFlow(\n    base_job_name=\"training\",\n    entry_point=f\"{CODE_FOLDER}/train.py\",\n    \n    # SageMaker will pass these hyperparameters as arguments\n    # to the entry point of the training script.\n    hyperparameters={\n        \"epochs\": 50,\n        \"batch_size\": 32,\n    },\n\n    # SageMaker will track these metrics as part of the experiment\n    # associated to this pipeline. The metric definitions tells \n    # SageMaker how to parse the values from the Training Job logs.\n    metric_definitions=[\n        {\"Name\": \"loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\"},\n        {\"Name\": \"accuracy\", \"Regex\": \"accuracy: ([0-9\\\\.]+)\"},\n        {\"Name\": \"val_loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n        {\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"},\n    ],\n\n    image_uri=config[\"image\"],\n    framework_version=config[\"framework_version\"],\n    py_version=config[\"py_version\"],\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    disable_profiler=True,\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\nWe can now create a Training Step. This Training Step will create a SageMaker Training Job in the background, run the training script, and upload the output to S3. Check the TrainingStep SageMaker’s SDK documentation for more information.\nThis step will receive the train and validation split from the previous step as inputs.\nHere, we are using two input channels, train and validation. SageMaker will automatically create an environment variable corresponding to each of these channels following the format SM_CHANNEL_[channel_name]:\n\nSM_CHANNEL_TRAIN: This environment variable will contain the path to the data in the train channel\nSM_CHANNEL_VALIDATION: This environment variable will contain the path to the data in the validation channel\n\n\nfrom sagemaker.workflow.steps import TrainingStep\nfrom sagemaker.inputs import TrainingInput\n\n\ntrain_model_step = TrainingStep(\n    name=\"train-model\",\n    step_args=estimator.fit(\n        inputs={\n            \"train\": TrainingInput(\n                s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n                    \"train\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\"\n            ),\n            \"validation\": TrainingInput(\n                s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n                    \"validation\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\"\n            )\n        }\n    ),\n    cache_config=cache_config\n)\n\n/Users/svpino/dev/ml.school/.venv/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning:\n\nRunning within a PipelineSession, there will be No Wait, No Logs, and No Job being started."
  },
  {
    "objectID": "cohort-old.html#tuning",
    "href": "cohort-old.html#tuning",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Tuning",
    "text": "Tuning\nWe can also use a Tuning Step to build a model. This Tuning Step will create a SageMaker Hyperparameter Tuning Job in the background and use the training script to train different versions of the model to choose the best one.\nSince we could use the Training of the Tuning Step to create the model, we’ll define a constant USE_TUNING_STEP to indicate which approach we want to run.\n\nUSE_TUNING_STEP = False\n\nThe TuningStep requires a HyperparameterTuner reference to configure the Hyperparameter Tuning Job.\nHere is the configuration that we’ll use to find the best model:\n\nobjective_metric_name: This is the name of the metric the tuner will use to determine the best model. We’ll use val_accuracy to select the model with the highest validation accuracy.\nobjective_type: This is the objective of the tuner. It can be Minimize or Maximize. Since we are using the validation accuracy of the model, we want the objective to be Maximize. If we were using the loss of the model, we would set the objective to Minimize.\nmetric_definitions: This defines how SageMaker will determine the metric’s value by looking at the output logs of the training process.\n\nThe tuner expects the list of the hyperparameters you want to explore. You can use subclasses of the Parameter class to specify different types of hyperparameters. This example explores different values for the epochs hyperparameter.\nYou can control the number of jobs and how many of them will run in parallel using the following two arguments:\n\nmax_jobs: Defines the maximum total number of training jobs to start for the hyperparameter tuning job.\nmax_parallel_jobs: Defines the maximum number of parallel training jobs to start.\n\nFinally, we’ll create the Tuning Step using the tuner. Notice how the Tuning Step looks very similar to the Training Step.\n\nfrom sagemaker.tuner import HyperparameterTuner\nfrom sagemaker.parameter import IntegerParameter\nfrom sagemaker.workflow.steps import TuningStep\n\n\ntuner = HyperparameterTuner(\n    estimator,\n    objective_metric_name = \"val_accuracy\",\n    objective_type=\"Maximize\",\n    hyperparameter_ranges = {\n        \"epochs\": IntegerParameter(10, 50),\n    },\n    metric_definitions = [\n        {\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}\n    ],\n    max_jobs=3,\n    max_parallel_jobs=3,\n)\n\ntune_model_step = TuningStep(\n    name = \"tune-model\",\n    step_args=tuner.fit(\n        inputs={\n            \"train\": TrainingInput(\n                s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n                    \"train\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\"\n            ),\n            \"validation\": TrainingInput(\n                s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n                    \"validation\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\"\n            )\n        },\n    ),\n    cache_config=cache_config\n)"
  },
  {
    "objectID": "cohort-old.html#evaluation",
    "href": "cohort-old.html#evaluation",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Evaluation",
    "text": "Evaluation\nWe’ll use a Processing Step to evaluate the model.\nLet’s create the evaluation script. The Processing Step will spin up a Processing Job and run this script inside a container. This script is responsible for loading the model we created and evaluating it on the test set. Before finishing, this script will generate an evaluation report of the model.\n\nimport os\nimport json\nimport tarfile\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom tensorflow import keras\nfrom sklearn.metrics import accuracy_score\n\n\nMODEL_PATH = \"/opt/ml/processing/model/\"\nTEST_PATH = \"/opt/ml/processing/test/\"\nOUTPUT_PATH = \"/opt/ml/processing/evaluation/\"\n\n\ndef evaluate(model_path, test_path, output_path):\n    # The first step is to extract the model package so we can load \n    # it in memory.\n    with tarfile.open(Path(model_path) / \"model.tar.gz\") as tar:\n        tar.extractall(path=Path(model_path))\n        \n    model = keras.models.load_model(Path(model_path) / \"001\")\n    \n    X_test = pd.read_csv(Path(test_path) / \"test.csv\")\n    y_test = X_test[X_test.columns[-1]]\n    X_test.drop(X_test.columns[-1], axis=1, inplace=True)\n    \n    predictions = np.argmax(model.predict(X_test), axis=-1)\n    accuracy = accuracy_score(y_test, predictions)\n    print(f\"Test accuracy: {accuracy}\")\n\n    # Let's create an evaluation report using the model accuracy.\n    evaluation_report = {\n        \"metrics\": {\n            \"accuracy\": {\n                \"value\": accuracy\n            },\n        },\n    }\n    \n    Path(output_path).mkdir(parents=True, exist_ok=True)\n    with open(Path(output_path) / \"evaluation.json\", \"w\") as f:\n        f.write(json.dumps(evaluation_report))\n        \n        \nif __name__ == \"__main__\":\n    evaluate(\n        model_path=MODEL_PATH, \n        test_path=TEST_PATH,\n        output_path=OUTPUT_PATH\n    )\n\nOverwriting code/evaluation.py\n\n\nLet’s test the script to ensure everything is working as expected.\n\nimport os\nimport shutil\nimport tarfile\nimport pytest\nimport tempfile\nimport joblib\nimport json\n\nfrom preprocessor import preprocess\nfrom train import train\nfrom evaluation import evaluate\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    \n    preprocess(base_directory=directory)\n    \n    train(\n        model_directory=directory / \"model\",\n        train_path=directory / \"train\", \n        validation_path=directory / \"validation\",\n        epochs=1\n    )\n    \n    # After training a model, we need to prepare a package just like\n    # SageMaker would. This package is what the evaluation script is\n    # expecting as an input.\n    with tarfile.open(directory / \"model.tar.gz\", \"w:gz\") as tar:\n        tar.add(directory / \"model\" / \"001\", arcname=\"001\")\n        \n    evaluate(\n        model_path=directory, \n        test_path=directory / \"test\",\n        output_path=directory / \"evaluation\",\n    )\n\n    yield directory / \"evaluation\"\n    \n    shutil.rmtree(directory)\n\n\ndef test_evaluate_generates_evaluation_report(directory):\n    output = os.listdir(directory)\n    assert \"evaluation.json\" in output\n\n\ndef test_evaluation_report_contains_accuracy(directory):\n    with open(directory / \"evaluation.json\", 'r') as file:\n        report = json.load(file)\n        \n    assert \"metrics\" in report\n    assert \"accuracy\" in report[\"metrics\"]\n\nWARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\nINFO:tensorflow:Assets written to: /var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmpf79nisc5/model/001/assets\nWARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\nWARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\nWARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\nINFO:tensorflow:Assets written to: /var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmpd_5_ehu4/model/001/assets\nWARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\nWARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n\n\n8/8 - 0s - loss: 1.7050 - accuracy: 0.2083 - val_loss: 1.5639 - val_accuracy: 0.1538 - 222ms/epoch - 28ms/step\n2/2 [==============================] - 0s 1ms/step\nValidation accuracy: 0.15384615384615385\nINFO:tensorflow:Assets written to: /var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmpf79nisc5/model/001/assets\n2/2 [==============================] - 0s 2ms/step\nTest accuracy: 0.19607843137254902\n.8/8 - 0s - loss: 1.2137 - accuracy: 0.0458 - val_loss: 1.1446 - val_accuracy: 0.0769 - 220ms/epoch - 27ms/step\n2/2 [==============================] - 0s 2ms/step\nValidation accuracy: 0.07692307692307693\nINFO:tensorflow:Assets written to: /var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmpd_5_ehu4/model/001/assets\n2/2 [==============================] - 0s 1ms/step\nTest accuracy: 0.0392156862745098\n.\n2 passed in 1.26s\n\n\nTo run the evaluation script, we’ll use a Processing Step with a TensorFlowProcessor.\nWe can use the USE_TUNING_STEP flag to determine whether we created the model using a Training Step or a Tuning Step. In case we are using the Tuning Step, we can use the TuningStep.get_top_model_s3_uri() function to get the model artifacts from the top performing training job of the Hyperparameter Tuning Job.\nThe ProcessingStep lets us specify a list of PropertyFile instances from the output of the job. We can use this to map the evaluation report generated in the evaluation script. Check How to Build and Manage Property Files for more information.\n\nfrom sagemaker.tensorflow import TensorFlowProcessor\nfrom sagemaker.workflow.properties import PropertyFile\n\n\ntensorflow_processor = TensorFlowProcessor(\n    base_job_name=\"evaluation-processor\",\n    image_uri=config[\"image\"],\n    framework_version=config[\"framework_version\"],\n    py_version=config[\"py_version\"],\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    role=role,\n    sagemaker_session=config[\"session\"],\n)\n\n# We want to map the evaluation report that we generate inside\n# the evaluation script so we can later reference it.\nevaluation_report = PropertyFile(\n    name=\"evaluation-report\",\n    output_name=\"evaluation\",\n    path=\"evaluation.json\"\n)\n\n# Notice how this step uses the model generated by the tuning or training\n# step, and the test set generated by the preprocessing step.\nevaluate_model_step = ProcessingStep(\n    name=\"evaluate-model\",\n    step_args=tensorflow_processor.run(\n        inputs=[\n            ProcessingInput(\n                source=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n                    \"test\"\n                ].S3Output.S3Uri,\n                destination=\"/opt/ml/processing/test\"\n            ),\n            ProcessingInput(\n                source=(\n                    tune_model_step.get_top_model_s3_uri(top_k=0, s3_bucket=config[\"session\"].default_bucket()) \n                    if USE_TUNING_STEP \n                    else train_model_step.properties.ModelArtifacts.S3ModelArtifacts\n                ),\n                destination=\"/opt/ml/processing/model\",\n            )\n        ],\n        outputs=[\n            ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n        ],\n        code=f\"{CODE_FOLDER}/evaluation.py\"\n    ),\n    property_files=[evaluation_report],\n    cache_config=cache_config\n)\n\n/Users/svpino/dev/ml.school/.venv/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning:\n\nRunning within a PipelineSession, there will be No Wait, No Logs, and No Job being started."
  },
  {
    "objectID": "cohort-old.html#inference-pipeline",
    "href": "cohort-old.html#inference-pipeline",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Inference Pipeline",
    "text": "Inference Pipeline\nDeploying the model we trained directly to an endpoint doesn’t lets us control the data that goes in and comes out of the endpoint. The TensorFlow model we trained requires the data to come in a specific format, which makes it useless to other applications. Fortunately, we can create an Inference Pipeline using SageMaker to control the data that goes in and comes out of the endpoint.\nOur inference pipeline will have three components:\n\nA preprocessing transformer that will transform the input data into the format the model expects.\nThe TensorFlow model we trained.\nA postprocessing transformer that will transform the output of the model into a human-readable format.\n\nWe want our endpoint to handle unprocessed data in CSV and JSON format and return the penguin’s species. Here is an example of the payload input we want the endpoint to support:\n{\n    \"island\": \"Biscoe\",\n    \"culmen_length_mm\": 48.6,\n    \"culmen_depth_mm\": 16.0,\n    \"flipper_length_mm\": 230.0,\n    \"body_mass_g\": 5800.0,\n}\nAnd here is an example of the output we’d like to get from the endpoint:\n{\n    \"prediction\": \"Adelie\", \n    \"confidence\": 0.802672\n}\nLet’s start by setting up a local folder where we will create the inference.py script.\n\nINFERENCE_CODE_FOLDER = CODE_FOLDER / \"inference\"\nPath(INFERENCE_CODE_FOLDER).mkdir(parents=True, exist_ok=True)\nsys.path.append(f\"./{INFERENCE_CODE_FOLDER}\")\n\n\nPreprocessing Script\nThe first component of our inference pipeline is a transformer that will transform the input data into the format the model expects. We’ll use the Scikit-Learn transformer we saved when we split and transformed the data. To deploy this transformer as part of an inference pipeline, we need to write a script that loads the transformer, uses it to modify the input data, and returns the output in the format the TensorFlow model expects.\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport json\nimport joblib\n\nfrom io import StringIO\n\ntry:\n    from sagemaker_containers.beta.framework import encoders, worker\nexcept ImportError:\n    # We don't have access to the `worker` instance when testing locally. \n    # We'll set it to None so we can change the way functions create a response.\n    worker = None\n\n\nTARGET_COLUMN = \"species\"\nFEATURE_COLUMNS = [\n    \"island\",\n    \"culmen_length_mm\",\n    \"culmen_depth_mm\", \n    \"flipper_length_mm\",\n    \"body_mass_g\",\n    \"sex\"\n]\n\n\ndef input_fn(input_data, content_type):\n    \"\"\"\n    Parses the input payload and creates a Pandas DataFrame.\n    \n    This function will check whether the target column is present in the\n    input data, and will remove it.\n    \"\"\"\n    \n    if content_type == \"text/csv\":\n        df = pd.read_csv(StringIO(input_data), header=None, skipinitialspace=True)\n\n        if len(df.columns) == len(FEATURE_COLUMNS) + 1:\n            df = df.drop(df.columns[0], axis=1)\n        \n        df.columns = FEATURE_COLUMNS\n        return df\n    \n    if content_type == \"application/json\":\n        df = pd.DataFrame([json.loads(input_data)])\n        \n        if \"species\" in df.columns:\n            df = df.drop(\"species\", axis=1)\n        \n        return df\n    \n    else:\n        raise ValueError(f\"{content_type} is not supported.!\")\n\n\ndef output_fn(prediction, accept):\n    \"\"\"\n    Formats the prediction output to generate a response.\n    \n    The default accept/content-type between containers for serial inference is JSON. \n    Since this model will preceed a TensorFlow model, we want to return a JSON object\n    following TensorFlow's input requirements.\n    \"\"\"\n    \n    if prediction is None:\n        raise Exception(f\"There was an error transforming the input data\")\n\n    if accept == \"text/csv\":\n        return worker.Response(encoders.encode(prediction, accept), mimetype=accept) if worker else prediction, accept \n    \n    if accept == \"application/json\":\n        instances = [p for p in prediction.tolist()]\n        response = {\"instances\": instances}\n        return worker.Response(json.dumps(response), mimetype=accept) if worker else (response, accept)\n\n    raise Exception(f\"{accept} accept type is not supported.\")\n\n\ndef predict_fn(input_data, model):\n    \"\"\"\n    Preprocess the input using the transformer.\n    \"\"\"\n    \n    try:\n        response = model.transform(input_data)\n        return response\n    except ValueError as e:\n        print(\"Error transforming the input data\", e)\n        return None\n\n\ndef model_fn(model_dir):\n    \"\"\"\n    Deserializes the model that will be used in this container.\n    \"\"\"\n    \n    return joblib.load(os.path.join(model_dir, \"features.joblib\"))\n\nOverwriting code/inference/preprocessing_component.py\n\n\nLet’s test the script to ensure everything is working as expected.\n\nimport json\n\nfrom preprocessing_component import input_fn, predict_fn, output_fn, model_fn\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    \n    preprocess(base_directory=directory)\n    \n    with tarfile.open(directory / \"model\" / \"model.tar.gz\") as tar:\n        tar.extractall(path=directory / \"model\")\n    \n    yield directory / \"model\"\n    \n    shutil.rmtree(directory)\n\n\n\ndef test_input_csv_drops_target_column_if_present():\n    input_data = \"\"\"\n    Adelie, Torgersen, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    df = input_fn(input_data, \"text/csv\")\n    assert len(df.columns) == 6 and \"species\" not in df.columns\n\n\ndef test_input_json_drops_target_column_if_present():\n    input_data = json.dumps({\n        \"species\": \"Adelie\", \n        \"island\": \"Torgersen\",\n        \"culmen_length_mm\": 44.1,\n        \"culmen_depth_mm\": 18.0,\n        \"flipper_length_mm\": 210.0,\n        \"body_mass_g\": 4000.0,\n        \"sex\": \"MALE\"\n    })\n    \n    df = input_fn(input_data, \"application/json\")\n    assert len(df.columns) == 6 and \"species\" not in df.columns\n\n\ndef test_input_csv_works_without_target_column():\n    input_data = \"\"\"\n    Torgersen, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    df = input_fn(input_data, \"text/csv\")\n    assert len(df.columns) == 6\n\n\ndef test_input_json_works_without_target_column():\n    input_data = json.dumps({\n        \"island\": \"Torgersen\",\n        \"culmen_length_mm\": 44.1,\n        \"culmen_depth_mm\": 18.0,\n        \"flipper_length_mm\": 210.0,\n        \"body_mass_g\": 4000.0,\n        \"sex\": \"MALE\"\n    })\n    \n    df = input_fn(input_data, \"application/json\")\n    assert len(df.columns) == 6\n\n\ndef test_output_csv_raises_exception_if_prediction_is_none():\n    with pytest.raises(Exception):\n        output_fn(None, \"text/csv\")\n    \n    \ndef test_output_json_raises_exception_if_prediction_is_none():\n    with pytest.raises(Exception):\n        output_fn(None, \"application/json\")\n    \n    \ndef test_output_csv_returns_prediction():\n    prediction = np.array([\n        [-1.3944109908736013,1.15488062669371,-0.7954340636549508,-0.5536447804097907,0.0,1.0,0.0],\n        [1.0557485835338234,0.5040085971987002,-0.5824506029515057,-0.5851840035995248,0.0,1.0,0.0]\n    ])\n    \n    response = output_fn(prediction, \"text/csv\")\n    \n    assert response == (prediction, \"text/csv\")\n    \n    \ndef test_output_json_returns_tensorflow_ready_input():\n    prediction = np.array([\n        [-1.3944109908736013,1.15488062669371,-0.7954340636549508,-0.5536447804097907,0.0,1.0,0.0],\n        [1.0557485835338234,0.5040085971987002,-0.5824506029515057,-0.5851840035995248,0.0,1.0,0.0]\n    ])\n    \n    response = output_fn(prediction, \"application/json\")\n    \n    assert response[0] == {\n        \"instances\": [\n            [-1.3944109908736013,1.15488062669371,-0.7954340636549508,-0.5536447804097907,0.0,1.0,0.0],\n            [1.0557485835338234,0.5040085971987002,-0.5824506029515057,-0.5851840035995248,0.0,1.0,0.0]\n        ]\n    }\n    \n    assert response[1] == \"application/json\"\n\n    \ndef test_predict_transforms_data(directory):\n    input_data = \"\"\"\n    Torgersen, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    model = model_fn(str(directory))\n    df = input_fn(input_data, \"text/csv\")\n    response = predict_fn(df, model)\n    assert type(response) is np.ndarray\n    \n\ndef test_predict_returns_none_if_invalid_input(directory):\n    input_data = \"\"\"\n    Invalid, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    model = model_fn(str(directory))\n    df = input_fn(input_data, \"text/csv\")\n    assert predict_fn(df, model) is None\n\n..........                                                                                   [100%]\n10 passed in 0.06s\n\n\n\n\nPostprocessing Script\nThe final component of our inference pipeline is a transformer that will transform the output from the model into a human-readable format. We’ll use the Scikit-Learn target transformer we saved when we split and transformed the data. To deploy this transformer as part of an inference pipeline, we need to write a script that loads the transformer, uses it to modify the output from the model, and returns a human-readable format.\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport json\nimport tarfile\nimport joblib\n\nfrom pathlib import Path\nfrom io import StringIO\n\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, OrdinalEncoder\nfrom pickle import dump, load\n\n\ntry:\n    from sagemaker_containers.beta.framework import encoders, worker\nexcept ImportError:\n    # We don't have access to the `worker` instance when testing locally. \n    # We'll set it to None so we can change the way functions create a response.\n    worker = None\n\n\ndef input_fn(input_data, content_type):\n    if content_type == \"application/json\":\n        predictions = json.loads(input_data)[\"predictions\"]\n        return predictions\n    \n    else:\n        raise ValueError(f\"{content_type} is not supported.!\")\n\n\ndef output_fn(prediction, accept):\n    if accept == \"text/csv\":\n        return worker.Response(encoders.encode(prediction, accept), mimetype=accept) if worker else (prediction, accept)\n    \n    if accept == \"application/json\":\n        response = []\n        for p, c in prediction:\n            response.append({\n                \"prediction\": p,\n                \"confidence\": c\n            })\n            \n        return worker.Response(json.dumps(response), mimetype=accept) if worker else (response, accept)\n    \n    raise RuntimeException(f\"{accept} accept type is not supported.\")\n\n\ndef predict_fn(input_data, model):\n    \"\"\"\n    Transforms the prediction into its corresponding category.\n    \"\"\"\n    \n    predictions = np.argmax(input_data, axis=-1)\n    confidence = np.max(input_data, axis=-1)\n    return [(confidence, model[prediction]) for confidence, prediction in zip(confidence, predictions)]\n\n\ndef model_fn(model_dir):\n    \"\"\"\n    Deserializes the target model and returns the list of fitted categories.\n    \"\"\"\n    \n    model = joblib.load(os.path.join(model_dir, \"target.joblib\"))\n    return model.named_transformers_[\"species\"].categories_[0]\n\nOverwriting code/inference/postprocessing_component.py\n\n\nLet’s test the script to ensure everything is working as expected.\n\nimport json\nimport numpy as np\n\nfrom postprocessing_component import predict_fn\n\n\ndef test_predict_returns_prediction_as_last_column():\n    input_data = [\n        [0.6, 0.2, 0.2], \n        [0.1, 0.8, 0.1],\n        [0.2, 0.1, 0.7]\n    ]\n    \n    categories = [\"Adelie\", \"Gentoo\", \"Chinstrap\"]\n    \n    response = predict_fn(input_data, categories)\n    \n    assert response == [\n        (0.6, \"Adelie\"),\n        (0.8, \"Gentoo\"),\n        (0.7, \"Chinstrap\")\n    ]\n\n.                                                                                            [100%]\n1 passed in 0.01s\n\n\n\n\nPipeline Model\nWe can now create a PipelineModel to define our inference pipeline. This Pipeline Model will create a SageMaker Model in the background and use the preprocessing and postprocessing scripts to transform the input and output of the model.\n\nfrom sagemaker.workflow.functions import Join\nfrom sagemaker.sklearn.model import SKLearnModel\nfrom sagemaker.tensorflow.model import TensorFlowModel\nfrom sagemaker.pipeline import PipelineModel\n\n\n# We'll use the model we generated from the first step of the\n# pipeline as the input to the first and last components of the\n# inference pipeline. This model.tar.gz file contains the two\n# transformers we need to preprocess and postprocess the data.\ntransformation_pipeline_model = Join(\n    on=\"/\",\n    values=[\n        split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n            \"model\"\n        ].S3Output.S3Uri,\n        \"model.tar.gz\",\n    ],\n)\n\n# This is the first component of the inference pipeline. It will\n# preprocess the data before sending it to the TensorFlow model.\npreprocessing_model = SKLearnModel(\n    model_data=transformation_pipeline_model,\n    entry_point=\"preprocessing_component.py\",\n    source_dir=str(INFERENCE_CODE_FOLDER),\n    framework_version=\"1.2-1\",\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\ntensorflow_model = TensorFlowModel(\n    model_data=(\n        tune_model_step.get_top_model_s3_uri(\n            top_k=0, s3_bucket=config[\"session\"].default_bucket()\n        )\n        if USE_TUNING_STEP\n        else train_model_step.properties.ModelArtifacts.S3ModelArtifacts\n    ),\n    image_uri=config[\"image\"],\n    framework_version=config[\"framework_version\"],\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\n# This is the last component of the inference pipeline. It will\n# postprocess the output from the TensorFlow model before sending \n# it back to the user.\npost_processing_model = SKLearnModel(\n    model_data=transformation_pipeline_model,\n    entry_point=\"postprocessing_component.py\",\n    source_dir=str(INFERENCE_CODE_FOLDER),\n    framework_version=\"1.2-1\",\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\n# We can now create the inference pipeline using the three models.\npipeline_model = PipelineModel(\n    name=\"inference-model\",\n    models=[preprocessing_model, tensorflow_model, post_processing_model],\n    sagemaker_session=config[\"session\"],\n    role=role,\n)"
  },
  {
    "objectID": "cohort-old.html#data-quality-baseline",
    "href": "cohort-old.html#data-quality-baseline",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Data Quality Baseline",
    "text": "Data Quality Baseline\nWe need to create a data quality baseline to compare against the real-time traffic our endpoint receives. This baseline will help us detect any data dristribution shifts.\nWe’ll use a Quality Check Step to generate the baseline and configure the instance that will run the quality check using the CheckJobConfig class.\n\nfrom sagemaker.workflow.check_job_config import CheckJobConfig\nfrom sagemaker.workflow.quality_check_step import (\n    DataQualityCheckConfig,\n    QualityCheckStep,\n)\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\n\n\nDATA_QUALITY_LOCATION = f\"{S3_LOCATION}/monitoring/data-quality\"\n\n\ndata_quality_baseline_step = QualityCheckStep(\n    name=\"generate-data-quality-baseline\",\n    check_job_config=CheckJobConfig(\n        instance_type=\"ml.c5.xlarge\",\n        instance_count=1,\n        volume_size_in_gb=20,\n        sagemaker_session=pipeline_session,\n        role=role,\n    ),\n    quality_check_config=DataQualityCheckConfig(\n        baseline_dataset=f\"{S3_LOCATION}/data\",\n        dataset_format=DatasetFormat.csv(header=True, output_columns_position=\"END\"),\n        output_s3_uri=DATA_QUALITY_LOCATION,\n    ),\n    skip_check=True,\n    register_new_baseline=True,\n    cache_config=cache_config,\n)\n\nINFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\nINFO:sagemaker.image_uris:Ignoring unnecessary instance type: None."
  },
  {
    "objectID": "cohort-old.html#model-quality-baseline",
    "href": "cohort-old.html#model-quality-baseline",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Model Quality Baseline",
    "text": "Model Quality Baseline\nTo monitor the performance of the model, we need to generate a baseline performance. This baseline will help us detect any performance degradation.\nTo create the baseline, we must generate predictions for the test set and compare them with the predictions from the model. We can do this by running a Batch Transform Job to generate predictions for every sample from the test set. We can use a Transform Step as part of the pipeline to run this job. We’ll configure the Batch Transform Job using a Transform Step.\n\nfrom sagemaker.workflow.steps import TransformStep\nfrom sagemaker.workflow.model_step import ModelStep\nfrom sagemaker.transformer import Transformer\n\n# The Transform Step requires a model to generate predictions, \n# so we need to create the inference pipeline model.\ncreate_model_step = ModelStep(\n    name=\"create\",\n    display_name=\"create-model\",\n    step_args=pipeline_model.create(\n        instance_type=\"ml.m5.xlarge\"\n    ),\n)\n\ntransformer = Transformer(\n    model_name=create_model_step.properties.ModelName,\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    strategy=\"MultiRecord\",\n    accept=\"text/csv\",\n    assemble_with=\"Line\",\n    output_path=f\"{S3_LOCATION}/transform\",\n    sagemaker_session=config[\"session\"]\n)\n\ngenerate_test_predictions_step = TransformStep(\n    name=\"generate-test-predictions\",\n    step_args=transformer.transform(\n        # We will use the baseline set we generated when we split the data.\n        # This set corresponds to the test split before the transformation step.\n        data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\"baseline\"].S3Output.S3Uri,\n        \n        join_source=\"Input\",\n        split_type=\"Line\",\n        content_type=\"text/csv\",\n        input_filter=\"$\",\n        \n        # We want to output the first and the last field from the joint set.\n        # The first field corresponds to the groundtruth, and the last field\n        # corresponds to the prediction.\n        output_filter=\"$[0,-1]\",\n    ),\n    cache_config=cache_config\n)\n\n/Users/svpino/dev/ml.school/.venv/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning:\n\nRunning within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n\nINFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n\n\nLet’s now configure the Quality Check Step and feed it the data we generated in the Transform Step.\nWe’ll use a Model Quality Step to generate the baseline, and we’ll configure the instance that will run the quality check using the ModelQualityJobConfig class.\n\nfrom sagemaker.workflow.quality_check_step import ModelQualityCheckConfig\n\n\nMODEL_QUALITY_LOCATION = f\"{S3_LOCATION}/monitoring/model-quality\"\n\nmodel_quality_baseline_step = QualityCheckStep(\n    name=\"generate-model-quality-baseline\",\n    \n    check_job_config = CheckJobConfig(\n        instance_type=\"ml.c5.xlarge\",\n        instance_count=1,\n        volume_size_in_gb=20,\n        sagemaker_session=pipeline_session,\n        role=role,\n    ),\n    \n    quality_check_config = ModelQualityCheckConfig(\n        # We are going to use the output of the Transform Step to generate\n        # the model quality baseline.\n        baseline_dataset=generate_test_predictions_step.properties.TransformOutput.S3OutputPath,\n        dataset_format=DatasetFormat.csv(header=False),\n\n        # We need to specify the problem type and the fields where the prediction\n        # and groundtruth are so the process knows how to interpret the results.\n        problem_type=\"MulticlassClassification\",\n        \n        # Since the data doesn't have headers, SageMaker will autocreate headers for it.\n        # _c0 corresponds to the first column, and _c1 corresponds to the second column.\n        ground_truth_attribute=\"_c0\",\n        inference_attribute=\"_c1\",\n\n        output_s3_uri=MODEL_QUALITY_LOCATION,\n    ),\n    \n    skip_check=True,\n    register_new_baseline=True,\n    cache_config=cache_config\n)\n\nINFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\nINFO:sagemaker.image_uris:Ignoring unnecessary instance type: None."
  },
  {
    "objectID": "cohort-old.html#registration",
    "href": "cohort-old.html#registration",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Registration",
    "text": "Registration\nWe can now register the inference pipeline in the Model Registry.\nWhen we register a model, we can specify a set of ModelMetrics that will be saved in the Model Registry. We’ll use the metrics that we calculated using the Quality Check Steps.\n\nfrom sagemaker.model_metrics import MetricsSource, ModelMetrics \nfrom sagemaker.drift_check_baselines import DriftCheckBaselines\n\n\nmodel_metrics = ModelMetrics(\n    model_data_statistics=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineStatistics,\n        content_type=\"application/json\",\n    ),\n    model_data_constraints=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineConstraints,\n        content_type=\"application/json\",\n    ),\n    model_statistics=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.CalculatedBaselineStatistics,\n        content_type=\"application/json\",\n    ),\n    \n    model_constraints=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.CalculatedBaselineConstraints,\n        content_type=\"application/json\",\n    ),\n)\n\ndrift_check_baselines = DriftCheckBaselines(\n    model_data_statistics=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckStatistics,\n        content_type=\"application/json\",\n    ),\n    model_data_constraints=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckConstraints,\n        content_type=\"application/json\",\n    ),\n    model_statistics=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.BaselineUsedForDriftCheckStatistics,\n        content_type=\"application/json\",\n    ),\n    model_constraints=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.BaselineUsedForDriftCheckConstraints,\n        content_type=\"application/json\",\n    )\n)\n\nLet’s use a ModelStep to register the model.\n\nMODEL_PACKAGE_GROUP = \"penguins\"\n\nregister_model_step = ModelStep(\n    name=\"register\",\n    display_name=\"register-model\",\n    step_args=pipeline_model.register(\n        model_package_group_name=MODEL_PACKAGE_GROUP,\n        model_metrics=model_metrics,\n        drift_check_baselines=drift_check_baselines,\n        approval_status=\"PendingManualApproval\",\n        \n        # Our inference pipeline model supports two content \n        # types: text/csv and application/json.\n        content_types=[\"text/csv\", \"application/json\"],\n        response_types=[\"text/csv\", \"application/json\"],       \n\n        # This is the suggested inference instance types when \n        # deploying the model or using it as part of a batch\n        # transform job.\n        inference_instances=[\"ml.m5.xlarge\"],\n        transform_instances=[\"ml.m5.xlarge\"],\n        \n        domain=\"MACHINE_LEARNING\",\n        task=\"CLASSIFICATION\",\n        framework=\"TENSORFLOW\",\n        framework_version=config[\"framework_version\"],\n    )\n)"
  },
  {
    "objectID": "cohort-old.html#condition-step",
    "href": "cohort-old.html#condition-step",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Condition Step",
    "text": "Condition Step\nWe only want to register the model and generate the baseline predictions if the model’s accuracy exceeds a predefined threshold. We can use a Condition Step together with the evaluation report we generated to accomplish this. Check the ConditionStep SageMaker’s SDK documentation for more information. In this example, we will use a ConditionGreaterThanOrEqualTo condition to compare the model’s accuracy with the threshold. Look at the Conditions section in the documentation for more information about the types of supported conditions.\nIf the model’s accuracy is not greater than or equal our threshold, we will send the pipeline to a Fail Step with the appropriate error message. Check the FailStep SageMaker’s SDK documentation for more information.\nWe are going to use a new Pipeline Parameter in our pipeline to specify the minimum accuracy that the model should reach for it to be registered.\n\nfrom sagemaker.workflow.fail_step import FailStep\nfrom sagemaker.workflow.functions import JsonGet\nfrom sagemaker.workflow.functions import Join\nfrom sagemaker.workflow.parameters import ParameterFloat\nfrom sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\nfrom sagemaker.workflow.condition_step import ConditionStep\n\n\naccuracy_threshold = ParameterFloat(name=\"accuracy_threshold\", default_value=0.70)\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[\n        # We want to check whether the accuracy of the model is greater than\n        # the threshold we defined.\n        ConditionGreaterThanOrEqualTo(\n            left=JsonGet(\n                step_name=evaluate_model_step.name,\n                property_file=evaluation_report,\n                json_path=\"metrics.accuracy.value\",\n            ),\n            right=accuracy_threshold,\n        )\n    ],\n    if_steps=[]\n    if LOCAL_MODE\n    else [\n        create_model_step,\n        generate_test_predictions_step,\n        model_quality_baseline_step,\n        register_model_step,\n    ],\n    # If the condition is not met, we want to fail the execution\n    # of the pipeline by sending it to a FailStep.\n    else_steps=[\n        FailStep(\n            name=\"fail\",\n            error_message=Join(\n                on=\" \",\n                values=[\n                    \"Execution failed because the model's accuracy was lower than\",\n                    accuracy_threshold,\n                ],\n            ),\n        )\n    ],\n)"
  },
  {
    "objectID": "cohort-old.html#pipeline",
    "href": "cohort-old.html#pipeline",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Pipeline",
    "text": "Pipeline\nWe can now create the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nfrom sagemaker.workflow.pipeline import Pipeline\nfrom sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n\n\npipeline = Pipeline(\n    name=\"cohort-pipeline\",\n    parameters=[\n        dataset_location,\n        accuracy_threshold,\n    ],\n    steps=[\n        split_and_transform_data_step,\n        train_model_step if not USE_TUNING_STEP else tune_model_step,\n        evaluate_model_step,\n        condition_step\n    ],\n    pipeline_definition_config=PipelineDefinitionConfig(use_custom_job_prefix=True),\n    sagemaker_session=config[\"session\"],\n)\n\nif not LOCAL_MODE:\n    # SageMaker doesn't support running any of these steps in Local Mode.\n    pipeline.steps.extend([data_quality_baseline_step])\n\npipeline.upsert(role_arn=role)\n\n{'PipelineArn': 'cohort-pipeline'}\n\n\n\nTo run the pipeline, comment out the %%script cell magic line to execute the cell.\n\n\n# %%script false --no-raise-error\n\npipeline.start()\n\nWARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\nWARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\nINFO:sagemaker.processing:Uploaded None to s3://mlschool/evaluation-processor-2023-10-20-15-03-23-222/source/sourcedir.tar.gz\nINFO:sagemaker.processing:runproc.sh uploaded to s3://mlschool/evaluation-processor-2023-10-20-15-03-23-222/source/runproc.sh\nWARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\nINFO:sagemaker.processing:Uploaded None to s3://mlschool/evaluation-processor-2023-10-20-15-03-23-989/source/sourcedir.tar.gz\nINFO:sagemaker.processing:runproc.sh uploaded to s3://mlschool/evaluation-processor-2023-10-20-15-03-23-989/source/runproc.sh\nWARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\nINFO:sagemaker.processing:Uploaded None to s3://mlschool/evaluation-processor-2023-10-20-15-03-24-565/source/sourcedir.tar.gz\nINFO:sagemaker.processing:runproc.sh uploaded to s3://mlschool/evaluation-processor-2023-10-20-15-03-24-565/source/runproc.sh\nWARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\nWARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\nWARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\nINFO:sagemaker.processing:Uploaded None to s3://mlschool/evaluation-processor-2023-10-20-15-03-25-535/source/sourcedir.tar.gz\nINFO:sagemaker.processing:runproc.sh uploaded to s3://mlschool/evaluation-processor-2023-10-20-15-03-25-535/source/runproc.sh\nWARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\nINFO:sagemaker.processing:Uploaded None to s3://mlschool/evaluation-processor-2023-10-20-15-03-26-086/source/sourcedir.tar.gz\nINFO:sagemaker.processing:runproc.sh uploaded to s3://mlschool/evaluation-processor-2023-10-20-15-03-26-086/source/runproc.sh\nWARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\nINFO:sagemaker.processing:Uploaded None to s3://mlschool/evaluation-processor-2023-10-20-15-03-26-534/source/sourcedir.tar.gz\nINFO:sagemaker.processing:runproc.sh uploaded to s3://mlschool/evaluation-processor-2023-10-20-15-03-26-534/source/runproc.sh\nWARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\nWARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\nINFO:sagemaker.local.image:'Docker Compose' found using Docker CLI.\nINFO:sagemaker.local.local_session:Starting processing job\nINFO:sagemaker.local.image:Using the long-lived AWS credentials found in session\nINFO:sagemaker.local.image:docker compose file: \nnetworks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-11uyr:\n    container_name: z95a9txez0-algo-1-11uyr\n    entrypoint:\n    - python3\n    - /opt/ml/processing/input/code/preprocessor.py\n    environment:\n    - '[Masked]'\n    - '[Masked]'\n    image: 683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-11uyr\n    stdin_open: true\n    tty: true\n    volumes:\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp76takbc2/algo-1-11uyr/config:/opt/ml/config\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp76takbc2/algo-1-11uyr/output:/opt/ml/output\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp3og0t308:/opt/ml/processing/input\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmpi2ttx5h6:/opt/ml/processing/input/code\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmpnbs8q41l/output/train:/opt/ml/processing/train\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmpnbs8q41l/output/validation:/opt/ml/processing/validation\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmpnbs8q41l/output/test:/opt/ml/processing/test\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmpnbs8q41l/output/model:/opt/ml/processing/model\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmpnbs8q41l/output/baseline:/opt/ml/processing/baseline\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp76takbc2/shared:/opt/ml/shared\nversion: '2.3'\n\nINFO:sagemaker.local.image:docker command: docker compose -f /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp76takbc2/docker-compose.yaml up --build --abort-on-container-exit\nWARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\nINFO:sagemaker.local.image:'Docker Compose' found using Docker CLI.\nINFO:sagemaker.local.local_session:Starting training job\nINFO:sagemaker.local.image:Using the long-lived AWS credentials found in session\nINFO:sagemaker.local.image:docker compose file: \nnetworks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-vb8is:\n    command: train\n    container_name: lyrronf17q-algo-1-vb8is\n    environment:\n    - '[Masked]'\n    - '[Masked]'\n    - '[Masked]'\n    - '[Masked]'\n    image: sagemaker-tensorflow-training-toolkit-local\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-vb8is\n    stdin_open: true\n    tty: true\n    volumes:\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp8r1o388c/algo-1-vb8is/output/data:/opt/ml/output/data\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp8r1o388c/algo-1-vb8is/input:/opt/ml/input\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp8r1o388c/algo-1-vb8is/output:/opt/ml/output\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp8r1o388c/model:/opt/ml/model\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmpnuidaskc:/opt/ml/input/data/train\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp3j14r_n6:/opt/ml/input/data/validation\nversion: '2.3'\n\nINFO:sagemaker.local.image:docker command: docker compose -f /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp8r1o388c/docker-compose.yaml up --build --abort-on-container-exit\nINFO:sagemaker.processing:Uploaded None to s3://mlschool/evaluation-processor-2023-10-20-15-03-42-919/source/sourcedir.tar.gz\nINFO:sagemaker.processing:runproc.sh uploaded to s3://mlschool/evaluation-processor-2023-10-20-15-03-42-919/source/runproc.sh\n/Users/svpino/dev/ml.school/.venv/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning:\n\nRunning within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n\nWARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\nINFO:sagemaker.local.image:'Docker Compose' found using Docker CLI.\nINFO:sagemaker.local.local_session:Starting processing job\nINFO:sagemaker.local.image:Using the long-lived AWS credentials found in session\nINFO:sagemaker.local.image:docker compose file: \nnetworks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-2t8nd:\n    container_name: ymgdk4sfdc-algo-1-2t8nd\n    entrypoint:\n    - /bin/bash\n    - /opt/ml/processing/input/entrypoint/runproc.sh\n    environment:\n    - '[Masked]'\n    - '[Masked]'\n    image: sagemaker-tensorflow-training-toolkit-local\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-2t8nd\n    stdin_open: true\n    tty: true\n    volumes:\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp6u04qqo7/algo-1-2t8nd/config:/opt/ml/config\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp6u04qqo7/algo-1-2t8nd/output:/opt/ml/output\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmpag42z8wx:/opt/ml/processing/test\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp8wvnrp0d:/opt/ml/processing/model\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp45bhq7eb:/opt/ml/processing/input/code/\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp31p5c6lj:/opt/ml/processing/input/entrypoint\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmpivgeah7f/output/evaluation:/opt/ml/processing/evaluation\n    - /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp6u04qqo7/shared:/opt/ml/shared\nversion: '2.3'\n\nINFO:sagemaker.local.image:docker command: docker compose -f /private/var/folders/4c/v1q3hy1x4mb5w0wpc72zl3_w0000gp/T/tmp6u04qqo7/docker-compose.yaml up --build --abort-on-container-exit\n\n\nStarting execution for pipeline cohort-pipeline. Execution ID is ef2a475a-e7e7-4a54-8ff4-9e3b2cd4eac3\nStarting pipeline step: 'split-and-transform-data'\nContainer z95a9txez0-algo-1-11uyr  Creating\nalgo-1-11uyr The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \nContainer z95a9txez0-algo-1-11uyr  Created\nAttaching to z95a9txez0-algo-1-11uyr\nz95a9txez0-algo-1-11uyr exited with code 0\nAborting on container exit...\nContainer z95a9txez0-algo-1-11uyr  Stopping\nContainer z95a9txez0-algo-1-11uyr  Stopped\n===== Job Complete =====\nPipeline step 'split-and-transform-data' SUCCEEDED.\nStarting pipeline step: 'train-model'\nContainer lyrronf17q-algo-1-vb8is  Creating\nContainer lyrronf17q-algo-1-vb8is  Created\nAttaching to lyrronf17q-algo-1-vb8is\nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:37,168 botocore.credentials INFO     Found credentials in environment variables.\nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:37,467 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:37,471 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:37,481 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:37,487 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:37,490 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:37,500 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:37,508 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:37,510 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:37,518 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:37,521 sagemaker-training-toolkit INFO     Invoking user script\nlyrronf17q-algo-1-vb8is  | \nlyrronf17q-algo-1-vb8is  | Training Env:\nlyrronf17q-algo-1-vb8is  | \nlyrronf17q-algo-1-vb8is  | {\nlyrronf17q-algo-1-vb8is  |     \"additional_framework_parameters\": {},\nlyrronf17q-algo-1-vb8is  |     \"channel_input_dirs\": {\nlyrronf17q-algo-1-vb8is  |         \"train\": \"/opt/ml/input/data/train\",\nlyrronf17q-algo-1-vb8is  |         \"validation\": \"/opt/ml/input/data/validation\"\nlyrronf17q-algo-1-vb8is  |     },\nlyrronf17q-algo-1-vb8is  |     \"current_host\": \"algo-1-vb8is\",\nlyrronf17q-algo-1-vb8is  |     \"current_instance_group\": \"homogeneousCluster\",\nlyrronf17q-algo-1-vb8is  |     \"current_instance_group_hosts\": [],\nlyrronf17q-algo-1-vb8is  |     \"current_instance_type\": \"local\",\nlyrronf17q-algo-1-vb8is  |     \"distribution_hosts\": [\nlyrronf17q-algo-1-vb8is  |         \"algo-1-vb8is\"\nlyrronf17q-algo-1-vb8is  |     ],\nlyrronf17q-algo-1-vb8is  |     \"distribution_instance_groups\": [],\nlyrronf17q-algo-1-vb8is  |     \"framework_module\": null,\nlyrronf17q-algo-1-vb8is  |     \"hosts\": [\nlyrronf17q-algo-1-vb8is  |         \"algo-1-vb8is\"\nlyrronf17q-algo-1-vb8is  |     ],\nlyrronf17q-algo-1-vb8is  |     \"hyperparameters\": {\nlyrronf17q-algo-1-vb8is  |         \"epochs\": 50,\nlyrronf17q-algo-1-vb8is  |         \"batch_size\": 32,\nlyrronf17q-algo-1-vb8is  |         \"model_dir\": \"s3://mlschool/training-2023-10-20-15-03-22-962/model\"\nlyrronf17q-algo-1-vb8is  |     },\nlyrronf17q-algo-1-vb8is  |     \"input_config_dir\": \"/opt/ml/input/config\",\nlyrronf17q-algo-1-vb8is  |     \"input_data_config\": {\nlyrronf17q-algo-1-vb8is  |         \"train\": {\nlyrronf17q-algo-1-vb8is  |             \"TrainingInputMode\": \"File\",\nlyrronf17q-algo-1-vb8is  |             \"ContentType\": \"text/csv\"\nlyrronf17q-algo-1-vb8is  |         },\nlyrronf17q-algo-1-vb8is  |         \"validation\": {\nlyrronf17q-algo-1-vb8is  |             \"TrainingInputMode\": \"File\",\nlyrronf17q-algo-1-vb8is  |             \"ContentType\": \"text/csv\"\nlyrronf17q-algo-1-vb8is  |         }\nlyrronf17q-algo-1-vb8is  |     },\nlyrronf17q-algo-1-vb8is  |     \"input_dir\": \"/opt/ml/input\",\nlyrronf17q-algo-1-vb8is  |     \"instance_groups\": [],\nlyrronf17q-algo-1-vb8is  |     \"instance_groups_dict\": {},\nlyrronf17q-algo-1-vb8is  |     \"is_hetero\": false,\nlyrronf17q-algo-1-vb8is  |     \"is_master\": true,\nlyrronf17q-algo-1-vb8is  |     \"is_modelparallel_enabled\": null,\nlyrronf17q-algo-1-vb8is  |     \"is_smddpmprun_installed\": false,\nlyrronf17q-algo-1-vb8is  |     \"job_name\": \"train-model-1697814215-c254\",\nlyrronf17q-algo-1-vb8is  |     \"log_level\": 20,\nlyrronf17q-algo-1-vb8is  |     \"master_hostname\": \"algo-1-vb8is\",\nlyrronf17q-algo-1-vb8is  |     \"model_dir\": \"/opt/ml/model\",\nlyrronf17q-algo-1-vb8is  |     \"module_dir\": \"s3://mlschool/training-2023-10-20-15-03-35-427/source/sourcedir.tar.gz\",\nlyrronf17q-algo-1-vb8is  |     \"module_name\": \"train\",\nlyrronf17q-algo-1-vb8is  |     \"network_interface_name\": \"eth0\",\nlyrronf17q-algo-1-vb8is  |     \"num_cpus\": 5,\nlyrronf17q-algo-1-vb8is  |     \"num_gpus\": 0,\nlyrronf17q-algo-1-vb8is  |     \"num_neurons\": 0,\nlyrronf17q-algo-1-vb8is  |     \"output_data_dir\": \"/opt/ml/output/data\",\nlyrronf17q-algo-1-vb8is  |     \"output_dir\": \"/opt/ml/output\",\nlyrronf17q-algo-1-vb8is  |     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\nlyrronf17q-algo-1-vb8is  |     \"resource_config\": {\nlyrronf17q-algo-1-vb8is  |         \"current_host\": \"algo-1-vb8is\",\nlyrronf17q-algo-1-vb8is  |         \"hosts\": [\nlyrronf17q-algo-1-vb8is  |             \"algo-1-vb8is\"\nlyrronf17q-algo-1-vb8is  |         ]\nlyrronf17q-algo-1-vb8is  |     },\nlyrronf17q-algo-1-vb8is  |     \"user_entry_point\": \"train.py\"\nlyrronf17q-algo-1-vb8is  | }\nlyrronf17q-algo-1-vb8is  | \nlyrronf17q-algo-1-vb8is  | Environment variables:\nlyrronf17q-algo-1-vb8is  | \nlyrronf17q-algo-1-vb8is  | SM_HOSTS=[\"algo-1-vb8is\"]\nlyrronf17q-algo-1-vb8is  | SM_NETWORK_INTERFACE_NAME=eth0\nlyrronf17q-algo-1-vb8is  | SM_HPS={\"batch_size\":32,\"epochs\":50,\"model_dir\":\"s3://mlschool/training-2023-10-20-15-03-22-962/model\"}\nlyrronf17q-algo-1-vb8is  | SM_USER_ENTRY_POINT=train.py\nlyrronf17q-algo-1-vb8is  | SM_FRAMEWORK_PARAMS={}\nlyrronf17q-algo-1-vb8is  | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-vb8is\",\"hosts\":[\"algo-1-vb8is\"]}\nlyrronf17q-algo-1-vb8is  | SM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"text/csv\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"TrainingInputMode\":\"File\"}}\nlyrronf17q-algo-1-vb8is  | SM_OUTPUT_DATA_DIR=/opt/ml/output/data\nlyrronf17q-algo-1-vb8is  | SM_CHANNELS=[\"train\",\"validation\"]\nlyrronf17q-algo-1-vb8is  | SM_CURRENT_HOST=algo-1-vb8is\nlyrronf17q-algo-1-vb8is  | SM_CURRENT_INSTANCE_TYPE=local\nlyrronf17q-algo-1-vb8is  | SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\nlyrronf17q-algo-1-vb8is  | SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\nlyrronf17q-algo-1-vb8is  | SM_INSTANCE_GROUPS=[]\nlyrronf17q-algo-1-vb8is  | SM_INSTANCE_GROUPS_DICT={}\nlyrronf17q-algo-1-vb8is  | SM_DISTRIBUTION_INSTANCE_GROUPS=[]\nlyrronf17q-algo-1-vb8is  | SM_IS_HETERO=false\nlyrronf17q-algo-1-vb8is  | SM_MODULE_NAME=train\nlyrronf17q-algo-1-vb8is  | SM_LOG_LEVEL=20\nlyrronf17q-algo-1-vb8is  | SM_FRAMEWORK_MODULE=\nlyrronf17q-algo-1-vb8is  | SM_INPUT_DIR=/opt/ml/input\nlyrronf17q-algo-1-vb8is  | SM_INPUT_CONFIG_DIR=/opt/ml/input/config\nlyrronf17q-algo-1-vb8is  | SM_OUTPUT_DIR=/opt/ml/output\nlyrronf17q-algo-1-vb8is  | SM_NUM_CPUS=5\nlyrronf17q-algo-1-vb8is  | SM_NUM_GPUS=0\nlyrronf17q-algo-1-vb8is  | SM_NUM_NEURONS=0\nlyrronf17q-algo-1-vb8is  | SM_MODEL_DIR=/opt/ml/model\nlyrronf17q-algo-1-vb8is  | SM_MODULE_DIR=s3://mlschool/training-2023-10-20-15-03-35-427/source/sourcedir.tar.gz\nlyrronf17q-algo-1-vb8is  | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1-vb8is\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-vb8is\"],\"distribution_instance_groups\":[],\"framework_module\":null,\"hosts\":[\"algo-1-vb8is\"],\"hyperparameters\":{\"batch_size\":32,\"epochs\":50,\"model_dir\":\"s3://mlschool/training-2023-10-20-15-03-22-962/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"text/csv\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"train-model-1697814215-c254\",\"log_level\":20,\"master_hostname\":\"algo-1-vb8is\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://mlschool/training-2023-10-20-15-03-35-427/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":5,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-vb8is\",\"hosts\":[\"algo-1-vb8is\"]},\"user_entry_point\":\"train.py\"}\nlyrronf17q-algo-1-vb8is  | SM_USER_ARGS=[\"--batch_size\",\"32\",\"--epochs\",\"50\",\"--model_dir\",\"s3://mlschool/training-2023-10-20-15-03-22-962/model\"]\nlyrronf17q-algo-1-vb8is  | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\nlyrronf17q-algo-1-vb8is  | SM_CHANNEL_TRAIN=/opt/ml/input/data/train\nlyrronf17q-algo-1-vb8is  | SM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\nlyrronf17q-algo-1-vb8is  | SM_HP_EPOCHS=50\nlyrronf17q-algo-1-vb8is  | SM_HP_BATCH_SIZE=32\nlyrronf17q-algo-1-vb8is  | SM_HP_MODEL_DIR=s3://mlschool/training-2023-10-20-15-03-22-962/model\nlyrronf17q-algo-1-vb8is  | PYTHONPATH=/opt/ml/code:/root/miniconda3/envs/ml-dependencies/bin:/root/miniconda3/envs/ml-dependencies/lib/python39.zip:/root/miniconda3/envs/ml-dependencies/lib/python3.9:/root/miniconda3/envs/ml-dependencies/lib/python3.9/lib-dynload:/root/miniconda3/envs/ml-dependencies/lib/python3.9/site-packages\nlyrronf17q-algo-1-vb8is  | \nlyrronf17q-algo-1-vb8is  | Invoking script with the following command:\nlyrronf17q-algo-1-vb8is  | \nlyrronf17q-algo-1-vb8is  | /root/miniconda3/envs/ml-dependencies/bin/python train.py --batch_size 32 --epochs 50 --model_dir s3://mlschool/training-2023-10-20-15-03-22-962/model\nlyrronf17q-algo-1-vb8is  | \nlyrronf17q-algo-1-vb8is  | \nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:37,522 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:38,615 numexpr.utils INFO     NumExpr defaulting to 5 threads.\nlyrronf17q-algo-1-vb8is  | Epoch 1/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 1.1680 - accuracy: 0.4283 - val_loss: 1.1430 - val_accuracy: 0.4369 - 254ms/epoch - 16ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 2/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 1.0998 - accuracy: 0.5031 - val_loss: 1.0632 - val_accuracy: 0.5243 - 22ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 3/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 1.0330 - accuracy: 0.6050 - val_loss: 0.9739 - val_accuracy: 0.7379 - 22ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 4/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.9526 - accuracy: 0.7484 - val_loss: 0.8759 - val_accuracy: 0.8447 - 21ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 5/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.8715 - accuracy: 0.7775 - val_loss: 0.7880 - val_accuracy: 0.8544 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 6/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.7981 - accuracy: 0.7838 - val_loss: 0.7052 - val_accuracy: 0.8544 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 7/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.7284 - accuracy: 0.7859 - val_loss: 0.6331 - val_accuracy: 0.8544 - 22ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 8/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.6681 - accuracy: 0.7859 - val_loss: 0.5682 - val_accuracy: 0.8544 - 21ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 9/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.6152 - accuracy: 0.7859 - val_loss: 0.5164 - val_accuracy: 0.8544 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 10/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.5703 - accuracy: 0.7859 - val_loss: 0.4708 - val_accuracy: 0.8544 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 11/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.5319 - accuracy: 0.7859 - val_loss: 0.4344 - val_accuracy: 0.8544 - 19ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 12/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.4985 - accuracy: 0.7859 - val_loss: 0.4035 - val_accuracy: 0.8544 - 22ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 13/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.4697 - accuracy: 0.7859 - val_loss: 0.3761 - val_accuracy: 0.8544 - 26ms/epoch - 2ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 14/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.4455 - accuracy: 0.7859 - val_loss: 0.3531 - val_accuracy: 0.8544 - 26ms/epoch - 2ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 15/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.4241 - accuracy: 0.7859 - val_loss: 0.3340 - val_accuracy: 0.8544 - 22ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 16/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.4056 - accuracy: 0.7859 - val_loss: 0.3181 - val_accuracy: 0.8544 - 21ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 17/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.3889 - accuracy: 0.7859 - val_loss: 0.3054 - val_accuracy: 0.8641 - 21ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 18/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.3722 - accuracy: 0.8129 - val_loss: 0.2906 - val_accuracy: 0.9126 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 19/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.3579 - accuracy: 0.8420 - val_loss: 0.2788 - val_accuracy: 0.9417 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 20/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.3446 - accuracy: 0.8857 - val_loss: 0.2669 - val_accuracy: 0.9417 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 21/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.3323 - accuracy: 0.8919 - val_loss: 0.2573 - val_accuracy: 0.9515 - 21ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 22/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.3187 - accuracy: 0.9272 - val_loss: 0.2473 - val_accuracy: 0.9709 - 23ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 23/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.3067 - accuracy: 0.9459 - val_loss: 0.2379 - val_accuracy: 0.9709 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 24/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.2963 - accuracy: 0.9501 - val_loss: 0.2296 - val_accuracy: 0.9709 - 21ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 25/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.2867 - accuracy: 0.9501 - val_loss: 0.2208 - val_accuracy: 0.9709 - 21ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 26/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.2776 - accuracy: 0.9501 - val_loss: 0.2131 - val_accuracy: 0.9709 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 27/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.2688 - accuracy: 0.9543 - val_loss: 0.2059 - val_accuracy: 0.9709 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 28/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.2606 - accuracy: 0.9605 - val_loss: 0.1988 - val_accuracy: 0.9709 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 29/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.2509 - accuracy: 0.9626 - val_loss: 0.1926 - val_accuracy: 0.9709 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 30/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.2430 - accuracy: 0.9647 - val_loss: 0.1866 - val_accuracy: 0.9709 - 22ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 31/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.2354 - accuracy: 0.9647 - val_loss: 0.1806 - val_accuracy: 0.9709 - 23ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 32/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.2279 - accuracy: 0.9647 - val_loss: 0.1734 - val_accuracy: 0.9709 - 22ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 33/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.2204 - accuracy: 0.9647 - val_loss: 0.1676 - val_accuracy: 0.9709 - 21ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 34/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.2121 - accuracy: 0.9647 - val_loss: 0.1613 - val_accuracy: 0.9806 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 35/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.2048 - accuracy: 0.9667 - val_loss: 0.1564 - val_accuracy: 0.9806 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 36/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1981 - accuracy: 0.9667 - val_loss: 0.1516 - val_accuracy: 0.9806 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 37/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1915 - accuracy: 0.9688 - val_loss: 0.1468 - val_accuracy: 0.9806 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 38/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1852 - accuracy: 0.9688 - val_loss: 0.1443 - val_accuracy: 0.9806 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 39/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1779 - accuracy: 0.9813 - val_loss: 0.1387 - val_accuracy: 0.9806 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 40/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1716 - accuracy: 0.9834 - val_loss: 0.1328 - val_accuracy: 0.9806 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 41/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1657 - accuracy: 0.9792 - val_loss: 0.1283 - val_accuracy: 0.9806 - 21ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 42/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1600 - accuracy: 0.9813 - val_loss: 0.1234 - val_accuracy: 0.9806 - 23ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 43/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1533 - accuracy: 0.9834 - val_loss: 0.1190 - val_accuracy: 0.9806 - 21ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 44/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1478 - accuracy: 0.9834 - val_loss: 0.1146 - val_accuracy: 0.9806 - 22ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 45/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1427 - accuracy: 0.9834 - val_loss: 0.1100 - val_accuracy: 0.9806 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 46/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1372 - accuracy: 0.9834 - val_loss: 0.1065 - val_accuracy: 0.9903 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 47/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1324 - accuracy: 0.9896 - val_loss: 0.1019 - val_accuracy: 0.9806 - 20ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 48/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1277 - accuracy: 0.9875 - val_loss: 0.0986 - val_accuracy: 0.9903 - 21ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 49/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1231 - accuracy: 0.9896 - val_loss: 0.0954 - val_accuracy: 0.9903 - 23ms/epoch - 1ms/step\nlyrronf17q-algo-1-vb8is  | Epoch 50/50\nlyrronf17q-algo-1-vb8is  | 16/16 - 0s - loss: 0.1187 - accuracy: 0.9896 - val_loss: 0.0923 - val_accuracy: 0.9903 - 22ms/epoch - 1ms/step\n4/4 [==============================] - 0s 599us/step\nlyrronf17q-algo-1-vb8is  | Validation accuracy: 0.9902912621359223\nlyrronf17q-algo-1-vb8is  | WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\nlyrronf17q-algo-1-vb8is  | 2023-10-20 15:03:42,111 sagemaker-training-toolkit INFO     Reporting training SUCCESS\nlyrronf17q-algo-1-vb8is exited with code 0\nAborting on container exit...\nContainer lyrronf17q-algo-1-vb8is  Stopping\nContainer lyrronf17q-algo-1-vb8is  Stopped\n===== Job Complete =====\nPipeline step 'train-model' SUCCEEDED.\nStarting pipeline step: 'evaluate-model'\nContainer ymgdk4sfdc-algo-1-2t8nd  Creating\nContainer ymgdk4sfdc-algo-1-2t8nd  Created\nAttaching to ymgdk4sfdc-algo-1-2t8nd\n4/4 [==============================] - 0s 594us/step\nymgdk4sfdc-algo-1-2t8nd  | Test accuracy: 1.0\nymgdk4sfdc-algo-1-2t8nd exited with code 0\nAborting on container exit...\nContainer ymgdk4sfdc-algo-1-2t8nd  Stopping\nContainer ymgdk4sfdc-algo-1-2t8nd  Stopped\n===== Job Complete =====\nPipeline step 'evaluate-model' SUCCEEDED.\nStarting pipeline step: 'check-model-accuracy'\nPipeline step 'check-model-accuracy' SUCCEEDED.\nPipeline execution ef2a475a-e7e7-4a54-8ff4-9e3b2cd4eac3 SUCCEEDED\n\n\n&lt;sagemaker.local.entities._LocalPipelineExecution at 0x31e4af6d0&gt;"
  },
  {
    "objectID": "cohort-old.html#quality-baselines",
    "href": "cohort-old.html#quality-baselines",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Quality Baselines",
    "text": "Quality Baselines\nOur pipeline generated data baseline statistics and constraints using our train set. We can take a look at what these values look like by downloading them from S3.\n\nimport json\nfrom IPython.display import JSON\nfrom sagemaker.s3 import S3Downloader\n\n\nstatistics = f\"{DATA_QUALITY_LOCATION}/statistics.json\"\n\nresponse = None\ntry:\n    response = json.loads(S3Downloader.read_file(statistics))\nexcept Exception as e:\n    pass\n\nJSON(response or {})\n\n&lt;IPython.core.display.JSON object&gt;\n\n\n\nconstraints = f\"{DATA_QUALITY_LOCATION}/constraints.json\"\n\nresponse = None\ntry:\n    response = json.loads(S3Downloader.read_file(constraints))\nexcept Exception as e:\n    pass\n\nJSON(response or {})\n\n&lt;IPython.core.display.JSON object&gt;\n\n\nWe also generated the baseline performance using the test set.\n\nconstraints = f\"{MODEL_QUALITY_LOCATION}/constraints.json\"\n\nresponse = None\ntry:\n    response = json.loads(S3Downloader.read_file(constraints))\nexcept Exception as e:\n    pass\n\nJSON(response or {})\n\n&lt;IPython.core.display.JSON object&gt;"
  },
  {
    "objectID": "cohort-old.html#lambda",
    "href": "cohort-old.html#lambda",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Lambda",
    "text": "Lambda\nLet’s start by writing the Lambda function to take the model information and create a new endpoint.\nWe’ll enable Data Capture as part of the endpoint configuration. With Data Capture we can record the inputs and outputs of the endpoint to use them later for monitoring the model: * InitialSamplingPercentage represents the percentage of traffic that we want to capture. * DestinationS3Uri specifies the S3 location where we want to store the captured data.\n\nimport os\nimport json\nimport boto3\nimport time\n\nsagemaker = boto3.client(\"sagemaker\")\n\ndef lambda_handler(event, context):\n    model_package_arn = event[\"detail\"][\"ModelPackageArn\"]\n    approval_status = event[\"detail\"][\"ModelApprovalStatus\"]\n    \n    print(f'Model: \"{model_package_arn}\". Approval Status: \"{approval_status}\"')\n    \n    # We only want to deploy the model if it's new approval\n    # status is \"Approved.\"\n    if approval_status != \"Approved\":\n        return {\n            \"statusCode\": 200,\n            \"body\": json.dumps(f'Skipping deployment. Approval status: \"{approval_status}\"')\n        }    \n    \n    \n    endpoint_name = os.environ[\"ENDPOINT\"]\n    data_capture_destination = os.environ[\"DATA_CAPTURE_DESTINATION\"]\n    role = os.environ[\"ROLE\"]\n    \n    timestamp = time.strftime(\"%m%d%H%M%S\", time.localtime())\n    model_name = f\"{endpoint_name}-model-{timestamp}\"\n    endpoint_config_name = f\"{endpoint_name}-config-{timestamp}\"\n\n    sagemaker.create_model(\n        ModelName=model_name, \n        ExecutionRoleArn=role, \n        Containers=[{\n            \"ModelPackageName\": model_package_arn\n        }] \n    )\n\n    sagemaker.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[{\n            \"ModelName\": model_name,\n            \"InstanceType\": \"ml.m5.xlarge\",\n            \"InitialVariantWeight\": 1,\n            \"InitialInstanceCount\": 1,\n            \"VariantName\": \"AllTraffic\",\n        }],\n        \n        # We can enable Data Capture to record the inputs and outputs of the endpoint\n        # to use them later for monitoring the model. \n        DataCaptureConfig={\n            \"EnableCapture\": True,\n            \"InitialSamplingPercentage\": 100,\n            \"DestinationS3Uri\": data_capture_destination,\n            \"CaptureOptions\": [\n                {\n                    \"CaptureMode\": \"Input\"\n                },\n                {\n                    \"CaptureMode\": \"Output\"\n                },\n            ],\n            \"CaptureContentTypeHeader\": {\n                \"CsvContentTypes\": [\n                    \"text/csv\",\n                    \"application/octect-stream\"\n                ],\n                \"JsonContentTypes\": [\n                    \"application/json\",\n                    \"application/octect-stream\"\n                ]\n            }\n        },\n    )\n    \n    response = sagemaker.list_endpoints(NameContains=endpoint_name, MaxResults=1)\n\n    if len(response[\"Endpoints\"]) == 0:\n        # If the endpoint doesn't exist, let's create it.\n        sagemaker.create_endpoint(\n            EndpointName=endpoint_name, \n            EndpointConfigName=endpoint_config_name,\n        )\n    else:\n        # If the endpoint already exist, let's update it with the\n        # new configuration.\n        sagemaker.update_endpoint(\n            EndpointName=endpoint_name, \n            EndpointConfigName=endpoint_config_name,\n        )\n    \n    return {\n        \"statusCode\": 200,\n        \"body\": json.dumps(\"Endpoint deployed successfully\")\n    }\n\nOverwriting code/lambda.py\n\n\nWe need to ensure our Lambda function has permission to interact with SageMaker, so let’s create a new role and then create the lambda function.\n\nENDPOINT = \"penguins-endpoint\"\nDATA_CAPTURE_DESTINATION = f\"{S3_LOCATION}/monitoring/data-capture\"\n\n\nlambda_role_name = \"lambda-deployment-role\"\nlambda_role_arn = None\n\ntry:\n    response = iam_client.create_role(\n        RoleName = lambda_role_name,\n        AssumeRolePolicyDocument = json.dumps({\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Effect\": \"Allow\",\n                    \"Principal\": {\n                        \"Service\": [\n                            \"lambda.amazonaws.com\",\n                            \"events.amazonaws.com\"\n                        ]\n                    },\n                    \"Action\": \"sts:AssumeRole\",\n                }\n            ]\n        }),\n        Description=\"Lambda Endpoint Deployment\"\n    )\n\n    lambda_role_arn = response[\"Role\"][\"Arn\"]\n    \n    iam_client.attach_role_policy(\n        RoleName=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\",\n        PolicyArn=lambda_role_arn\n    )\n    \n    iam_client.attach_role_policy(\n        RoleName=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\",\n        PolicyArn=lambda_role_arn\n    )\n    \n    print(f'Role \"{lambda_role_name}\" created with ARN \"{lambda_role_arn}\".')\nexcept iam_client.exceptions.EntityAlreadyExistsException:\n    print(f\"Role {lambda_role_name} already exists.\")\n    response = iam_client.get_role(RoleName=lambda_role_name)\n    lambda_role_arn = response[\"Role\"][\"Arn\"]\n\nRole lambda-deployment-role already exists.\n\n\nLet’s create the Lambda function.\n\nfrom sagemaker.lambda_helper import Lambda\n\n\ndeploy_lambda_fn = Lambda(\n    function_name=\"deploy_fn\",\n    execution_role_arn=lambda_role_arn,\n    script=str(CODE_FOLDER / \"lambda.py\"),\n    handler=\"lambda.lambda_handler\",\n    timeout=600,\n    session=sagemaker_session,\n    runtime=\"python3.11\",\n    environment={\n        \"Variables\": {\n            \"ENDPOINT\": ENDPOINT,\n            \"DATA_CAPTURE_DESTINATION\": DATA_CAPTURE_DESTINATION,\n            \"ROLE\": role,\n        }\n    }\n)\n\nlambda_response = None\nif not LOCAL_MODE:\n    lambda_response = deploy_lambda_fn.upsert()\n\nlambda_response\n\n{'ResponseMetadata': {'RequestId': '1e676d09-bacc-4b88-af4b-086aed5abe16',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'date': 'Thu, 19 Oct 2023 19:51:18 GMT',\n   'content-type': 'application/json',\n   'content-length': '1428',\n   'connection': 'keep-alive',\n   'x-amzn-requestid': '1e676d09-bacc-4b88-af4b-086aed5abe16'},\n  'RetryAttempts': 0},\n 'FunctionName': 'deploy_fn',\n 'FunctionArn': 'arn:aws:lambda:us-east-1:325223348818:function:deploy_fn',\n 'Runtime': 'python3.11',\n 'Role': 'arn:aws:iam::325223348818:role/lambda-deployment-role',\n 'Handler': 'lambda.lambda_handler',\n 'CodeSize': 3142,\n 'Description': '',\n 'Timeout': 600,\n 'MemorySize': 128,\n 'LastModified': '2023-10-19T19:51:18.000+0000',\n 'CodeSha256': 'GvIH6E8ZyghrSf4h2g/j+qIcE0HVvoFrno9erm8Qheo=',\n 'Version': '$LATEST',\n 'Environment': {'Variables': {'ROLE': 'arn:aws:iam::325223348818:role/service-role/AmazonSageMaker-ExecutionRole-20230312T160501',\n   'DATA_CAPTURE_DESTINATION': 's3://mlschool/penguins/monitoring/data-capture',\n   'ENDPOINT': 'penguins-endpoint'}},\n 'TracingConfig': {'Mode': 'PassThrough'},\n 'RevisionId': '11ecfd01-0601-44e1-8bf6-1b47d75f4ccd',\n 'Layers': [],\n 'State': 'Active',\n 'LastUpdateStatus': 'InProgress',\n 'LastUpdateStatusReason': 'The function is being created.',\n 'LastUpdateStatusReasonCode': 'Creating',\n 'PackageType': 'Zip',\n 'Architectures': ['x86_64'],\n 'EphemeralStorage': {'Size': 512},\n 'SnapStart': {'ApplyOn': 'None', 'OptimizationStatus': 'Off'},\n 'RuntimeVersionConfig': {'RuntimeVersionArn': 'arn:aws:lambda:us-east-1::runtime:6cf63f1a78b5c5e19617d6b4b111370fdbda415ea91bdfdc5aacef9fee76b64a'}}"
  },
  {
    "objectID": "cohort-old.html#eventbridge",
    "href": "cohort-old.html#eventbridge",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "EventBridge",
    "text": "EventBridge\nLet’s create an EventBridge rule that triggers the deployment process whenever a model approval status becomes “Approved”.\n\nevents_client = boto3.client(\"events\")\n\nevent_pattern = f\"\"\"\n{{\n  \"source\": [\"aws.sagemaker\"],\n  \"detail-type\": [\"SageMaker Model Package State Change\"],\n  \"detail\": {{\n    \"ModelPackageGroupName\": [\"{MODEL_PACKAGE_GROUP}\"],\n    \"ModelApprovalStatus\": [\"Approved\"]\n  }}\n}}\n\"\"\"\n\nrule_response = None\nif not LOCAL_MODE:\n  rule_response = events_client.put_rule(\n      Name=\"model-approval-rule\",\n      EventPattern=event_pattern,\n      State=\"ENABLED\",\n      RoleArn=role,\n  )\n\n  response = events_client.put_targets(\n      Rule=\"model-approval-rule\",\n      Targets=[\n          {\n              \"Id\": \"1\",\n              \"Arn\": lambda_response[\"FunctionArn\"],\n          }\n      ]\n  )\n\nrule_response\n\n{'RuleArn': 'arn:aws:events:us-east-1:325223348818:rule/model-approval-rule',\n 'ResponseMetadata': {'RequestId': 'a8d3fda6-0bf6-49f6-a28e-7cd6aac2b28f',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': 'a8d3fda6-0bf6-49f6-a28e-7cd6aac2b28f',\n   'content-type': 'application/x-amz-json-1.1',\n   'content-length': '76',\n   'date': 'Thu, 19 Oct 2023 19:51:18 GMT'},\n  'RetryAttempts': 0}}\n\n\nWe need to give the event permissions to invoke the Lambda function.\n\nlambda_client = boto3.client(\"lambda\")\n\nif not LOCAL_MODE:\n    try:\n        response = lambda_client.add_permission(\n            Action=\"lambda:InvokeFunction\",\n            FunctionName=lambda_response[\"FunctionName\"],\n            Principal=\"events.amazonaws.com\",\n            SourceArn=rule_response[\"RuleArn\"],\n            StatementId=\"EventBridge\",\n        )\n    except lambda_client.exceptions.ResourceConflictException as e:\n        print(f'Function \"{lambda_response[\"FunctionName\"]}\" already has the correct permissions.')\n\nFunction \"deploy_fn\" already has the correct permissions."
  },
  {
    "objectID": "cohort-old.html#predictions",
    "href": "cohort-old.html#predictions",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Predictions",
    "text": "Predictions\nLet’s now test the endpoint we deployed automatically with the pipeline. We will use the function to create a predictor with a JSON encoder and decoder.\n\nUncomment the %%script cell magic line to execute this cell.\n\n\nfrom sagemaker.predictor import Predictor\nfrom sagemaker.serializers import CSVSerializer\n\n\nwaiter = sagemaker_client.get_waiter(\"endpoint_in_service\")\nwaiter.wait(\n    EndpointName=ENDPOINT,\n    WaiterConfig={\n        \"Delay\": 10,\n        \"MaxAttempts\": 30\n    }\n)\n\npredictor = Predictor(\n    endpoint_name=ENDPOINT, \n    serializer=CSVSerializer(),\n    sagemaker_session=sagemaker_session\n)\n\ndata = pd.read_csv(DATA_FILEPATH)\ndata = data.drop(\"species\", axis=1)\n\npayload = data.iloc[:3].to_csv(header=False, index=False)\nresponse = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\nprint(response.decode(\"utf-8\"))"
  },
  {
    "objectID": "cohort-old.html#data-capture",
    "href": "cohort-old.html#data-capture",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Data Capture",
    "text": "Data Capture\nLet’s check the S3 location where the endpoint stores the requests and responses that it receives.\nNotice that it make take a few minutes for the first few files to show up in S3. Keep running the following line until you get some.\n\nfiles = S3Downloader.list(DATA_CAPTURE_DESTINATION)[:3]\nfiles\n\n['s3://mlschool/penguins/monitoring/data-capture/penguins-endpoint/AllTraffic/2023/09/25/13/15-40-735-9cc3750d-ba42-472c-903d-969695d2096d.jsonl',\n 's3://mlschool/penguins/monitoring/data-capture/penguins-endpoint/AllTraffic/2023/09/27/15/45-31-289-001fc69f-c352-4da2-b57a-a3a69fe3fecf.jsonl',\n 's3://mlschool/penguins/monitoring/data-capture/penguins-endpoint/AllTraffic/2023/10/05/16/50-04-992-e16242d1-925c-4b07-9289-dffa0e026679.jsonl']\n\n\nThese files contain the data captured by the endpoint in a SageMaker-specific JSON-line format. Each inference request is captured in a single line in the jsonl file. The line contains both the input and output merged together.\nLet’s read the first line from the first file:\n\nif len(files):\n    lines = S3Downloader.read_file(files[0])\n    print(json.dumps(json.loads(lines.split(\"\\n\")[0]), indent=2))\n\n{\n  \"captureData\": {\n    \"endpointInput\": {\n      \"observedContentType\": \"text/csv\",\n      \"mode\": \"INPUT\",\n      \"data\": \"Torgersen,39.1,18.7,181.0,3750.0,MALE\\nTorgersen,39.5,17.4,186.0,3800.0,FEMALE\\nTorgersen,40.3,18.0,195.0,3250.0,FEMALE\\n\",\n      \"encoding\": \"CSV\"\n    },\n    \"endpointOutput\": {\n      \"observedContentType\": \"application/json\",\n      \"mode\": \"OUTPUT\",\n      \"data\": \"[{\\\"prediction\\\": \\\"Adelie\\\", \\\"confidence\\\": 0.775418103}, {\\\"prediction\\\": \\\"Adelie\\\", \\\"confidence\\\": 0.775709867}, {\\\"prediction\\\": \\\"Adelie\\\", \\\"confidence\\\": 0.67967391}]\",\n      \"encoding\": \"JSON\"\n    }\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"d33f9a23-5ae3-4403-9aa1-3759d7fa8015\",\n    \"inferenceTime\": \"2023-09-25T13:15:40Z\"\n  },\n  \"eventVersion\": \"0\"\n}"
  },
  {
    "objectID": "cohort-old.html#clean-up",
    "href": "cohort-old.html#clean-up",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Clean up",
    "text": "Clean up\nLet’s now delete the endpoint.\n\nUncomment the %%script cell magic line to execute this cell.\n\n\npredictor.delete_endpoint()"
  },
  {
    "objectID": "cohort-old.html#fake-traffic",
    "href": "cohort-old.html#fake-traffic",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Fake Traffic",
    "text": "Fake Traffic\nTo test the monitoring functionality, we need to generate traffic to the endpoint.\nTo generate traffic, we will repeatedly send every sample from the dataset to the endpoint to simulate real prediction requests.\nThe following function will generate the traffic to the endpoint.\n\nfrom time import sleep\nfrom threading import Thread, Event\n\n\ndef generate_traffic(predictor):\n    \n    def _predict(data, predictor, stop_traffic_thread):\n        for index, row in data.iterrows():\n            data = row.tolist()\n            data = ','.join(map(str, data))\n            predictor.predict(data, inference_id=str(index), initial_args={\"ContentType\": \"text/csv\"})\n            \n            sleep(1)\n\n            if stop_traffic_thread.is_set():\n                break\n\n    def _generate_prediction_data(data, predictor, stop_traffic_thread):\n        while True:\n            print(f\"Generating {data.shape[0]} predictions...\")\n            _predict(data, predictor, stop_traffic_thread)\n            \n            if stop_traffic_thread.is_set():\n                break\n\n                \n    stop_traffic_thread = Event()\n    \n    data = pd.read_csv(DATA_FILEPATH, header=0).dropna()\n    data.drop([\"species\"], axis=1, inplace=True)\n    \n    traffic_thread = Thread(\n        target=_generate_prediction_data,\n        args=(data, predictor, stop_traffic_thread,)\n    )\n    \n    traffic_thread.start()\n    \n    return stop_traffic_thread, traffic_thread\n\nLet’s wait for the endpoint to be in service, and then we can start generating traffic to the endpoint.\n\nUncomment the %%script cell magic line to execute this cell.\n\n\nwaiter = sagemaker_client.get_waiter(\"endpoint_in_service\")\nwaiter.wait(\n    EndpointName=ENDPOINT,\n    WaiterConfig={\n        \"Delay\": 10,\n        \"MaxAttempts\": 30\n    }\n)\n\npredictor = Predictor(\n    endpoint_name=ENDPOINT, \n    serializer=CSVSerializer(),\n    sagemaker_session=sagemaker_session\n)\n\nstop_traffic_thread, traffic_thread = generate_traffic(predictor)"
  },
  {
    "objectID": "cohort-old.html#fake-labels",
    "href": "cohort-old.html#fake-labels",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Fake Labels",
    "text": "Fake Labels\nTo test the performance of the model, we need to label the samples captured by the endpoint. We can simulate the labeling process by generating a random label for every sample. Check Ingest Ground Truth Labels and Merge Them With Predictions for more information about this.\n\nimport random\nfrom datetime import datetime\n\n\ndef generate_ground_truth_data(ground_truth_location):\n    \n    def _generate_ground_truth_record(inference_id):\n        random.seed(inference_id)\n\n        return {\n            \"groundTruthData\": {\n                \"data\": random.choice([\"Adelie\", \"Chinstrap\", \"Gentoo\"]),\n                \"encoding\": \"CSV\",\n            },\n            \"eventMetadata\": {\n                \"eventId\": str(inference_id),\n            },\n            \"eventVersion\": \"0\",\n        }\n\n\n    def _upload_ground_truth(records, upload_time):\n        records = [json.dumps(r) for r in records]\n        data = \"\\n\".join(records)\n        uri = f\"{ground_truth_location}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\n\n        print(f\"Uploading ground truth data to {uri}...\")\n\n        S3Uploader.upload_string_as_file_body(data, uri)    \n\n                \n    def _generate_ground_truth_data(max_records, stop_ground_truth_thread):\n        while True:\n            records = [_generate_ground_truth_record(i) for i in range(max_records)]\n            _upload_ground_truth(records, datetime.utcnow())\n\n            if stop_ground_truth_thread.is_set():\n                break\n\n            sleep(30)\n\n\n    stop_ground_truth_thread = Event()\n    data = pd.read_csv(DATA_FILEPATH).dropna()\n    \n    groundtruth_thread = Thread(\n        target=_generate_ground_truth_data,\n        args=(len(data), stop_ground_truth_thread,)\n    )\n    \n    groundtruth_thread.start()\n    \n    return stop_ground_truth_thread, groundtruth_thread\n\nWe can now start generating fake labels.\n\nUncomment the %%script cell magic line to execute this cell.\n\n\nstop_ground_truth_thread, groundtruth_thread = generate_ground_truth_data(\n    GROUND_TRUTH_LOCATION\n)"
  },
  {
    "objectID": "cohort-old.html#monitoring-jobs",
    "href": "cohort-old.html#monitoring-jobs",
    "title": "Machine Learning In Production Using SageMaker",
    "section": "Monitoring Jobs",
    "text": "Monitoring Jobs\nWe can now schedule the Monitoring Jobs to continuously monitor the data going into the endpoint and the model performance. We will use the baseline we generated in the pipeline to determine when there’s drift. Check Schedule Data Quality Monitoring Jobs and Schedule Model Quality Monitoring Jobs for more information.\nThe following functions will help us work with monitoring schedules later on.\n\ndef describe_monitoring_schedules(endpoint_name):\n    schedules = []\n    response = sagemaker_client.list_monitoring_schedules(EndpointName=endpoint_name)[\"MonitoringScheduleSummaries\"]\n    for item in response:\n        name = item[\"MonitoringScheduleName\"]\n        schedule = {\n            \"MonitoringScheduleName\": name,\n            \"MonitoringType\": item[\"MonitoringType\"]\n        }\n        \n        description = sagemaker_client.describe_monitoring_schedule(\n            MonitoringScheduleName=name\n        )\n        \n        schedule[\"Status\"] = description[\"LastMonitoringExecutionSummary\"][\"MonitoringExecutionStatus\"]\n        \n        if schedule[\"Status\"] == \"Failed\":\n            schedule[\"FailureReason\"] = description[\"LastMonitoringExecutionSummary\"][\"FailureReason\"]\n        elif schedule[\"Status\"] == \"CompletedWithViolations\":\n            processing_job_arn = description[\"LastMonitoringExecutionSummary\"][\"ProcessingJobArn\"]\n            execution = MonitoringExecution.from_processing_arn(\n                sagemaker_session=sagemaker_session, \n                processing_job_arn=processing_job_arn\n            )\n            execution_destination = execution.output.destination\n\n            violations_filepath = os.path.join(execution_destination, \"constraint_violations.json\")\n            violations = json.loads(S3Downloader.read_file(violations_filepath))[\"violations\"]\n            \n            schedule[\"Violations\"] = violations\n\n        schedules.append(schedule)\n        \n    return schedules\n\ndef describe_monitoring_schedule(endpoint_name, monitoring_type):\n    found = False\n    \n    schedules = describe_monitoring_schedules(endpoint_name)\n    for schedule in schedules:\n        if schedule[\"MonitoringType\"] == monitoring_type:\n            found = True\n            print(json.dumps(schedule, indent=2))\n\n    if not found:            \n        print(f\"There's no {monitoring_type} Monitoring Schedule.\")\n\n\ndef describe_data_monitoring_schedule(endpoint_name):\n    describe_monitoring_schedule(endpoint_name, \"DataQuality\")\n\n    \ndef describe_model_monitoring_schedule(endpoint_name):\n    describe_monitoring_schedule(endpoint_name, \"ModelQuality\")\n\n    \ndef delete_monitoring_schedule(endpoint_name, monitoring_type):\n    attempts = 30\n    found = False\n    \n    response = sagemaker_client.list_monitoring_schedules(EndpointName=endpoint_name)[\"MonitoringScheduleSummaries\"]\n    for item in response:\n        if item[\"MonitoringType\"] == monitoring_type:\n            found = True\n            status = sagemaker_client.describe_monitoring_schedule(\n                MonitoringScheduleName=item[\"MonitoringScheduleName\"]\n            )[\"MonitoringScheduleStatus\"]\n            while status in (\"Pending\", \"InProgress\") and attempts &gt; 0:\n                attempts -= 1\n                print(f\"Monitoring schedule status: {status}. Waiting for it to finish.\")\n                sleep(30)\n                \n                status = sagemaker_client.describe_monitoring_schedule(\n                    MonitoringScheduleName=item[\"MonitoringScheduleName\"]\n                )[\"MonitoringScheduleStatus\"]\n\n            if status not in (\"Pending\", \"InProgress\"):\n                sagemaker_client.delete_monitoring_schedule(\n                    MonitoringScheduleName=item[\"MonitoringScheduleName\"]\n                )\n                print(\"Monitoring schedule deleted.\")\n            else:\n                print(\"Waiting for monitoring schedule timed out\")\n                \n    if not found:            \n        print(f\"There's no {monitoring_type} Monitoring Schedule.\")\n\n        \ndef delete_data_monitoring_schedule(endpoint_name):\n    delete_monitoring_schedule(endpoint_name, \"DataQuality\")\n\n    \ndef delete_model_monitoring_schedule(endpoint_name):\n    delete_monitoring_schedule(endpoint_name, \"ModelQuality\")\n\n\nData Monitoring\nSageMaker looks for violations in the data captured by the endpoint. By default, it combines the input data with the endpoint output and compare the result with the baseline we generated. If we let SageMaker do this, we will get a few violations, for example an “extra column check” violation because the field confidence doesn’t exist in the baseline data.\nWe can fix these violations by creating a preprocessing script configuring the data we want the monitoring job to use. Check Preprocessing and Postprocessing for more information about how to configure these scripts.\n\nDATA_QUALITY_PREPROCESSOR = \"data_quality_preprocessor.py\"\n\n\nimport json\n\ndef preprocess_handler(inference_record):\n    input_data = inference_record.endpoint_input.data\n    output_data = json.loads(inference_record.endpoint_output.data)\n    \n    response = json.loads(input_data)\n    response[\"species\"] = output_data[\"prediction\"]\n\n    # The `response` variable contains the data that we want the\n    # monitoring job to use to compare with the baseline.\n    return response\n\nOverwriting code/data_quality_preprocessor.py\n\n\nThe monitoring schedule expects an S3 location pointing to the preprocessing script. Let’s upload the script to the default bucket.\n\nimport os\n\nif not LOCAL_MODE:\n    bucket = boto3.Session().resource(\"s3\").Bucket(pipeline_session.default_bucket())\n    prefix = \"penguins-monitoring\"\n    bucket.Object(os.path.join(prefix, DATA_QUALITY_PREPROCESSOR)).upload_file(str(CODE_FOLDER / DATA_QUALITY_PREPROCESSOR))\n    data_quality_preprocessor = f\"s3://{os.path.join(bucket.name, prefix, DATA_QUALITY_PREPROCESSOR)}\"\n    data_quality_preprocessor\n\nINFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n\n\nWe can now set up the Data Quality Monitoring Job using the DefaultModelMonitor class. Notice how we specify the record_preprocessor_script using the S3 location where we uploaded our script.\n\nUncomment the %%script cell magic line to execute this cell.\n\n\nfrom sagemaker.model_monitor import CronExpressionGenerator, DefaultModelMonitor\n\ndata_monitor = DefaultModelMonitor(\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=1,\n    max_runtime_in_seconds=3600,\n    role=role,\n)\n\ndata_monitor.create_monitoring_schedule(\n    monitor_schedule_name=\"penguins-data-monitoring-schedule\",\n    endpoint_input=ENDPOINT,\n    record_preprocessor_script=data_quality_preprocessor,\n    statistics=f\"{DATA_QUALITY_LOCATION}/statistics.json\",\n    constraints=f\"{DATA_QUALITY_LOCATION}/constraints.json\",\n    schedule_cron_expression=CronExpressionGenerator.hourly(),\n    enable_cloudwatch_metrics=True,\n)\n\nWe can check the results of the monitoring job by looking at whether it generated any violations.\n\ndescribe_data_monitoring_schedule(ENDPOINT)\n\nThere's no DataQuality Monitoring Schedule.\n\n\n\n\nModel Monitoring\nTo set up a Model Quality Monitoring Job, we can use the ModelQualityMonitor class. The EndpointInput instance configures the attribute the monitoring job should use to determine the prediction from the model.\nCheck Amazon SageMaker Model Quality Monitor for a complete tutorial on how to run a Model Monitoring Job in SageMaker.\n\nUncomment the %%script cell magic line to execute this cell.\n\n\nfrom sagemaker.model_monitor import ModelQualityMonitor, EndpointInput\n\n\nmodel_monitor = ModelQualityMonitor(\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=1,\n    max_runtime_in_seconds=1800,\n    role=role\n)\n\nmodel_monitor.create_monitoring_schedule(\n    monitor_schedule_name=\"penguins-model-monitoring-schedule\",\n    \n    endpoint_input = EndpointInput(\n        endpoint_name=ENDPOINT,\n        inference_attribute=\"prediction\",\n        destination=\"/opt/ml/processing/input_data\",\n    ),\n    \n    problem_type=\"MulticlassClassification\",\n    ground_truth_input=GROUND_TRUTH_LOCATION,\n    \n    constraints=f\"{MODEL_QUALITY_LOCATION}/constraints.json\",\n    \n    schedule_cron_expression=CronExpressionGenerator.hourly(),\n    output_s3_uri=f\"{S3_LOCATION}/monitoring/model-quality\",\n    enable_cloudwatch_metrics=True,\n)\n\nWe can check the results of the monitoring job by looking at whether it generated any violations.\n\ndescribe_model_monitoring_schedule(ENDPOINT)\n\nThere's no ModelQuality Monitoring Schedule.\n\n\n\n\nClean up\nThe following code will stop the generation of traffic and labels, delete the monitoring jobs, and delete the endpoint.\n\nUncomment the %%script cell magic line to execute this cell.\n\n\nstop_traffic_thread.set()\ntraffic_thread.join()\n\nstop_ground_truth_thread.set()\ngroundtruth_thread.join()\n\ndelete_data_monitoring_schedule(ENDPOINT)\ndelete_model_monitoring_schedule(ENDPOINT)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to the program!"
  },
  {
    "objectID": "index.html#program-structure",
    "href": "index.html#program-structure",
    "title": "Introduction",
    "section": "Program Structure",
    "text": "Program Structure\n\nSession 1 - Production Machine Learning Is Different\n\nAn overview of the components of a machine learning system\nThe role of data in real-world applications\n4 popular sampling strategies to collect data\nLabeling strategies and a quick introduction to weak supervision and active learning\nData splitting and how data leakage can destroy your models\nBuilding good features and a quick introduction to imputation, standardization, and encoding\nProcessing data at scale using data parallelism\nUsing pipelines to orchestrate machine learning workflows\nA template architecture of a production-ready machine learning system\nUnderstanding SageMaker’s Processing Step and Processing Jobs\n\n\n\nSession 2 - Building Models And The Training Pipeline\n\nThe first rule of Machine Learning Engineering\nA 3-step process to solve a problem using machine learning\n9 tips to select the best machine learning model for your solution\nStrategies for working with imbalanced data, dealing with rare events, and a quick introduction to cost-sensitive learning\nThe reason you should not balance your data\nAn introduction to hyperparameter tuning\nThe importance of reproducibility and a quick introduction to experiment tracking\nDistributed Training using data and model parallelism\nUnderstanding SageMaker’s Training and Tuning Steps, and Training and Tuning Jobs\n\n\n\nSession 3 - Evaluating and Versioning Models\n\nThe difference between good models and useful models\nFraming evaluation metrics in the context of business performance\nAn 8-step process to evaluate machine learning models\nIntroduction to backtesting\nHow to deal with disproportionally important examples and rare cases\nStrategies to determine whether a model is fair and robust to future changes\nA 3-step process to perform error analysis and measure the impact of potential improvements\nHow to determine whether individual predictions are useful\nEvaluating Large Language Models using Supervised Evaluation and Self-Evaluation\nAn introduction to model versioning\nUnderstanding SageMaker’s Model Registry, Condition, and Model Steps\n\n\n\nSession 4 - Deploying Models and Serving Predictions\n\nHow do model performance, speed, and cost affect models in production\nLatency, throughput, and their relationships\nUnderstanding on-demand inference and batch inference and when to use each one\nHow to make models run fast using model compression and a quick introduction to quantization and knowledge distillation\nDeploying models in dedicated and multi-model endpoints\nA comparison of the tools you can use to serve predictions\nDesigning a 3-component inference pipeline\nUnderstanding the internal structure of a SageMaker Endpoint\nUnderstanding SageMaker’s PipelineModel and Amazon EventBridge\n\n\n\nSession 5 - Data Distribution Shifts And Model Monitoring\n\nThe 3 most common problems your model will face in production\nAn introduction to data distribution shifts, edge cases, and unintended feedback loops\nCatastrophic predictions and the problem with edge cases\nUnderstanding covariate shift and concept drift\nMonitoring schema violations, data statistics, model performance, prediction distribution, and changes in user feedback\nThe 3 strategies to keep your models working despite data distribution shifts\nUnderstanding SageMaker’s Transform Step, QualityCheck Step, Transform Jobs, and Monitoring Jobs"
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "Introduction",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nConfiguration Setup\nCohort Notebook"
  }
]